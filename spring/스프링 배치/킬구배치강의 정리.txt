1.스프링 배치 종결의 서막
*1.서문
스킵
*2.배치처리란
  배치는 일일정산,데이터마이그레이션,리포트생성,데이터정제,데이터통합등 다양한 용도로 사용됨
  배치에서 중요한건 정확성과 완결성임,처리에 시간이 걸려도 완벽하게 처리하는게 목표
  웹에서는 실패시 에러를 반환하고 끝냈지만,배치는 복구를 시도해볼수있음
  
  또한 트랜잭션도 체크포인트 기반으로 할수도있고,처리할 데이터양과 특성에 따라 범위조절도 가능함
  
  그리고 청크지향이라 데이터를 나눠서 순차처리할수있고,병렬처리도 가능함

*3.사전점검
  job은 하나의 완전한 배치 처리를 의미함(일일매출집계,정기결제등)
  step은 job을 구성하는 실행단위로,하나의 job은 하나이상의 step으로 구성됨
  즉 일일매출집계 job은
    매출집계step
	  전일주문데이터 read
	  결제완료된것만 필터링 process
	  상품별로 집계후 저장 write
	알림발송step
	  집계요약정보생성후 관리자에게 전달
	캐시갱신 step
	  집계된 데이터로 캐시정보 업데이트
  이런식으로 구성됨
  
  스프링배치는 배치잡을 만들기위해 필요한 거의 모든걸 제공함
  우리는 이 뼈대위에 스프링배치가 제공하는 컴포넌트들을 @configuration으로 구성만 하면됨
  즉 스프링배치를 스프링배치가 제공하는영역과 개발자가 제어하는 영역으로 나눠보는게 쉬움
  보통 잡과 스탭,잡런처,잡레포,ItemReader구현체,writer구현체등이 스프링 배치가 제공하는 영역임
  우리는 Job과 Step의 구성을 해야하고(configuration),기본적으로 제공되지않는 영역의 로직을 만들어야함(보통 processor은 제공하지않음)
  
  추가적으로 배치에선 시스템 시작의 결과를
    SpringApplication.run(abcApplication.class, args);
  이게 아닌
    System.exit(SpringApplication.exit(SpringApplication.run(abcApplication.class, args)));
  이런식으로 exit로 처리하는게 권장됨
  이러면 배치작업의 성공실패를 exit코드로 외부에 전달할수있어서 배치모니터링과 제어에 필수적임
  
  잡은 잡빌더를 통해 생성되는데,이때 잡의 이름과 잡레포지토리가 파라미터로 들어감
  또한 잡레포지토리와 트랜잭션매니저또한 자동으로 주입받을수있음
  잡이 스탭을 실행시키면,스탭은 순서대로 실행되고,이전스탭이 성공해야지만 실행됨
  
2.스프링 배치 시작
*1.스탭의 두가지 유형
  스탭은 크게 청크지향처리와 태스크릿 지향처리 두가지로 나눠짐
  1.태스크릿
	태스크릿은 가장 기본적인 처리방식으로,복잡하지않은 단순작업을 실행할때 사용함
	즉 대량데이터처리가 아닌,단순히 실행에 초점을 맞춘 오래된로그삭제,이동,알림발송등을 할때 사용됨
	즉 함수호출 하나로 끝날만한걸 할때 사용하면됨
	이걸 쓸떈 그냥 execute()에 원하는로직을 구현하고,구현체를 배치에 넘기기만 하면됨

	또한 RepeatStatus를 사용해서 반복을 할수있음(FINISHED, CONTINUABLE)
	FINISHED는 스탭이 종료됐다는거고,CONTINUABLE는 아직 더 진행해야한다는것
	물론 내부에서 반복문을 쓸수도있지만,이렇게 처리하는 이유는 짧은 트랜잭션을 사용해서 안전하게 배치처리를 하기위해서 이런식으로 처리함
	이러면 실패해도 한번에 처리하는양만큼만 실패하지,전체를 다시할필요는없음

	태스크릿을 구현했다면,이걸 스텝에 테스크릿으로 등록하면됨
	이건 @configuration파일에서 태스크릿과 스탭 빈을 만들고,이걸 잡 빈에 넣으면됨
	이떄 트랜잭션매니저와 잡레포지토리가 필요한데,만약 태스크릿작업이 트랜잭션이 필요없다면 ResourcelessTransactionManager을 사용할수있음
	이건 그냥 해당 클래스를 직접 new로 생성해서 스탭의 태스크릿넣는곳에 같이 넣어주면됨

	ResourcelessTransactionManager를 빈으로 만들고싶을땐 주의가 필요함
	이걸 빈으로 만들어버리면 메타데이터관리 트랜잭션에서도 저걸써버리기때문에 별도의 트랜잭션매니저구성을 해야함

	태스크릿을 사용할때,간단한작업이라면 그냥 스탭에서 람다식으로 정의해도되고,좀 복잡하다싶으면 별도클래스로 빼는게 나음
	보통 태스크릿작업을 할땐 태스크릿에서 생성자di로 처리할때 사용할 파라미터(레포지토리나 파일경로등)을 받아서 처리하는경우가 많은듯
	그리고 포인터같은걸 둬서 일정단위로 밀어내면서 처리하고,빈거만나면 끝내고 이런식
	
	즉 태스크릿은 단순하고 명확한 작업을 수행할때 사용되는 step이고
	  단순작업에 적합(알림발송,파일복사,오래된데이터삭제등)
	  Tasklet 인터페이스 구현후 이를 StepBuilder.tasklet()에 전달
	  RepeatStatus로 실행제어
	  트랜잭션지원(execute단위로 트랜잭션이 걸림)
	이런 특징들이 있음
  2.청크
    일반적으로 배치를 처리할때 사용하는 읽기-처리-쓰기를 할땐 청크지향 처리를 사용하면됨
	청크랑 데이터를 일정단위로 쪼갠 덩어리를 말함(백만데이터를 백개단위로 쪼개서 처리 이런느낌)
	이렇게 처리해야 메모리관리,가벼운 트랜잭션(작은실패)등이 가능해짐
	
	배치에서 읽기-처리-쓰기 패턴은 딱 3가지로 나뉘어짐
	이게 ItemReader,ItemProcessor,ItemWriter임
	
	1.ItemReader
	  이건 한번에 하나씩(db 한 row씩) 읽어서 반환하고,읽을데이터가 없다면 null을 반환함
	  이 null을 반환하는게 청크지향처리스탭의 종료시점임
	  
	  또한 이미 아이템리더의 경우 표준구현체들이 많이있음(FlatFileItemReader,JdbcCursorItemReader등)
	2.ItemProcessor
	  이건 데이터를 원하는 형태로 깎아내는,즉 실제 로직을 처리함
	  이것도 리더처럼 데이터 하나하나씩 입력받아서 반환함
	  얘는 보통
	    데이터가공(입력데이터를 출력데이터의 형태로 변환함)
		필터링(null을 반환해서 해당데이터를 처리흐름에서 제외시킴)
		데이터검증(입력데이터의 유효성을 검사,조건에 맞지않는 데이터를 만나면 예외를 발생시켜 잡을 중단시킴)
	  이런 작업을 처리하고,꼭 프로세서가 있을필요는없음,즉 스탭에서 직접 데이터를 읽고바로쓰게할수도있음
	
	3.ItemWriter
	  이건 프로세서의 결과물을 받아서 원하는방식으로 최종저장/출력함
	  이건 청크단위로 한번에 데이터를 쓰고,이단위로 트랜잭션에 묶임
	  이것도 다양한 구현체들이 이미 만들어져있음(FlatFileItemWriter,JdbcCursorItemWriter 등)
	
	이런식으로 크게 3개단위로 분리해서 처리되는게 기본패턴인데,이걸통해
	  완벽한 책임분리
	  재사용성극대화
	  높은 유연성(데이터소스가 바뀌면 리더만,데이터형식이 바뀌면 프로세서만 변경하면됨)
	  대용량처리의 표준
	를 얻을수있음
	
	이 청크지향스탭을 조립할때도 StepBuilder를 사용하면됨
	이때 
	  return new StepBulder("스탭명",jobRepository)
	    .<리더의반환타입,프로세서의반환타입>chunk(청크사이즈,트랜잭션메니저)
		.reader(itemReader())
        .processor(itemProcessor()) 
        .writer(itemWriter())
		.build()
	이런식으로 하면됨
	이떄 스탭의 동작방식은,
	  청크갯수만큼 리더가 읽어서 청크를 구성하고(단일호출로 n개의 데이터를 가져와 하나의 청크를 생성)
	  구성된 청크의 내용물을 하나하나 프로세서가 처리해서 처리완료된 청크로 만들고(단일처리는 맞음,즉 청크크기가 10이면 한 청크를 처리할때 10번호출)
	  처리완료된 청크를 라이터가 저장(이건 청크 전체를 한번에 처리함)
	이런 방식으로 진행됨
	이걸 더이상 읽을데이터가 없을때(read가 null을 반환할때)까지 반복함
	즉 청크크기만큼 read하고,청크크기만큼 process한다음 한번 write해서 저장하는게 한 청크처리임
	
	청크지향처리에서 트랜잭션관리는 각 청크단위로 진행됨
	즉 실패시 해당 청크만 롤백된다는것
	이떄 청크사이즈는 트레이드오프와 업무요구사항,데이터양을 고려해서 적절히 골라야함
	이때 트레이드오프는
	  청크사이즈가 클때
	    메모리에 많은데이터를 한번에 로드함
		트랜잭션범위가 커져서 롤백데이터양이 많아짐
	  청크사이즈가 작을때
	    롤백데이터가 최소화됨
		읽기쓰기io가 자주발생하게됨
	이런 문제가 있음
  
*2.JobParamater
  1.잡파라미터란?
    잡파라미터는 처리대상과 조건등 배치작업에 전달되는 입력값임 
	이걸 사용해서 같은 잡을 입력값만 바꿔서 유연하게 실행할수있음
	물론 -D로 프로퍼티전달을 할순있지만,목적이 서로 다름
	
  2.프로퍼티와 잡파라미터의 결정적 차이
    1.입력값 동적 변경
	  단순한경우 프로퍼티로 충분하지만,웹요청에 전달된값을 Job의 매개변수로 주입하려는등은 프로퍼티로 해결하기힘듬(배치 프로그램을 띄워두고 잡을 실행시키는형태일떄)
	  프로퍼티는 앱 시작시 한번 주입되고 끝이기때문
	
	2.메타데이터
	  스프링 배치는 잡파라미터의 모든값을 메타데이터 저장소에 저장하고,이걸통해
	    Job인스턴스 식별 및 재시작 처리
		Job 실행이력추적
	  등을 할수있음
	  
	  이때 메타데이터는 JobRepository를 통해 Job과 Step의 실행이력을 저장소에 저장하고,
	  여기엔 잡과 스탭의 시작/종료시간,실행상태,처리레코드수 등이 포함됨
	  
	  반면 프로퍼티는 메타데이터로 저장되지않아서 관리가 불가능해,배치운영과 제어를 제한하게됨
	  즉 재시작이나 처리이력관리등이 안되게됨
  3.JobParameters 전달하기
    1.커맨드라인(CLI)
	  커맨드라인에선
	    ./gradlew bootRun --args='--spring.batch.job.name=dataProcessingJob inputFilePath=/data/input/users.csv,java.lang.String'
	  이런식으로 
		 --spring.batch.job.name=실행잡이름 파라미터1이름=값1,파라미터타입 파라미터2이름=값2,파라미터타입
      이렇게 전달하면됨
	  
	  이때 파라미터타입은 DefaultJobParametersConverter를 통해 적절한 타입으로 변환되는데,
	  기본적인 타입과 LocalDateTime등 시간관련타입등 다양한 타입을 지원함
	2.프로그래밍방식
	  이경우엔 JobParametersBuilder컴포넌트를 사용해서 잡 파라미터를 만들고,이걸 JobLauncher에 넣어서 잡을 실행시킬수있음
	  이때 addJobParameter()의 체이닝을 통해 잡파라미터를 추가하고,toJobParameters()로 빌드를 할수있음
	  이것도 
	    addJobParameter(파라미터명,파라미터값,타입)
	  즉
		JobParameters jobParameters = new JobParametersBuilder()
          .addJobParameter("inputFilePath", "/data/input/users.csv", String.class)
          .toJobParameters();
	  이렇게 넣으면됨
	  
	  여기서 JobLauncher는 잡을 실행하는데 사용되는 핵심도구로,잡파라미터를 입력받아 잡의 실행컨텍스트를 생성하고,잡이 성공적으로 실행되게 관리하는역할을 함
	3.잡파라미터 직접 접근
	  잡파라미터에 직접 접근하려면,어디서 잡파라미터가 관리되는지를 알아야함
	  스프링배치에선 JobExecution이 잡의 실행정보를 쥐고있음,즉 잡파라미터도 이안에있음
	  스탭에서 잡파라미터를 찾으려면 이 JobExecution을 통해서 찾아야함
	  테스크릿에서 잡 파라미터를 찾을땐 
	     public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) {
         JobParameters jobParameters = chunkContext.getStepContext()																			            
            .getStepExecution()																				      
            .getJobParameters();
	  이런식으로 컨텍스트에서 익스큐션을 찾고,거기에 잡파라미터를 찾아야함
	  이때 잡익스큐션이 아닌 스텝익스큐션을 사용하는데,스텝익스큐션안에 부모잡의 잡익스큐션을 참조하고있어서 이렇게 찾는거
	  
  4.다양한 타입의 잡파라미터
    1.기본데이터타입
      기본적으로 잡파라미터를 받아서 사용할땐 di로 하게되는데
	     @Bean
         @StepScope //@Value로 잡파라미터를 전달받을떈 @StepScope등을 선언해줘야함
	       public Tasklet terminatorTasklet(
	  	     @Value("#{jobParameters['terminatorId']}") String terminatorId, 
	  	     @Value("#{jobParameters['targetCount']}") Integer targetCount
	       ) { 
	  이런식으로 di받으면됨
	  그리고
	    ./gradlew bootRun --args='--spring.batch.job.name=processTerminatorJob terminatorId=KILL-9,java.lang.String targetCount=5,java.lang.Integer'
      이런식으로 넣으면되는거
	2.날짜와 시간 타입
	  또한 날짜의 경우도 같은방식으로 di받고 
	    executionDate=2024-01-01,java.time.LocalDate startTime=2024-01-01T14:30:00,java.time.LocalDateTime
	  이런식으로 받으면됨
	  이떄 주의점은 LocalDate는 ISO_LOCAL_DATE,LocalDateTime은 ISO_LOCAL_DATE_TIME형식으로 전달해야한다는점
	  다른것도 마찬가지임 java.util.Date는 ISO_INSTANT 형식으로, java.time.LocalTime은 ISO_LOCAL_TIME형식으로 전달해야함
	  
    3.Enum타입
	  enum의 경우엔 받을땐 똑같이
	    @Value("#{jobParameters['questDifficulty']}") QuestDifficulty questDifficulty
	  이렇게하면되고,줄떈
	    questDifficulty=HARD,com.system.batch.killbatchsystem.TerminatorConfig$QuestDifficulty
	  이렇게하면되는데,이때 $는 중첩클래스일땐 저렇게 표시해야함,만약 중첩클래스가 아니라면 그냥 똑같이 경로설정해주면됨
	
	4.POJO를 활용한 잡파라미터주입
	  만약 여러 Job파라미터를 효율적으로 관리해야한다면,별도의 클래스를 만들어 파라미터를 관리하면 구조화와 재사용성이 높아짐
	  즉 잡파라미터 관리용 컴포넌트를 만들고,여기에 StepScope를 붙인다음 여기서 잡파라미터를 다 주입받은다음에 이걸 가져다가 쓰는거
	  어짜피 싱글톤으로 실행되니까
	
	5.기본파라미터 표기법의 한계
	  문제는,기본파라미터에서 쉼표(,)가 파라미터값에 포함되게되면 문제가 발생함 
	  그래서 사용되는게 Json기반의 파라미터표기임
	6.Json기반 표기법
	  기본적으론
	    implementation 'org.springframework.boot:spring-boot-starter-json'
	  이 의존성이 필요함
	  그리고나서 JsonJobParametersConverter빈을 직접 등록해줘야함
	  그리고나서 잡파라미터로
	    ./gradlew bootRun --args="--spring.batch.job.name=terminatorJob infiltrationTargets='{\"value\":\"판교서버실,안산데이터센터\",\"type\":\"java.lang.String\"}'"
	  이렇게 json을 넘겨주면됨
	  받는건 똑같이
	    @Value("#{jobParameters['infiltrationTargets']}") String infiltrationTargets
	  이렇게받으면됨(파라미터명으로)
    7.커맨드라인 파라미터는 어떻게 실제 Job으로 전달될까?
	  배치를 스프링부트3이랑 쓰면,앱이 시작될떄 JobLauncherApplicationRunner라는 컴포넌트가 자동으로 동작함
	  이건 스프링부트가 제공하는 어플리케이션러너중 한 종류로,얘는 커맨드라인으로 전달된 잡파라미터를 해석하고 이걸바탕으로 실제 잡을 실행하는 역할을 맡음
	  얘는
	    잡목록준비:등록된 모든 잡타입 빈을 자동주입받음
		유효성검증:잡타입빈이 여러개인데 잡네임이 없을경우 검증실패,만약 하나라면 생략가능
		명령어해석:커맨드라인으로 전달된 값(잡파라미터)들을 파싱함,
		  여기서 DefaultJobParametersConverter나 JsonJobParametersConverter을 사용해 문자열을 적절한 잡파라미터로 변환함
		Job실행:첫단계의 잡리스트에서 해당이름의 잡을 찾고,해당잡을 잡파라미터를 넣어서 실행시킴,이과정에서 JobLauncher라는 잡실행컴포넌트가 사용됨
	  이런식으로 동작함

  5.JobParametersValidator
    이건 말그대로 잡파라미터 밸리데이터,즉 검증도구임
	이 인터페이스는 단순하게 하나의 검증메서드만 있음 
		public interface JobParametersValidator {
			void validate(@Nullable JobParameters parameters) throws JobParametersInvalidException;
		}	
    말그대로 잡파라미터를 받아서 검증한후 아니다싶으면 예외를 던지면됨
	이건 잡을 등록할때 JobBuilder에서 validator로 등록해주면됨,이러면 잡실행시점에 호출되어 파라미터검증을 진행함
	
	물론 매번 만들긴 귀찮으니까  DefaultJobParametersValidator를 사용할수도 있는데
	이건 그냥 파라미터의 존재여부만 확인해줌
	이건
	  DefaultJobParametersValidator([필수파라미터명],[선택파라미터명])
	이렇게 넣어두면 검증해줌
	그리고 만약 선택파라미터에 값이 있다면,모든 잡파라미터들은 필수나 선택중 한군데에 들어가야함,만약 저기에없는 파라미터가 들어오면실패함
	선택파라미터에 값이 없다면,그냥 자유롭게 허용함
	
*3.배치 Scope  
  1.Job과 Step의 Scope 이해하기
    배치의 스코프는 스프링의 기본 스코프인 싱글톤과 다른 특별한 스코프를 제공함
	JobScope와 StepScope임
	
	얘들이 선언된 빈은 어플리케이션 구동시점에는 프록시로만 존재하다가,잡이나 스탭이 실행된후에 프록시객체에 접근하면 그때 실제빈이 생성됨
	이러면 이점이 뭐냐면,런타임에서 잡파라미터가 결정되어서 실행시점에 정확하게 주입받을수있고,
	동시에 여러잡이 실행되더라도 각각 독립적인 빈을 사용하게되어 동시성문제도 해결됨
	또한 잡이나 스탭의 실행이 끝나면 해당빈도 같이 제거되므로 메모리적으로도 효율적임
  
  2.@JobScope
    잡스코프는 잡이 실행될때 실제 빈이 생성되고 잡이 종료될떄 함께 제거되는 스코프임
	즉 JobExecution과 생명주기를 같이함
	
	이게 붙은 빈은 구동시점에는 프록시만 생성됨
	그래서 어플리케이션 실행중에 잡파라미터가 변경되거나,잡을 실행 직전에 잡파라미터를 만들어서 잡을 실행시키는등의 일이 가능해짐
	또한 잡이 실행될때 실제 빈이 생성되니까 병렬처리도 가능해짐
  
  3.@StepScope
    스텝스코프는 잡스코프와 유사하지만 스텝레벨에서 동작하는 스코프임
	이건 스텝의 실행범위에서 빈을 관리함
	즉 각각 스텝의 실행마다 새로운 빈이 생성되고,스텝이 종료될때 함께 제거됨
	그래서 동시성이슈가 터지지않음

  4.JobScope와 StepScope사용시 주의사항
    1.프록시대상의 타입이 클래스라면,반드시 상속가능한클래스여야함
	  cglib기반의 프록시생성이기떄문에 상속가능해야함
	2.Step빈에는 @StepScope와 @JobScope를 사용하지말라
	  스텝에 StepScope를 달면 스텝빈생성과 스코프활성화시점이 맞지않아 오류가발생함
	  배치는 스텝실행전 메타데이터관리를 위해 스텝빈에 접근해야하는데,이시점엔 스텝이 실행되지않아 스텝스코프가 활성화되지않은상태임
	  그래서 스코프없이 프록시에 접근하니까 안되는거
	  
	  마찬가지로 @JobScope도 스텝에 선언하면안됨
	  단순한 배치에선 문제가 없지만,복잡한 상황에선 예상치못한 문제가 발생함
	    JobOperator을 통한 Step실행제어
		Spring Integration(Remote Partitioning)을 활용한 배치확장기능등
	  이런거
	  
	  그런데 스텝에서 잡파라미터를 쓸땐 스코프가 필요함
	  이럴땐 스텝단위로 스코프를 받는게 아닌,Tasklet에서 스코프를 달아서 파라미터를 받으면됨
	  또 생기는 문제는 잡빌더에서 컴파일시점에 없는값을 참조할때 생기는 문제임
	  스텝에서 받는 잡파라미터는 컴파일시점엔 값이 존재하지않음
	  가장 깔끔한 방법은 스텝을 빈으로 등록하고 이걸 생성자주입받는것
	  
	  그런데 만약 직접 스탭을 호출해야한다면,해당자리에 null을 넣으면 알아서 잡이 실행될때 입력받은 잡파라미터값으로 교체함(지연바인딩)
	  
  5.ExecutionContext
    잡익스큐션과 스텝익스큐션은 시작시간,종료시간,실행상태등의 메타데이터를 관리하는데,이런 기본적인 정보만으로는 시스템을 완벽하게 제어하기 부족할때가 있음
	비즈니스로직 처리중 발생하는 커스텀데이터를 관리할 방법이 필요한데,이때 사용하는게 ExecutionContext라는 데이터컨테이너임
	기본적으로 잡파라미터는 잡의 실행시점에서 바뀌지않는데(불변),그래서 추가적인 데이터저장수단이 필요한것
	
	얘를 쓰면 커스텀컬렉션의 마지막처리인덱스같은 데이터를 저장할수있고,이건 잡이 중단된후 재시작할떄 특히 유용함
	배치가 재시작할때 이걸 자동으로 복원하기때문(이것도 메타데이터저장소에서 관리함)
	
	JobScope와 StepScope에서 ExecutionContext에 접근할때도 @Value로 접근할수있음
	  @Value("#{jobExecutionContext['previousSystemState']}") String prevState
	  @Value("#{stepExecutionContext['targetSystemStatus']}") String targetStatus
	이런식
	단 jobExecutionContext와 stepExecutionContext는 서로 다른 범위를 가지는데,
	jobExecutionContext는 해당 잡에 속한 모든 컴포넌트에서 접근할수있지만,stepExecutionContext는 해당 스텝에 속한 컴포넌트에서만 접근할수있음
	
	즉 스텝의 ExecutionContext에 저장된 데이터는 jobExecutionContext로 가져올수없고,다른 스탭의 stepExecutionContext데이터를 가져올수없음
	이렇게 스텝간 데이터 독립성이 완벽하게 보장됨
	만약 이전스텝의 처리결과를 다음스텝에서 활용해야한다면 jobExecutionContext에 담아서 사용해야함
	즉 jobExecutionContext는 job재시작시 복원과 step간 데이터공유수단으로도 사용됨
  
	
*4.Spring Batch Listener
  리스너는 배치의 주요순간들에 후킹해서 각 시점에 동작을 끼워넣을수있는 도구임
  잡이 시작하기 직전,완료직후,스텝시작직전,완료직후,청크단위,아이템단위등 다양한 위치에 로직을 끼워넣을수있음
  이때 로깅,모니터링,에러처리등 다양한 작업을 할수있음
  1.리스너 종류  
    1.JobExecutionListener
      이건 잡실행의 시작과 종료시점에 호출되는 인터페이스임
	  잡의 실행결과를 로깅,이메일전송하거나 잡 시작전에 필요한 리소스를 준비하고 끝난후에 정리하는등의 작업을 할수있음
	  afterJob은 잡 실행정보가 메타데이터 저장소에 저장되기전에 호출됨
	  그래서 잡의 실행결과를 완료에서 실패로 변경하거나,그 반대로 처리하는등의 작업도 가능함
    
    2.StepExecutionListener
      이건 스탭의 시작과 종료시점에 호출되는 리스너 인터페이스임
	  스텝의 시작시간,종료시간,처리데이터수를 로그로 기록하는등의 사용자정의작업을 추가할수있음
	  또한 afterStep에선 ExitStatus를 반환하는데,이걸통해 스텝의 실행결과상태를 직접 변경할수있음
    
    3.ChunkListener
      청크지향처리는 청크단위로 아이템읽기쓰기를 반복하는데,ChunkListener는 이런 하나의 청크단위처리가 시작되기 직전,완료된후,에러가 발생했을때 호출됨
	  즉 청크단위로 모니터링하거나 로깅할수있음
	  여기서 afterChunk는 트랜잭션이 커밋된후 호출되고,
	  청크처리중 예외가 발생하면 afterChunkError이 afterChunk대신 호출되는데,이땐 트랜잭션롤백이후 호출됨
    
    4.Item[Read|Process|Write]Listener
      아이템리스너는 아이템의 읽기,쓰기,처리작업이 수행되기 직전,직후,에러발생시점에 호출됨
	  여기서 주의점은
	    ItemReadListener.afterRead()는 read호출이후 호출되지만,null을 반환할떈 호출되지않음
	    ItemProcessListener.afterProcess()는 process가 null을 반환하더라도(필터링하더라도) 호출됨
	    ItemWriteListener.afterWrite()는 트랜잭션이 커밋되기전,그리고 ChunkListener.afterChunk()가 호출되기전에 호출됨
    
  2.배치 리스너, 이런 것들을 할 수 있다
    배치 리스너에선
	  단계별 모니터링과 추적:각 잡이나 스탭의 실행전후 로그를 남길수있고,이떄 언제시작하고 언제끝났는지,몇개데이터를 처리했는지등을 기록하고 추적할수있음
	  실행결과에 따른 후속처리:잡과 스텝의 실행상태를 리스너로 직접 확인하고 그에따른 조치를 할수있음(잡이 실패했을떄 다시돌린다든가)
	  데이터 가공과 전달:실제 처리로직 전후에 데이터를 추가로 정제하거나 변환할수있음
	    StepExecutionListener나 ChunkListener를 사용해서 ExecutionContext의 데이터를 수정하거나 필요한 데이터를 추가하는등
		다음 처리에 필요한내용을 미리 준비할수있음
	  부가기능분리:주요처리로직과 부가로직을 깔끔하게 분리할수있음(aop처럼)

    1.배치리스너 구현방법
      리스너 구현법은 두가지가 있음
	  전용 리스너 인터페이스를 구현하던가,어노테이션을 붙이던가
	  가장 일반적인 방법은 그냥 리스너 인터페이스를 구현한 후에 Builder에서 .listener()에 넣어주면됨
	  이떄 JobExecutionListener는 JobBuilder에,나머지 빌더들은 StepBuilder에 넣으면됨
	  
	  더 간단한방법은 어노테이션기반구현임
	    @BeforeJob, @AfterJob, @BeforeStep, @AfterStep //이런느낌,밑에는 모든 어노테이션들
	    @AfterChunk, @AfterChunkError, @AfterJob, @AfterProcess, @AfterRead, @AfterStep, @AfterWrite, @BeforeChunk, @BeforeJob, @BeforeProcess, @BeforeRead, @BeforeStep, @BeforeWrite, @OnProcessError, @OnReadError, @OnSkipInProcess, @OnSkipInRead, @OnSkipInWrite
	  그냥 리스너클래스를 상속없이 만든다음,저 어노테이션을 붙인 메서드를 만들고,이걸 위에처럼 Builder에 넣으면됨
    
    2.JobExecutionListener와 ExecutionContext를 활용한 동적 데이터 전달
      만약 잡파라미터만으로 전달할수없는 동적데이터가 필요할땐 JobExecutionListener의 beforeJob()를 사용해서 동적데이터를 각 스텝에 전달할수있음
  	  저기에다가 
	    jobExecution.getExecutionContext().put(데이터)
	  이런식으로 데이터를 집어넣는식
	  그리고 스탭에서 값을 꺼내다가 쓰면됨

    3.왜 JobParameters가 아닌 ExecutionContext를 사용할까?
	  잡파라미터는 한번 생성된 이후 변경될수없음(불변)
	  이걸통해 배치의 재현가능성과 일관성을 스프링배치는 보장함
	    재현가능성:같은 잡파라미터로 실행된 잡은 항상 같은결과를 생성해야함,중간에 변경될경우 이를 보장할수없음
	    추적가능성:배치작업의 실행기록과 잡파라미터는 메타데이터저장소에 저장됨,이게 변경가능하다면 기록과 실제작업간 불일치가 발생할수있음
	  그래서 동적으로 생성되거나 변경되어야 하는 데이터는 ExecutionContext를 통해 관리해야함
	
	  단,동적으로 전달하는건 유용하긴하지만 잡파라미터만으로 충분히 처리할수있다면 그걸로 처리하는게 좋음
	  만약 beforeJob에서 localDate.now등으로 현재날짜를 넘긴다면 그날데이터를 재처리하는등의 작업이 불가능해짐
	  이런건 외부에서 파라미터로 받는게 나은방식임
	  즉 가급적 잡파라미터로 di받고,외부에서 값을 받을수없는경우에만 동적으로 생성하던가 하면됨
	
	  그런데 잡수준의 ExecutionContext에선 모든스탭에서 공유되지만,스탭수준의 ExecutionContext에선 다른 스탭과 공유가 불가능함
	  그래서 스탭의 컨텍스트에서 값을 가져와서,잡의 컨텍스트로 밀어넣는 작업이 필요해지는데,이게 코드가 너무 많이필요하니까 배치에서 만들어둔 리스너가 있음
	  ExecutionContextPromotionListener임

    4.ExecutionContextPromotionListener를 활용한 스탭간 데이터 공유
      이건 스탭수준의 컨텍스트데이터를 잡수준 컨텍스트로 등록시켜주는 구현체임
	  즉 스탭데이터를 잡데이터로 프로모션(승격)시켜주는거임
	
	  이걸 리스너로 스탭에 등록해두면,거기에 세팅된 데이터의 키를 바탕으로 자동으로 승격처리를 시킴
	    @Bean
		public ExecutionContextPromotionListener promotionListener() {
			ExecutionContextPromotionListener listener = new ExecutionContextPromotionListener();
			listener.setKeys(new String[]{"targetSystem"});
			return listener;
		}
	  이런식임
	
	  단 가급적이면 각 스텝은 가능한 독립적으로 설계하는게 재사용성과 유지보수성이 올라감
	  불가피하지않다면 스텝간 데이터의존성은 최소화하는게 좋음

    5.Listener와 @JobScope, @StepScope 통합
      리스너와 스코프를 활용하면 리스너에서 잡파라미터를 쉽게 다룰수있음
	   @JobScope
	   public JobExecutionListener systemTerminationListener(
			   @Value("#{jobParameters['terminationType']}") String terminationType
	   ) {  
	  이렇게 받고
        return new JobBuilder("killDashNineJob", jobRepository)
	      .listener(systemTerminationListener(null))  // 파라미터는 런타임에 주입
	  컴파일시점에(빌더에) null을 넘겨주면됨
    
	  이때 JobExecutionListener라면 @JobScope를 붙여야함(잡의 실행과 생명주기를 같이하니까)
	  이렇게 주입받은 파라미터는 리스너의 어느메서드든 자유롭게 사용할수있음

  3.Listener 마지막 훈련: 성능과 모범 사례
    1.리스너를 효과적으로 다루는법
	  범위와 목적에 따라 적절한 리스너를 선택해야함
	    JobExecutionListener:잡의 시작과 종료를 통제
		StepExecutionListener:스탭의 단계를 통제
		ChunkListener:시스템을 청크단위로 제어하거나,반복의 시작과 종료시점을 통제
		ItemLintener:개별아이템 식별 통제
    2.예외처리는 신중하게
	  beforeJob과 beforeStep에서 예외가 발생하면 잡이나 스탭이 실패한걸로 취급되고,
	  afterJob이나 afterStep에서 예외가 발생하면 무시됨
	  만약 before쪽에서의 예외가 중요하지않다면 잡아서 무시할수있음
	
	3.단일책임원칙 준수
	  리스너에선 로깅 모니터링등 각 리스너별 책임만 담당하고,메인로직은 분리해야함
	  리스너가 너무 많은일을하면 유지보수가 어려워지고 시스템동작파악이 어려워짐
	
	4.성능최적화를 위한 경고
	  1.실행빈도를 고려하라
	    JobExecutionListener/StepExecutionListener의 경우엔 잡이나 스탭별로 한번씩만 실행되니까 작업이 좀 무거워도됨
	    그런데 ItemReadListener/ItemProcessListener는 매 아이템마다 실행되니까 치명적인 문제가 될수있음
      2.리소스사용을 최소화하라
	    db연결,파일io,외부api호출을 최소화하고,리스너내 로직은 가능한 가볍게 유지하는게 좋음
		특히 아이템단위리스너에서 더더욱 중요함
  
3.파일처리배치
*1.FlatFileItemReader
  1.청크지향처리
    데이터처리는 단순히 읽어서 가공하고 쓴다 딱 3개임
	청크처리도 마찬가지임
	  데이터읽기(ItemReader):데이터를 하나씩 읽어들임,청크사이즈만큼 반복실행
	  데이터가공(ItemProcessor):읽어들인 데이터를 원하는대로 가공,필터링도 수행
	  데이터저장(ItemWriter):청크단위로 모아진 데이터를 한번에 처리
	이렇게 3개가 다임
  2.파일기반 배치처리
    개발자가 직접 파일처리를 구현한다면 보일러플레이트도 많이생기고 귀찮아짐
	그래서 스프링배치에서는 파일기반 아이템리더와 라이터를 제공해줌
	이걸쓰면 io작업,데이터파싱,유효성검사,예외처리까지 다 처리해줌
	
	1.FlatFile
	  플랫파일이란 단순하게 행과 열로만 구성된파일,즉 csv같은거임
	  얘들은
	    각라인이 하나의 데이터 로우,\n이 레코드의 끝을 의미함
		쉼표나 탭,스페이스등으로 필드를 구분할수있고 고정길이로 구분할수도있음
		거의 모든시스템에서 읽고쓸수있는 표준형식,엑셀,db등 다양한 도구와 호환되고 사람이 읽기도 쉽고 대용량처리도 쉬움
	  이런 특징이 있음
	2.FlatFileItemReader
	  이건 플랫파일로부터 데이터를 읽어오는 클래스임
	  얘는 파일을 한줄씩 읽어서 지정한 도메인객체로 변환해서 반환함
	  즉
	    파일을 한줄씩 읽어온다
		읽어온 한줄의 문자열을 우리가 사용할 객체로 변환해 리턴한다
	  이런식으로 동작함
	  
	  이때 한줄의 데이터를 변환할땐 LineMapper이라는 컴포넌트 인터페이스를 사용하는데,
	  이건 단순하게 구분자로 분리해낸다음 각 분리한 데이터를 각 지정된 객체의 프로퍼티에 정확하게 매핑해야함
	  물론 이 인터페이스를 우리가 구현할필요는 없음
	  DefaultLineMapper이라는 기본구현체를 제공하고있기때문
	  
	3.DefaultLineMapper
	  이건 크게 두단계로 동작함
	    1.토큰화:하나의 구분자나 고정길이단위로 잘린 문자열을 각 토큰단위로 분리함//LineTokenizer사용하고,여기서도 인터페이스로 구분자나 고정길이별 구현체존재
		2.객체매핑:분리된 토큰을 도메인객체의 프로퍼티에 매핑함//FieldSetMapper사용하고,기본값은 BeanWrapperFieldSetMapper사용
	  즉 단순하게 토큰으로 만든다음 매핑하는걸 담당함
	  이때 매핑의 기본값인 BeanWrapperFieldSetMapper는 자바빈규약을 따르는 객체에 데이터를 매핑해줌
	  그래서 필드의 이름과 매핑객체의 프로퍼티명이 동일해야하고 setter가 필요함
	
	4.구분자로 분리된 형식의 파일 읽기
	  이걸 사용할땐 단순하게 빈으로 등록한다음 사용하면됨
		@Bean
		@StepScope
		public FlatFileItemReader<SystemFailure> systemFailureItemReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new FlatFileItemReaderBuilder<SystemFailure>()
					.name("systemFailureItemReader")
					.resource(new FileSystemResource(inputFile))
					.delimited()
					.delimiter(",")
					.names("errorId",
							"errorDateTime",
							"severity",
							"processId",
							"errorMessage")
					.targetType(SystemFailure.class)
					.linesToSkip(1)
					.build();
		}	  
	  이렇게 빌더로 
	    아이템리더 식별자를 넣고,
		타겟위치(파일위치)를 넣어주고
		구분자와 필드명을 넣어주고,
	    타겟클래스를 저장해주고,
		맨첫줄이 필드명이라면 스킵도 넣어주고 
		빌드한후 
	  스탭에서 가져다쓰면됨
	  또한 제네릭은 컴파일시점의 객체타입을 지정하고,타겟클래스는 런타임시점의 객체타입을 지정함
	  그리고 FlatFileItemReader는 #을 기본적으로 주석으로 인식하는데,만약 다른식으로 주석을 썼다면 comments("//")로 지정해줄수도있음
	  
	  그리고 파일 누락시 예외를 발생시키는걸 컨트롤하려면 .strict(true/false)를 사용하면됨
	  기본적으로는 true고 false로 하면 아이템리더가 null을 바로 반환하는식임
	  또한 라인토크나이저에서도 true면 한 row의 토큰리스트의 갯수가 전달된 객체프로퍼티(names)와 다르면 예외를 발생시키고,false면 자동으로 보정함
    
	5.고정길이형식의 파일 읽기
	  고정길이파일은 각 필드가 고정된길이로 맞춰진 텍스트파일임
	  이땐 각 필드의 길이를 스프링배치에 알려야함
		@Bean
		@StepScope
		public FlatFileItemReader<SystemFailure> systemFailureItemReader(
			   @Value("#{jobParameters['inputFile']}") String inputFile) {
		   return new FlatFileItemReaderBuilder<SystemFailure>()
			   .name("systemFailureItemReader")
			   .resource(new FileSystemResource(inputFile))
			   .fixedLength()
			   .columns(new Range[]{
				 new Range(1, 8),     // errorId: ERR001 + 공백 2칸
				 new Range(9, 29),    // errorDateTime: 날짜시간 + 공백 2칸
				 new Range(30, 39),   // severity: CRITICAL/FATAL + 패딩
				 new Range(40, 45),   // processId: 1234 + 공백 2칸
				 new Range(46, 66)    // errorMessage: 메시지 + \n
			   })
			   .names("errorId", "errorDateTime", "severity", "processId", "errorMessage")
				.targetType(SystemFailure.class)
			   .build();
		}
      이런식으로 구분자랑 거의 비슷한데,fixedLength()로 표시한후 .columns로 각 범위를 표시해주면됨	
	  그리고 .strict()도 true면 추가적으로 파일의 읽은길이를 엄격하게 검증함
	  파일의 읽은 라인길이가 range에 지정된 최대길이(마지막필드의 끝위치)와 다르다면 예외를 발생시킴
	  또한 공백의경우 내부적으로 trim을 돌리기때문에 신경안써도됨
	
	6.프로퍼티 타입에 LocalDateTime을 쓰고 싶다면?
	  시간타입을 사용하고싶으면 별도의 변환기가 필요함
	  BeanWrapperFieldSetMapper는 이런 복잡한 변환까진 처리하지않는데,
	  그래서 빌더에서 customEditors()를 사용해서 커스텀 propertyEditer를 등록할수있음
	    .customEditors(Map.of(LocalDateTime.class, customEditor()))	  
	  즉 propertyEditer를 구현하고,이걸 빌더의 .customEditors를 사용해 등록하면됨 
  
  
    7.RegexLineTokenizer
	  이건 복잡한 형식의 파일을 처리할때 사용하는 도구임
	  이걸 등록하는건 아이템리더빌더에서 .lineTokenizer()을 사용하면되고,
        RegexLineTokenizer tokenizer = new RegexLineTokenizer();
        tokenizer.setRegex("\\[\\w+\\]\\[Thread-(\\d+)\\]\\[CPU: \\d+%\\] (.+)");
	  이런식으로 정규식을 사용해서 토크나이저를 할수있음
    8.fieldSetMapper()
	  마찬가지로 fieldSetMapper도 커스텀을 등록할수있음(필드 매핑하는클래스)
		.fieldSetMapper(fieldSet -> new LogEntry(fieldSet.readString(0), fieldSet.readString(1)))
	  단 이걸 사용할땐 targetType를 사용하면안됨
	  스프링 배치 5.2.3이전버전이면 에러도 안뜨고 fieldSetMapper가 무시되고,이후버전이면 예외가 뜸(즉 둘다 설정하려고하면 예외발생)
	
	9.PatternMatchingCompositeLineMapper 
	  만약 하나의 파일안에 여러형식의 라인이 혼재되어있다면 이걸 사용할수있음
	  이건 패턴매칭을 지원해서,각 라인의 패턴을 먼저 파악한후,해당 패턴에 맞는 토크나이저와 필드매퍼를 적용할수있음
		@Bean
		@StepScope
		public FlatFileItemReader<SystemLog> systemLogReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new FlatFileItemReaderBuilder<SystemLog>()
					.name("systemLogReader")
					.resource(new FileSystemResource(inputFile))
					.lineMapper(systemLogLineMapper())
					.build();
		}
	  등록은 이렇게 등록한후에
		@Bean
		public PatternMatchingCompositeLineMapper<SystemLog> systemLogLineMapper() {
			PatternMatchingCompositeLineMapper<SystemLog> lineMapper = new PatternMatchingCompositeLineMapper<>();

			Map<String, LineTokenizer> tokenizers = new HashMap<>();
			tokenizers.put("ERROR*", errorLineTokenizer());
			tokenizers.put("ABORT*", abortLineTokenizer());
			tokenizers.put("COLLECT*", collectLineTokenizer());
			lineMapper.setTokenizers(tokenizers);

			Map<String, FieldSetMapper<SystemLog>> mappers = new HashMap<>();
			mappers.put("ERROR*", new ErrorFieldSetMapper());
			mappers.put("ABORT*", new AbortFieldSetMapper());
			mappers.put("COLLECT*", new CollectFieldSetMapper());
			lineMapper.setFieldSetMappers(mappers);

			return lineMapper;
		}
	  이렇게 각 종류에 맞는 토크나이저와 매퍼를 등록하면됨(앞에있는게 조건임)
	  여기선 맨앞의 값으로 구분했지만 패턴이니까 패턴만 맞추면 어떤형식이든 가능
	
	10.RecordFieldSetMapper: Record 매핑 지원
	  만약 targetType을 record타입으로 잡는다면 자동으로 BeanWrapperFieldSetMapper 대신 RecordFieldSetMapper를 사용함
	  동작자체는 비슷하니까 신경안써도됨
	
	11. MultiResourceItemReader:여러 파일 읽기
	  여러파일을 읽어야할때(여러서버의 로그파일들을 통합분석등),각 파일마다 스탭을 만드는건 비효율적임
	  이때 사용하는게 MultiResourceItemReader임
	  이건 여러파일들을 순차적으로 읽게 해주는 ItemReader구현체임
	  그냥 말그대로 여러파일들은 순서대로 읽음
	  
	  얘가 직접 읽진않고,위임대상 ItemReader에게 실제 읽기를 맡기고,걔한테 일주는일을 담당함
	  그래서 이걸 사용할땐 두가지 구성이 필요함
	    실제로 사용할 ItemReader
		읽어들일 파일의 목록
	  그래서 이런식으로 구성됨
		return new MultiResourceItemReaderBuilder<SystemFailure>()
		  .name("multiSystemFailureItemReader")
		  .resources(new Resource[]{
			  new FileSystemResource(inputFilePath + "/critical-failures.csv"),
			  new FileSystemResource(inputFilePath + "/normal-failures.csv")
		  })
		  .delegate(systemFailureFileReader())
		  .build();	 
	  resources로 파일들을 지정해주고,delegate로 실제 사용할 리더를 넣어주는식
	  단 여기서 중요한건,systemFailureFileReader() 빈생성시에 resource가 없어야함(그건 우리가 넣어줘야하니까),나머진 동일함
	  또한 resources의 파일읽기순서는 파일명의 알파벳순서로 기본적으로 읽고,이걸 바꾸고싶다면 comparator()메서드로 원하는 정렬기준을 지정할수있음
	
*2.FlatFileItemWriter 	  
  이건 데이터를 플랫파일형식으로 쓰는작업을 담당함,즉 도메인객체를 문자열로 변환해서 파일에 써내려감
  이것도 리더랑 거의비슷하게 선언해서 빈등록하면됨
  
  얘가 하는일은 필드추출과 문자열결합 두개로 나눠볼수있고,필드추출은 FieldExtractor,문자열결합은 LineAggregator이 담당함
  1.필드추출과 라인결합
    필드추출은 도메인객체에서 필드를 추출하는역할을 함
	즉 T를 받아서 object[]를 내보내는 역할을 함
	
	이것도 당연히 기본구현체가 있는데 BeanWrapperFieldExtractor 와 RecordFieldExtractor 임
	BeanWrapperFieldExtractor는 빈객체로부터 필드를 추출하는거고,RecordFieldExtractor는 레코드객체에서 필드를 추출함
	배치는 파일의 도메인타입에 따라 이 두개중 하나를 자동선택함
	
	문자열결합은 추출한 데이터들을 하나의 문자열로 결합하는 역할을 함
	이것도 구분자기반으로 읽던가 고정길이로 읽던가가 나뉜거처럼,여기도 2개로 나뉘는데
	DelimitedLineAggregator는 구분자로 구분하는거고 FormatterLineAggregator는 고정길이를 포함한 다른형식으로 쓸때 사용함
	이거 선택은 직접 라이터빌더설정에서 delimited()와 formatted()를 사용해서 선택할수있음
	
	또한 내부적으로는 LineAggregator가 FieldExtractor를 합성한 형태로 구성되어있음
	그래서 객체를 문자열로 변환하는 전과정은 LineAggregator가 담당함
	
	기본적으로 단일객체를 이렇게 바꾸고,라이터는 청크단위로 동작하니까 각 아이템마다 이걸 반복해서처리함

  2.구분자형식의 파일쓰기
    아이템 라이터는 기본적으로
	    @Bean
		@StepScope
		public FlatFileItemWriter<DeathNote> deathNoteWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new FlatFileItemWriterBuilder<DeathNote>()
				.name("deathNoteWriter")
				.resource(new FileSystemResource(outputDir + "/death_notes.csv"))
				.delimited()
				.delimiter(",")//구분자
				.sourceType(DeathNote.class)
				.names("victimId", "victimName", "executionDate", "causeOfDeath")
				.headerCallback(writer -> writer.write("처형ID,피해자명,처형일자,사인")) //파일헤더
				.build();
		}
	이런형태로 만들어짐
	FieldExtractor를 전달할때는 빈등록을 하던가 빌더의 fieldExtractor()메서드로 직접 넣어주면됨
	그리고 FieldExtractor가 객체로부터 어떤 필드를 추출할지를 알려줘야하는데,이게 .names()임
	단 이때 지정한순서대로 쓰여지니까 순서조심해야함
	그리고 names()는 자동구성방식에서만 사용되고,fieldExtractor()메서드로 직접 커스텀 fieldExtractor를 넣었을경우엔 무시됨
	RecordFieldExtractor를 사용할땐 names가 무시되는 버그가 있으니 조심
	Record타입을 sourceType으로 전달할경우에 자동으로 RecordFieldExtractor 가 사용됨
	커스텀FieldExtractor의 경우엔 setNames으로 필드명을 지정할수있음
		
	이때 스텝의 청크는
	  .<DeathNote, DeathNote>chunk(10, transactionManager)
	이런형태로 리더의 출력,라이터의 입력타입을 넣어주면됨

  3.커스텀 포맷 형식으로 파일 쓰기
    커스텀포맷을 사용할땐 FormatterLineAggregator를 사용할수있음
		@Bean
		@StepScope
		public FlatFileItemWriter<DeathNote> deathNoteWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new FlatFileItemWriterBuilder<DeathNote>()
					.name("deathNoteWriter")
					.resource(new FileSystemResource(outputDir + "/death_note_report.txt"))
					.formatted()
					.format("처형 ID: %s | 처형일자: %s | 피해자: %s | 사인: %s")
					.sourceType(DeathNote.class)
					.names("victimId", "executionDate", "victimName", "causeOfDeath")
					.headerCallback(writer -> writer.write("================= 처형 기록부 ================="))
					.footerCallback(writer -> writer.write("================= 처형 완료 =================="))
					.build();
		}	
	이런식으로 formatted()를 사용한후 format로 형식을 지정해주고, .names()의 순서대로 집어넣으면됨

  4.파일 처리 옵션
    현재 
	  .resource(new FileSystemResource(outputDir + "/death_note_report.txt")) 
	이런식으로 하면 실행할때마다 파일이 계속 덮어쓰기됨
	그래서 동작방식을 선택할수있는데,
		shouldDeleteIfExists: 기존 파일의 삭제 여부(FlatFileItemWriterBuilder.shouldDeleteIfExists()를 사용해 지정)
		append: 기존 파일에 데이터 덧붙이기 여부(FlatFileItemWriterBuilder.append() 메서드를 사용해 지정)
		shouldDeleteIfEmpty: 빈 결과 파일 처리 여부(FlatFileItemWriterBuilder.shouldDeleteIfEmpty() 메서드를 사용해 지정)
	이런식으로 여러 옵션들이 있음
	
	shouldDeleteIfExists는 기본값이 true로,겹치는파일이 있다면 삭제하고 새로 파일을 생성함
	만약 false라면 겹칠때 예외를 던짐
	
	append는 기본값이 false로,true로 설정하면 shouldDeleteIfExists는 자동으로 false로 설정되고,기존파일에 데이터를 추가함
	
	shouldDeleteIfEmpty는 기본값이 false로,파일에 헤더와 푸터를 제외한 데이터가 하나도없으면 파일을 삭제할지여부를 나타냄 	
	여기서 주의할건,해당파일에 얼마나 데이터가 있냐가 아닌,이번에 얼마나 데이터를 썼냐가 기준이라 append를 true로 설정해뒀는데 아무것도 안썼으면 지워버리니 주의
	
  5.FlatFileItemWriter의 롤백 전략: 버퍼링을 통한 안전한 파일 쓰기
    파일은 db와 달리 이미 쓰여진 데이터를 롤백할수없음
	그래서 FlatFileItemWriter는 데이터를 즉시 쓰지않고 내부버퍼에 일시적으로 저장해두다가,
	트랜잭션이 커밋될때(beforeCommit()이 호출될때) 버퍼의 데이터를 파일에 씀
	즉 트랜잭션과 같은범위로 성공실패를 시킬수있음
	이건 FlatFileItemWriterBuilder의 transactional()메서드로 설정할수있고,기본값은 true임

  6.파일 쓰기와 OS 캐시: forceSync 옵션
	os는 기본적으로 매번 디스크에 파일을 쓰지않고,메모리캐시에 먼저 저장한후 파일쓰기를 하는데 이 시차때문에 유실이 발생할수있음
    forceSync()를 true로 설정하면 이 시차를 없애서 즉시 동기화할수있음
	단 잦은동기화로 성능저하가 발생할수있음,기본값은 false임

  7.대용량 파일의 분할 처리: MultiResourceItemWriter
    한파일에 적게되는 양이 너무 커질경우,파일을 나눠서 저장해야하는 필요성이 생기는데,이때 사용되는게 MultiResourceItemWriter임
	이건 이름그대로 여러 리소스에 데이터를 분배하는 ItemWriter구현체임
	애는 직접 파일을 쓰지않고,쓰는작업은 쓰기를 할 ItemWriter에 위임함
	
	그래서 이걸 사용할땐
		@Bean
		@StepScope
		public MultiResourceItemWriter<DeathNote> multiResourceItemWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new MultiResourceItemWriterBuilder<DeathNote>()
					.name("multiDeathNoteWriter")
					.resource(new FileSystemResource(outputDir + "/death_note"))
					.itemCountLimitPerResource(10)
					.delegate(delegateItemWriter())
					.resourceSuffixCreator(index -> String.format("_%03d.txt", index))
					.build();
		}
	이런식으로 delegate에 실제로 사용할 아이템라이터를 넣어주고,
	itemCountLimitPerResource로 한 파일에 저장할 갯수를 설정한다음,resourceSuffixCreator로 어떤식으로 구분할지를 적어주면됨
	
	사용은 그냥 아이템라이터처럼쓰면됨

*3.JSON 파일 읽고 쓰기
  1.커스텀 LineMapper를 활용한 json 문자열 읽기
    만약 줄마다 각각 json객체로 이루어진
	  {...} 
	  {...} 
	  {...}
	이런형태가 있다면(jsonl),그리고 만약 특정객체로 파싱해야한다면
	이럴땐 커스텀 라인매퍼를 만들어야함,간단하게는 아이템리더빌더에 람다식을 써서 만들수있음
	  .lineMapper((line, lineNumber) -> objectMapper.readValue(line, SystemDeath.class))
	기본적으로 JsonLineMapper을 제공하긴하지만 특정객체를 요구한다면 이렇게 간단하게 커스텀객체를 만들수있음
	
	그리고 각 줄마다 json객체가 있는게 아닌,제대로 포맷팅된 객체가 올수도있음(하나의 json객체가 여러줄에 걸쳐 기록된경우)
	이럴땐 리더빌더에 recordSeparatorPolicy를 추가해 하나의 레코드가 어디서 끝나는지를 결정하면됨
	  .recordSeparatorPolicy(new JsonRecordSeparatorPolicy())//이건 기본적으로 있는거임
	RecordSeparatorPolicy 인터페이스는
	  public interface RecordSeparatorPolicy {
		boolean isEndOfRecord(String line); // 해당 줄이 레코드의 끝인지 판단.
		String postProcess(String record); // 레코드가 완성된 후 추가 처리.
		String preProcess(String record); // 레코드를 시작하기 전 사전 처리.
	  }
	이런식으로 구성되고
	이걸 구현한 JsonRecordSeparatorPolicy는 여는중괄호와 닫는중괄호의 갯수가 동일하고,현재읽은라인이 닫는중괄호로 끝나면 레코드가 완료된걸로 판단함
	
	그리고 만약 Json배열로 포장된 데이터라면 
	  [{...},{...},{...}]
	JsonItemReader를 사용할수있음
	이건 resource에 json의 소스를 지정하고(파일,url등 다양한걸 지원)
	  .resource(new UrlResource("http://kill-batch-system.com/array_system_death.json"))
	JsonObjectReader구현체를 선택할수있음  
	  Jackson: JacksonJsonObjectReader
	  Gson: GsonJsonObjectReader
	즉
		@Bean
		@StepScope
		public JsonItemReader<SystemDeath> systemDeathReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new JsonItemReaderBuilder<SystemDeath>()
					.name("systemDeathReader")
					.jsonObjectReader(new JacksonJsonObjectReader<>(SystemDeath.class))
					.resource(new FileSystemResource(inputFile))
					.build();
		}
	이런식으로 오브젝트리더와 리소스위치를 넣어주고 만들어서 쓰면 알아서해줌
  2.JsonFileItemWriter	
	비슷하게 JsonFileItemWriter를 사용하면 객체를 json배열로 변환해 파일로 저장할수있음
	얘는 내부적으로 JsonObjectMarshaller를 사용해 객체를 json문자열로 변환함
	얘는 
	  WritableResource: JSON 데이터를 저장할 대상 파일을 나타내는 Spring의 WritableResource.
	  JsonObjectMarshaller: 객체를 JSON 형식으로 마샬링하는 JSON 객체 변환기
	    Jackson: JacksonJsonObjectMarshaller
	    Gson: GsonJsonObjectMarshaller
	이렇게 얘도 저장위치와 json매퍼 두가지만 넣어주면됨 
		@Bean
		@StepScope
		public JsonFileItemWriter<DeathNote> deathNoteJsonWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new JsonFileItemWriterBuilder<DeathNote>()
					.jsonObjectMarshaller(new JacksonJsonObjectMarshaller<>())
					.resource(new FileSystemResource(outputDir + "/death_notes.json"))
					.name("logEntryJsonWriter")
					.build();
		}
	이렇게 생성하면되고
	
4.데이터베이스 배치
*1.rdbms 읽고쓰기	
  웹의 경우는 대부분 단일레코드를 조회하는게 일반적이지만,배치의 경우 모든 사용자데이터,특정조건을 만족한 데이터등 대량처리가 일반적임
  그래서 oom을 막기위해 
    SELECT * FROM users WHERE created_at >= '2024-01-01'
  이렇게 단순한 조회가 아닌 페이징이나 커서기반의 처리를 하게됨
  
  1.Spring Batch의 두 가지 생존 전략
    커서기반처리는 JdbcCursorItemReader 를 사용함,특징은
	  데이터베이스와 연결(커넥션)을 유지하면서 데이터를 순차적으로 가져옴
	  하나의 커넥션으로 데이터를 스트리밍하는거처럼 처리
	  메모리를 최소한으로 사용하면서 최대성능을 뽑아냄
	가 있고
	페이징 기반 처리는 JdbcPagingItemReader를 사용함,특징은
	  데이터를 정확한 크기로 잘라서 차근차근처리
	  각 페이지마다 새쿼리를 날려 안정성을 보장함
	이런특징이 있음

  2.JdbcCursorItemReader
    커서기반처리는 db와 커넥션을 유지한채 데이터를 resultSet으로 순차적으로 가져오는방식임
	즉 커서를 한칸씩 내리면서 읽는느낌
	이건 메모리사용량은 최소화할수있지만,db커넥션은 유지되므로 긴 배치작업동안 커넥션이 너무 오래 유지된다는 단점이 있음
	
	얘는
	  dataSource //db연결관리
	  sql //쿼리
	  RowMapper //ResultSet->자바객체변환
	  PreparedStatement //쿼리실행 및 결과조회
	  PreparedStatementSetter //파라미터 동적바인딩,옵셔널
	대략적으로 이렇게 구성됨
	
	dataSource는 db의 접속통로로,application.yaml에 적힌값을 자동으로 주입받아서 사용됨
	sql은 데이터조회에 사용할 쿼리
	
	RowMapper는 결과데이터를 객체로 변환하는,LineMapper와 비슷한애임
	스프링배치에서 RowMapper를 다루는 방법은
	  BeanPropertyRowMapper:setteer기반 매핑,자바빈객체에 db칼럼명과 필드명이 일치하면 자동매핑
	  DataClassRowMapper:data클래스나 record같은 불변객체를 위해 설계된 구현체,생성자로 매핑하고 생성자가 없는건 setter로 넣음
	  Custom RowMapper:복잡한 변환로직이 필요할때 직접 구현
	이렇게 3개로 나뉨
	
	PreparedStatement는 쿼리실행기로,이걸로 sql을 실행하고 그 결과를 ResultSet으로 가져옴
	PreparedStatementSetter는 동적으로 파라미터값을 주입할때 사용됨,기본값은 ArgumentPreparedStatementSetter로,?위치에 순서대로삽입함
  
    1.JdbcCursorItemReader 구성하기
 	  이것도 빌더로 빈만들면됨
		@Bean
		public JdbcCursorItemReader<Victim> terminatedVictimReader() {
		  return new JdbcCursorItemReaderBuilder<Victim>()
			.name("terminatedVictimReader")
			.dataSource(dataSource)
			.sql("SELECT * FROM victims WHERE status = ? AND terminated_at <= ?")
			.queryArguments(List.of("TERMINATED", LocalDateTime.now()))
			.beanRowMapper(Victim.class) //목표객체클래스,빈이라서 beanRowMapper를 사용했고,커스텀이라면 rowMapper를 사용
			.build();	
	  이렇게 그냥 그대로 넣어주면됨
	  여기서 dataSource는 클래스레벨로 DI받은다음 사용하면됨(DataSource dataSource)
	  만약 ResultSet을 데이터클래스나 Record를 사용하려면 DataClassRowMapper를 사용하면됨 
	    .rowMapper(new DataClassRowMapper<>(Victim.class)) //배치 5.2 이전
	    .dataRowMapper(Victim.class) //5.2이후
	  이렇게

    2.JdbcCursorItemReader: 데이터 가져오기, 정말 한 행씩일까?
      기본적으로 보이기는 한행씩 db에서 값을 가져오는거처럼 보이지만,여러개의 row를 미리 가져와서 내부버퍼에 저장해두고 next마다 하나씩 밀어주는방식임
	  그러다가 다떨어지면 다시 db가서 가져오고
	  여기서 주의점은,mysql의 경우엔 
	    useCursorFetch=true
	  를 설정하지않으면 조회결과를 전부 메모리에 적재하니 반드시 저걸 설정해서 분할로딩해야함
	  
	  기본적으로 fetchSize만큼의 값을 한번에 가져옴(이거자체는 힌트로 꼭 저거랑 똑같이 가져오진않음,대충 비슷하게자져옴)
	  이건 빌더의 fetchSize() 설정으로 값을 조정할수있음(FlatFileItemReader도 BufferedReader()로 똑같이 조절가능)
	
    3.커서 연속성
      청크처리마다 트랜잭션이 새로 시작되고 커밋되지만,커서는 Step트랜잭션과 별도의 커넥션을 사용하기때문에 괜찮음
	  그래서 스탭커밋의 영향을 커서는 받지않고,처음열렸을때 상태 그대로 데이터를 끝까지 읽을수있음

    4.스냅샷읽기
      아이템리더가 조회하는 데이터를 스텝에서 변경했을때도 아이템리더는 그 변화를 볼수없음
	  즉 처음 조회했던값을 그대로 계속 읽는게 보장됨
	  그래서 변경을 신경쓰지않고 전체처리를 할수있음

    5.JdbcCursorItemReader의 SQL ORDER BY 설정
      ORDER BY는 스탭이 실패했을때를 대비해서 꼭 설정하는게 좋음
	  이래야 고정된 순서로 처리가 가능해서,스탭이 실패했을때 해당 위치로 가서 처리를 할수있고,
	  만약 Order by가 없다면 매번 순서가 달라질 가능성이 있어서 이전 실패위치라고 보장할수없음(누락 및 중복)
	  그래서 pk같은걸로 Order by 해두는게좋음

  3.JdbcPagingItemReader
	이건 데이터를 페이지 단위로 나눠서 읽어오는 아이템리더임
	이건 Keyset 기반의 페이징을 수행함
	
	페이징엔 두가지방식이 있는데
	  offset:db가 결과셋을 정렬하고 offset만큼 건너띄고 limit갯수만큼 데이터를 가져옴
	    정렬하고 가지고있어야한다는 문제때문에 db메모리를 많이사용하고,페이지번호가 뒤로갈수록 성능이 저하됨
		  SELECT * FROM victims ORDER BY id LIMIT 10 OFFSET 20
		이런식
	  keyset:이전 페이지의 마지막키를 기준으로 그 다음데이터를 가져옴
	  정확히 이전에 가져온 마지막값 이후부터 읽어오므로 성능이 일정하게 유지됨
	    SELECT * FROM victims WHERE id > 1000 ORDER BY id LIMIT 10
	  만약 uuid를 사용한다면 생성시간과 uuid를 둘다 사용하고,복합인덱스를 걸수있음
	
	1.JdbcPagingItemReader 해부
	  얘도 별차이없음
	    dataSource
		rowMapper
		NamedParameterJdbcTemplate  //sql실행+파라미터바인딩
		PagingQueryProvider //쿼리생성 및 페이징전략,db별 sql최적화
	  이렇게 구성됨
	  데이터소스와 rowmapper은 커서와 동일함
	  
	  NamedParameterJdbcTemplate 는 :abc처럼 이름있는 파라미터를 사용해서 값을 삽입할수있게해줌
	  PagingQueryProvider 는 페이징을 위한 쿼리생성도구인터페이스고, 각 dbms종류에 맞는 페이징구현체를  제공해줌(MySqlPagingQueryProvider)
	  그래서 기본적으론 쿼리설정메서드를 사용하면되고,커스텀할필요가 있다면 커스텀을 사용할수있음(커스텀을 쓰면 쿼리설정메서드값은 무시됨)
		selectClause(): 가져올 컬럼을 지정한다
		fromClause(): 데이터를 가져올 테이블을 지정한다
		whereClause() (선택): 필요한 데이터만 필터링한다
		groupClause() (선택): 데이터 집계가 필요한 경우 사용한다
		sortKeys(): ORDER BY 절에 사용될 정렬 키를 지정한다. 
		  이는 keyset 기반 페이징의 핵심으로, ORDER BY 절 뿐만 아니라 keyset 기반 페이징을 위한 WHERE 절 조건에도 사용된다. 
		  이 키는 반드시 유니크한 값이어야 한다. 
		  정렬 키가 유니크하지 않으면 동일한 값을 가진 데이터들의 순서가 보장되지 않아 일부 데이터가 누락되거나 중복될 수 있다. 
		  그래서 보통 PK나 인덱스가 있는 컬럼을 사용한다.	  
	  ketset페이징 특성상 처음호출쿼리와 그이후쿼리는 다른데,처음은 정렬조건과 limit만 필요하지만,
	  두번째부턴 이전페이지의 마지막보다큰 이라는 조건이 들어가야하기때문 
	  그래서 내부적으로 firstPageSql과 remainingPagesSql같은 필드들로 이걸 구분함(알필욘없음)
	
	2.JdbcPagingItemReader 구성하기
	이건 데이터소스와 로우매퍼는 동일하고,페이지사이즈를 설정해줘야하고 sql대신 PagingQueryProvider설정이 들어가는게 다름
		@Bean
		public JdbcPagingItemReader<Victim> terminatedVictimReader() {
			return new JdbcPagingItemReaderBuilder<Victim>()
					.name("terminatedVictimReader")
					.dataSource(dataSource)
					.pageSize(5)
					.selectClause("SELECT id, name, process_id, terminated_at, status")
					.fromClause("FROM victims")
					.whereClause("WHERE status = :status AND terminated_at <= :terminatedAt")
					.sortKeys(Map.of("id", Order.ASCENDING))
					.parameterValues(Map.of(
							"status", "TERMINATED",
							"terminatedAt", LocalDateTime.now()
					))
					.beanRowMapper(Victim.class)
					.build();
		}	
	이렇게 select절과 from절,(where절과 group절)을 넣은다음 sortKeys로 키를 지정해주면됨
	이렇게 3개만 지정해주면,현재 db에 맞는 구현체를 자동으로 생성해서 페이징쿼리를 구성해줌
	커스텀 PagingQueryProvider를 사용하려면 queryProvider()에 직접 넣어주면됨
	
	그리고 .parameterValues()로 쿼리파라미터를 채워주면됨
  3.JdbcBatchItemWriter
    JdbcBatchItemWriter는 가장 기본적인 rdbms 쓰기도구임
	이건 NamedParameterJdbcTemplate를 사용하고,jdbc템플릿의 batchUpdate를 사용해서 청크단위 아이템을 효율적으로 저장함
	즉 벌크인서트로 저장함
	이때 배치쿼리는 하나의 트랜잭션내에서 수행되고,한청크가 전부 성공하거나 전부실패함
	
	mysql과 PostgreSql은 
	  # MYSQL
	  url: jdbc:mysql://localhost:3306/mysql?rewriteBatchedStatements=true 
	  #POSTGRESQL
	  url: jdbc:postgresql://localhost:5432/postgres?reWriteBatchedInserts=true 	
	설정으로 벌크인서트를 킬수있음
	
	1.JdbcBatchItemWriter 해부
	  얘는 이렇게 구성됨
	    NamedParameterJdbcTemplate:sql실행엔진,네임드파라미터바인딩을 지원
		sql:실행할 sql구문(insert,update,delete등)을 정의함,네임드파라미터 사용가능
		ItemSqlParameterSourceProvider or ItemPreparedStatementSetter:
		  ItemSqlParameterSourceProvider는 자바객체를 sql네임드파라미터에 매핑(userName은 :userName에 매핑)
		  ItemPreparedStatementSetter 는 자바객체의 데이터를 ?에 매핑
	  즉 쿼리를 받고,청크의 모든값을 넣은 벌크처리 sql로 변환한다음 그걸 한번 던져서 성공 or 실패를 받는식임
	
	2.JdbcBatchItemWriter 빌더
	  이건
		@Bean
		public JdbcBatchItemWriter<HackedOrder> orderStatusWriter() {
			return new JdbcBatchItemWriterBuilder<HackedOrder>()
					.dataSource(dataSource)
					.sql("UPDATE orders SET status = :status WHERE id = :id")
					.beanMapped()
					.assertUpdates(true)
					.build();
		}
	  이런식으로 구성됨
	  beanMapped()는 빈기반으로 매핑하겠다는거고,
	  assertUpdates()는 기본값이 true고,하나라도 업데이트나 실패하면 바로 예외던져서 트랜잭션실패처리,
	  false면 일부데이터가 업데이트되지않아도 계속 진행함
	  중복데이터처리할때나 조건부update를 할땐 false를 걸어야함
	
  4.JPA ItemReader / ItemWriter
	implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
	를 추가하면 jpa를 사용할수있음(web에서 쓰던 그거맞음)
	
	1.JpaCursorItemReader
	  이건 커서기반 jpa 아이템리더임
	  이게 JdbcCursorItemReader와 다른점은 내부적으로 entityManager을 통해 데이터를 읽는다는게 다름
	  그래서 sql이 아닌 엔티티중심의 처리가 가능해짐
	  
	  1.JpaCursorItemReader 해부
	    얘는
		  querySting(jpql)  or JpaQueryProvider:데이터조회쿼리
		  EntityManager :핵심컴포넌트,엔티티의 생명주기 관리 및 실제 db작업을 수행함
		  query:실행가능한 쿼리인스턴스,얘를통해 데이터를 스트리밍으로 읽어옴
		    하이버네이트꺼는 좀 유도리있게 스트리밍하는데,기본구현은 전체데이터를 메모리에 한꺼번에 로딩하니까 스트리밍이 아니고,그래서 주의가 필요함
		로 구성됨
	  2.JpaCursorItemReaderBuilder
		이건
			@Bean
			@StepScope
			public JpaCursorItemReader<Post> postBlockReader(
					@Value("#{jobParameters['startDateTime']}") LocalDateTime startDateTime,
					@Value("#{jobParameters['endDateTime']}") LocalDateTime endDateTime
			) {
				return new JpaCursorItemReaderBuilder<Post>()
						.name("postBlockReader")
						.entityManagerFactory(entityManagerFactory)
						.queryString("""
								SELECT p FROM Post p JOIN FETCH p.reports r
								WHERE r.reportedAt >= :startDateTime AND r.reportedAt < :endDateTime
								""")
						.parameterValues(Map.of(
								"startDateTime", startDateTime,
								"endDateTime", endDateTime
						))
						.build();
			}	
		이런식으로 사용됨
		엔티티매니저는 클래스레벨에서 di받으면되고, queryString에 있는거를 파라미터를 넣어서 조회하고 현재 제네릭타입의 객체에 넣어서 리턴해줌
		queryString말고 다른방식도 있는데,JpaQueryProvider 를 사용하면 동적쿼리생성등 유연한처리가 가능함
		
		기본적으론 
		  JpaNamedQueryProvider:엔티티에 미리 정의된 네임드쿼리를 사용
		  JpaNativeQueryProvider:네이티브sql로 데이터 조회
		이렇게 두개의 구현체를 기본제공해주는데,필요하다면 커스텀을 만들수도있음
		보통 동적쿼리할떄나 쿼리힌트제공할때 사용했는데,쿼리힌트는 5.2부터 .hintValues()를 사용할수있음
		  .hintValues(Map.of("org.hibernate.fetchSize", 100))
		이렇게
		
	2.JpaPagingItemReader
	  이건 offset기반으로 페이징을 수행함(limit와 offset절사용)
	  
	  1.데이터정합성붕괴
	    문제는 실시간 데이터변경에 취약하다는것
		  페이지1번을 읽고 새데이터가 맨앞에 추가되면 마지막하나 중복처리
		  페이지1번을 읽고 하나 삭제하면 다음페이지거 하나 누락됨
		일반적으로 고정된 입력 데이터셋을 대상으로 하면 괜찮지만 아닐경우 문제가될수있음
		이럴땐 offset기반 페이징은 쓰지않는게좋음
	
	  2.성능문제
	    아까말했던거처럼 매 페이지 요청마다 db는 전부 메모리에 올려야하니 문제가됨
		또한 db 메모리 사용량도 많이커짐
		그래서 작은 데이터셋에선 괜찮은데,좀 크다면 안쓰는게좋음
	
	  3.JpaPagingItemReader 해부
	    이건
		  queryString
		  EntityManager
		이렇게 JpaCursorItemReader와 별차이없지만,데이터를 읽는방식에서 차이가 있음
		커서기반은 초기화시점에 쿼리를 한번 실행하고 순차조회하지만,얘는 페이지별로 계속 읽음
		그리고 pageSize로 페이지크기 설정해주면됨
		
		또한 여기서도 orderby는 필수임
		
		또한 JpaPagingItemReader에서는 페치조인을 사용할수없음(커서는 상관없음)
		하이버네이트에서 페치조인과 페이징을 같이쓰면 쿼리에서 페이징하지않고 전체데이터를 로드한후 어플리케이션에서 페이징해버림(oom)
		
		그런데 문제가 있는데,n+1임		
		배치에서는 FetchType을 Eager로 설정하고 @BatchSize로 데이터를 가져오는식으로 n+1을 완화시켜야함
		JpaPagingItemReader 트랜잭션처리로직상 Lazy에선 @BatchSize가 적용되지않아서 eager로 설정해야함
		트랜잭션 범위때문에 생기는문제임
		이건 transacted 필드가 true일때 발생하는데,추가적인 문제가 더있음
		만약 이전청크의 itemProcessor에서 엔티티를 수정했다면,읽기를 하는게 더티체킹돼서 다시 변경될수있음
		그래서 빌더에서
		  .transacted(false)
		이렇게 설정해두고 쓰는게좋음
		
		이때 주의할건 지연로딩이 불가능해지니까 eager로 미리로드해두는게좋음
		배치는 eager가 더 맞기도 함,할일이 정해져있으니까
		그리고 연관관계엔티티가 실제로 필요하지않다면 그냥 jdbcPagingItemReader가 더 나음
		
		offset기반 문제를 해결하고 트랜잭션관리도 개선하고 querydsl까지 도입한 커스텀 구현체가 있는데
			jojoldu/spring-batch-querydsl
		조졸두아저씨꺼있음 가져다쓰면됨
	3.JpaItemWriter
	  implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
	  를 하면 PlatformTransactionManager의 구현체가 
	  기본 DataSourceTransactionManager 대신 JpaTransactionManager가 자동으로 사용됨
	  jpa기능(영속성컨텍스트,1차캐시등)을 사용하기위해서 바꾸는거
	  
	  JpaItemWriter는 넘겨받은 엔티티를 영속성컨텍스트에 넣고 db에 넣는일만 함
	  구성도
		@Bean
		public JpaItemWriter<BlockedPost> postBlockWriter() {
			return new JpaItemWriterBuilder<BlockedPost>()
					.entityManagerFactory(entityManagerFactory)
					.usePersist(true) //persist와 merge선택
					.build();
		}
	  이렇게 간단함
	  entityManagerFactory를 주입하고 persist를 사용할지 merge를 사용할지만 결정해주면됨,이건 어떤식으로 엔티티를 저장할지임
	  persist는 새 데이터를 추가할때,merge는 기존데이터를 수정할때 사용하면됨
	  
	  이것도 로그상에는 하나씩 처리되는거처럼 보일수있는데,배치업데이트(벌크인서트)로 처리됨(reWriteBatchedInserts=true)
	  
	  하이버네이트는 시퀀스를 기본값 50개정도씩 할당받아서 사용함(id생성할떄 GenerationType.SEQUENCE)
	  이때 db도 숫자를 맞춰줘야함
	    ALTER SEQUENCE blocked_posts_id_seq INCREMENT BY 50;
	  그리고 다른어플리케이션에서 이테이블을 사용중이라면,allocationSize 을 동일하게 맞춰줘야함
	  
	  이게 persist()방식임
	
	4. IDENTITY 전략 사용 시 배치 처리 제약사항
	  GenerationType.IDENTITY는 엔티티id가 db에서 생성되기때문에,영속화를 하려면 반드시 insert가 먼저 실행되어야함
	  그래서 모든 인서트가 개별실행되어야한다는걸 의미하는데,그래서 배치처리가 불가능해짐
	  그래서 성능이 중요하다면 IDENTITY대신 SEQUENCE를 사용하는게 좋음
	  
	5.merge()	
	  이건 기존데이터를 수정할때 사용되는데,주의점이 있음
	  merge()호출시 영속성컨텍스트는 해당 데이터의 존재여부를 알수없어 db조회를 강제하게되는데,그래서 청크단위처리마다 셀렉트쿼리가 청크크기만큼 추가로 발생함
	  
	  그거말곤 그냥 해당엔티티를 뱉는다고 생각하고 쓰면됨
	  
*2.nosql읽고쓰기
  몽고디비는
    도큐먼트:기본데이터단위,bson으로 저장
	컬렉션:도큐먼트의 그룹,테이블이라고 보면됨
	데이터베이스:컬렉션을 그룹화한 최상위컨테이너,rdb의 데이터베이스와 같음
  로 구성된 기본적인 nosql임
  얘는
    스키마정의가 없어서 도큐먼트구조를 자유롭게 변경가능
	수평확장이쉽고 샤딩을 지원
	대량데이터처리를 쉽게할수있음
  이런 특징이 있음
  얘를 쓸땐
    implementation 'org.springframework.boot:spring-boot-starter-data-mongodb'
  를 넣어줘야함
  
  1.MongoCursorItemReader
    얘도 커서기반의 데이터순차접근방식임
	얘는
	  MongoTemplate:핵심엔진,이걸사용해 데이터를 스트리밍조회함
	  Query :조회할 쿼리를 지정,다른커서리더처럼 얘도 초기화시점에 쿼리를 한번 생성하고,이를기반으로 커서를 가져옴
	  Cursor:read()가 호출될때마다 커서를 사용해 데이터를 하나씩 순차반환
	로 구성되어있음  
	
	얘는
		@Bean
		@StepScope
		public MongoCursorItemReader<SecurityLog> securityLogReader(
				@Value("#{jobParameters['searchDate']}") LocalDate searchDate
		) {
			Date startOfDay = Date.from(searchDate.atStartOfDay(ZoneId.systemDefault()).toInstant());
			Date endOfDay = Date.from(searchDate.plusDays(1).atStartOfDay(ZoneId.systemDefault()).toInstant());

			return new MongoCursorItemReaderBuilder<SecurityLog>()
					.name("securityLogReader")
					.template(mongoTemplate) //DI받은 몽고템플릿
					.collection("security_logs")//읽어올 컬렉션명
					.jsonQuery("""   //쿼리
					{
						"label": "PENDING_ANALYSIS",
						"timestamp": {
							"$gte": ?0,
							"$lt": ?1
						}
					}
					""")
					.parameterValues(List.of(startOfDay, endOfDay)) //파라미터
					.sorts(Map.of("timestamp", Sort.Direction.ASC)) //정렬
					.targetType(SecurityLog.class)
					.batchSize(10)  //변환타입
					.build();
		}
	이런식으로 구성됨
	이거말고도 
	  쿼리의 projection을 위한 fields()
	  쿼리 최적화를 위한 인덱스 힌트를 지정하는 hint()
	  조회할 도큐먼트 수를 제한하는 limit()
	  쿼리 수행 시간을 제한하는 maxTime()
	등등 다양한 메서드들이 있음
	
	1.MongoCursorItemReader의 쿼리 구성, 또 다른 방법
	  Query객체를 직접 사용할수도 있음
	  이러면
	    타입세이프
		ide자동완성가능
		체이닝으로 명확하게 구성가능
	  이런 이점이 있음
	  이건
		  Query query = new Query()
			.addCriteria(Criteria.where("label").is("PENDING_ANALYSIS"))
			.addCriteria(Criteria.where("timestamp")
					.gte(Date.from(searchDate.atStartOfDay(ZoneId.systemDefault()).toInstant()))
					.lt(Date.from(searchDate.plusDays(1).atStartOfDay(ZoneId.systemDefault()).toInstant())))
			.with(Sort.by(Sort.Direction.ASC, "timestamp"))
			.cursorBatchSize(10);
	  이렇게하면됨
	  이때 배치버그가 있는데,query객체에서 정렬조건이 있더라도 아이템리더빌더에서 sorts를 반드시 넣어줘야함
	  이걸 사용하진않는데 없으면 예외띄움

  2.MongoPagingItemReader
    이건 페이징방식 아이템리더임
	이건 skip와 limit를 사용해 페이징을 수행하고,이것도 offset기반 페이징임
	그래서 성능이 당연히 안좋고,특히 몽고db의 샤딩환경에선 처참함
	
	또한 오프셋기반 페이징이라서 프로세서에서 쿼리조건에 해당하는 값을 수정하는경우 계속 결과값이 바뀌어서 문제가됨	
	그래서 이건 실무에서 사용하지않는게 좋고,정 페이징이 필요하다면 아이템리더를 직접 구현해 keyset기반페이징을 해야함
	
	꼭 써야한다면
	  별도필드활용//쿼리조건필드는 절대 건드리지말고 임시필드에만 결과 저장후,배치완료후 일괄업데이트
	  id기반처리
	  스냅샷//임시컬렉션에 처리할거 복사한후에 원본대신 임시컬렉션에서 읽기
	등 우회할수있긴한데,가급적 커서쓰는게나음

  3.MongoItemWriter
    얘는 MongoTemplate을 사용해 청크의 아이템을 추가,수정,삭제함
	얘는 batchUpdate와 비슷한 bulkWrite를 사용함
	이때 각 연산은 순차실행되고,하나가 실패하면 후속작업은 실행되지않음
	
	얘는
	  MongoTemplate //핵심엔진
	  collection  //테이블비스무리
	  mode  //인서트,업서트(기존도큐먼트수정,없으면추가,기본값),리무브를 설정해야함
	로 구성됨
	얘는 뭘할질 미리 정해줘야함
	얘는
		@Bean
		public MongoItemWriter<SecurityLog> securityLogWriter() {
		   return new MongoItemWriterBuilder<SecurityLog>()
				   .template(mongoTemplate)
				   .collection("security_logs")
				   .mode(MongoItemWriter.Mode.UPSERT)  // 기존 문서 수정
				   .build();
		}
	이렇게 간단하게 만들수있음
	이때 업서트는 기본동작은 id를 사용해서 검색하고,없으면 새로생성함
	만약 로직을 추가하려면 5.2.3부터는 primaryKeys()를 사용해서 복합키기반 도큐먼트선택이 가능함
		public MongoItemWriterBuilder<T> primaryKeys(List<String> primaryKeys) {
			this.primaryKeys = List.copyOf(primaryKeys);
			return this;
		}
	단 이때 지정한 필드들의 조합이 반드시 단일도큐먼트를 유니크하게 식별할수있어야함

  4.MongoTransactionManager
    몽고아이템라이터의 벌크인서트에서 하나의 작업이 실패하면 후속작업이 실행되지않는다는건,바꿔말하면 이미 성공한데이터들은 남아있게됨
	즉 트랜잭션이 되지않는다는거
	
	만약 이때 이미 성공한애들을 롤백하려면 MongoTransactionManager를 스텝의 트랜잭션매니저로 별도구성해줘야함
	단 이때 주의할점은
	  MongoDB의 버전이 4.0이상이어야하고,
	  샤드클러스터나 레플리카셋환경에서만 지원함,즉 단일몽고에서 지원하징낳음
	  이건 자동빈등록안되니까 수동구성해야함
	이런 문제가 있음
	
	만약 단일환경일경우엔 파일라이터에서 쓰던 버퍼링을 사용해서 처리할순있음
	그렇지만 이건 위험도를 감소시키는거지 이미 쓰여진 데이터를 원자적으로 롤백해주진못함
	또한 MongoTransactionManager를 빈으로 정의할땐 배치메타데이터 트랜잭션매니저와 분리를 해야해서 별도구성이 필요함

  5.RedisItemReader
    5.1부터 레디스전용 아이템리더와 라이터를 지원함
	RedisItemReader는 scan을 사용하는 일종의 커서기반 아이템리더임
	
	그런데 스캔특성상 다른 커서랑 차이가 있음
	scan은 조회대상키들의 목록만을 반환하고,실제값을 얻을땐 get명령이 필요함
	이게 다른점임
	또한 scan은 스트링타입전용명령이고,다른타입은 HSCAN, SSCAN, ZSCAN등을 사용한 커스텀아이템리더를 직접 구현해야함
	
	얘의 동작은
	  초기화시점(open())에 scan을 최초호출해 커서를 생성함,커서엔 조회대상키목록이 담겨있고,그것도 전체가 아닌 일부키만 담겨있음
	  read로 커서로부터 키를 하나씩 받아와서 get함
	  커서의 키가 다 소모되면 추가스캔으로 새 키목록을 확보후 반복
	이런식으로 동작함
	이래서 문제가 있는데
	  아이템 하나당 get 한번을 하기때문에 청크크기만큼 get이 날아감
	    인메모리 db특성상 괜찮긴한데 네트워크왕복시간때문에 조회시간이 길어질수있고,레디스에도 부담
	  레디스 특성상 scan이 중복된 키를 반환할수있고,이거처리는 어플리케이션에서 해야함
	    즉 사소한 중복이 문제가 되지않거나(통계),별도의 중복처리방법이 없다면 반드시 멱등한 배치에서만 사용해야함
	  재시작불가
	    스캔은 순서를 보장하지않아서 재시작이 불가능함(order by 없으니까)
	이런문제들이 있음
	
	얘의 구성은
	  RedisTemplate  //핵심엔진
	  Cursor //스캔의 결과물,키가 다떨어지면 내부적으로 추가스캔함
	  ScanOptions //스캔할때 필요한 설정정보가 있음,key패턴이나 한번에 몇개가져올지등
	이렇게 구성되고,실제 빌더는
		RedisItemReader redisItemReader = new RedisItemReaderBuilder<String, AttackLog>()
			.redisTemplate(template)
			.scanOptions(ScanOptions.scanOptions()
				.match("attack:*")  //키패턴
				.count(10)  //가져올갯수
				.build())
			.build();	
	이렇게 하면됨
	단 여기서의 카운트는 힌트라서 엄격하지않음
	
	그리고 레디스템플릿을 기본제공해주는걸 써도되지만,json문자열형태를 객체로 변환할필요가 있다면 직접 생성해야함
	Jackson2JsonRedisSerializer를 별도로 구성해줘야하기때문
	그리고 match()는 저 패턴을 만족하는 모든 키를 대상으로 삼겠다는거
	추가적으로 레디스아이템빌더는 재시작이 불가능하기때문에 이름이없음
		
	추가적으로 배치에서 청크지향처리를 할때,전체데이터에 대한 집계나 요약정보가 필요할땐 별도의 집계컴포넌트가 필요함
	이떄 JobExecutionListener 를 상속받은 클래스를 만들어서 모으는식으로 집계나 요약정보를 생성할수있음
	이걸 잡빌더에서
		.listener(attackCounter)
	로 등록하면됨
	
	그리고 RedisItemReader는 value만 반환하는데,만약 키의 값이 필요할경우가 있음
	현재기준으로는 이걸 사용할수없지만 미래에 수정될예정같음

  6.RedisItemWriter
    이건 청크의 각 아이템을 하나하나 추가하거나 삭제함
	별도의 최적화없이 단순히 한건씩 처리함
	또한 얘도 String타입의 데이터만 쓰기를 지원함
	얘의 빌더는
		@Bean
		public RedisItemWriter<String, AttackLog> deleteAttackLogWriter() {
			return new RedisItemWriterBuilder<String, AttackLog>()
					.redisTemplate(redisTemplate)
					.itemKeyMapper(attackLog -> "attack:" + attackLog.getId())
					.delete(true)
					.build();
		}
	이렇게 됨
	여기서 .delete(true)로 두면 삭제모드로 동작하고(키를 추출해 해당키를 삭제),
	false로 두면 키를 추출해서 키로쓰고,AttackLog객체를 value로 삽입함

  7.RepositoryItemReader
    이건 레포지토리추상화를 가지고 동작하는 아이템리더임
	근데 문제는 얘는 페이징방식으로 오프셋으로 동작함
	그래서 데이터가 많아지면 성능이슈발생가능
	얘는
		public RepositoryItemReader<HackNote> reader() {
		return new RepositoryItemReaderBuilder<HackNote>()
				.name("hackNoteItemReader")
				.repository(hackNoteRepository)
				.methodName("searchNotesByMessageAndSentimentIsNull")
				.arguments(List.of("PWN"))
				.pageSize(10)
				.sorts(Map.of("timestamp", Sort.Direction.DESC))
				.build();
		}
	이렇게 레포지토리를 넣고 검색메서드를 넣어주고 파라미터넣고 페이징사이즈 넣고 정렬넣고 쓰면됨

  8.RepositoryItemWriter
    이건 라이터라서 기본적으로는 saveAll을 사용해서 벌크연산을 사용하고,
	methodName()을 사용해 다른메서드를 사용할수있긴한데 이런경우엔 아이템마다 개별호출되니까 주의
	가급적 기본값쓰는게나음
	
	빌더는
		@Bean
		public RepositoryItemWriter<HackNote> logWriter() {
			return new RepositoryItemWriterBuilder<HackNote>()
					.repository(hackNoteRepository)
					.build();
		}
	진짜 레포만 넣으면되고,필요하다면 중간에  .methodName("save") 같은걸 넣어서 메서드지정하면됨
	또한 이렇게 메서드가 진짜 있는지없는지와 파라미터(즉 시그니처)를 RepositoryMethodReference를 사용해서 검증할수있음
	이때 프록시로 검증하기때문에 레포지토리가 final이 아니어야함
	
*3.위임 ItemWriter와 ItemReader	
  CompositeItemReader/CompositeItemWriter를 사용하면 여러 아이템리더를 순차실행할수있고,
  ClassifierCompositeItemWriter를 사용하면 서로 다른 아이템라이터를 선택적용할수있음
  
  1.CompositeItemReader
    이건 여러 ItemReader를 순차적으로 실행하는 리더임
	이건 5.2에서 추가됐고,전용빌더는 없고 직접 생성해서 써야함
		List<ItemStreamReader<Customer>> readers = List.of(
			shard1ItemReader,  
			shard2ItemREader   
		);

		CompositeItemReader<Customer> compositeReader = new CompositeItemReader<>(readers);	
	이렇게 같은타입을 반환하는 아이템리더 리스트를 넘겨주기만하면됨
	
	얘는 이전 아이템리더가 읽을게 없다고 null을 반환하면 자동으로 다음 아이템리더로 넘어가는형식임
	
	이건 db가 여러샤드로 분산되어있을떄 유용함
	별도 샤드관리없이 각 샤드별로 아이템리더만들고 연결하면됨

  2.CompositeItemWriter
    이건 여러 아이템라이터에 같은 데이터를 전달하는거
	즉 여러종류의 db,이메일,알림등을 같이 보내야할때 사용됨
		CompositeItemWriter<Hacker> writer = new CompositeItemWriter<>(
		   List.of(
			   firstWriter,
			   secondWriter,
			   thirdWriter
		   )
		);
	이렇게 하던가,전용빌더를 써도되긴함
		CompositeItemWriter<Hacker> writer = new CompositeItemWriterBuilder<Hacker>()
		   .delegates(List.of(
			   firstWriter,
			   secondWriter,
			   thirdWriter
		   ))
		   .build();	
	이건 여러곳에 값을 보내야하거나,한곳에는 update,한곳에는 insert를 하는등의 작업을 해야할떄도 사용할수있음(백업이나 동기화등)
	
	단 얘는 동작특성상 트랜잭션문제가 생길수있음(전체실패나 전체성공이 되지않을수있음)
	그래서 일단 트랜잭션외부작업은 beforeCommit에서 버퍼링으로 실행하는식으로 처리하긴함

  3.ClassifierCompositeItemWriter
    이건 여러 라이터중에 하나를 선택해서 사용하는거
	write메서드에 청크가 전달되면,각 아이템은 classifier의 규칙에 따라 각 데이터를 위임라이터를 선택해서 걔한테 처리를 맡김
	얘도 당연히 이런구조니까 같은타입의 데이터를 처리해야함
	
	당연히 데이터위임규칙클래스는 직접 작성해야하고 Classifier클래스를 상속해서 classify를 구현해서 사용할수있음
	또한 아이템라이터는 같은 타입의 데이터를 처리해야하지만,인터페이스를 타겟으로 잡고 밑을 세분화시키면 실제 인스턴스타입에 따라 알맞은 대상에게 처리시킬수도있음
	단 이경우엔 SubclassClassifier라는 이미 있는구현체 쓸수도있음
	
	그리고 라이터에선 그냥 저거 등록만 하면됨
		@Bean
		public ClassifierCompositeItemWriter<SystemLog> classifierWriter() {
			ClassifierCompositeItemWriter<SystemLog> writer = new ClassifierCompositeItemWriter<>();
			writer.setClassifier(new SystemLogClassifier(criticalLogWriter(), normalLogWriter()));
			return writer;
		}	
	이렇게
	
	ClassifierCompositeItemReader는 없는데,PatternMatchingCompositeLineMapper처럼 이런식으로 커스텀으로 만들수있음
	
	
5.배치 스탭
*1.ItemStream
  ItemStream은 보통 청크지향처리 컴포넌트들과 파일기반라이터들이 구현하고,얘는
	자원초기화 및 해제
	메타데이터 관리 및 상태추적
  을 담당함
  
  1.자원 초기화 및 해제
    커서기반 아이템리더는 초기화시점에 커서를 생성하고 커서를 닫는건(해제) ItemStream이 담당함
	초기화가 없으면 로직을 시작할수없고,해제가 없으면 메모리누수,커넥션풀누수,파일핸들누수가 생김
	이렇게 자원을 초기화하고 해제하는 작업이 ItemStream 인터페이스의 open()과 close()메서드임
	리더와 라이터들이 구현하니까,여기에 연결을 시작하는것과 닫는 코드를 넣어두면됨
	
	이 open/close메서드는 스탭이 시작할때 오픈을 호출하고,실행을 완료하면 클로즈를 호출함

  2.메타데이터 관리 및 상태 추적
    ItemStream은 배치 스탭의 실행정보를 관리(저장 및 복구)하는 역할도 함
	배치는 항상 실패할수있고,그래서 우리는 처음부터 다시 처리하던가 실패한지점부터 다시 재시작해야함
	이때 처음부터 다시하는건 너무 비효율적이니까,실패지점부터 재처리가 되는게 좋음
	그래서 매 트랜잭션마다 현재실행상태를 메타데이터저장소에 저장하는데,이걸 ItemStream이 담당함
	
	ItemStream의 메타데이터 관리는
	  open()//저장된실행정보 복원,작업을 실패한시점부터 이어서 실행할수있게 상태를 복원함
	  update()//상태저장,현재작업이 어디까지 진행되었는지를 저장함
	두가지 메서드로 관리함
	
	open()은 ExecutionContext를 입력으로 받는데,여기엔 이전스탭실행의 실행정보가 담겨있고,이걸 사용해 이전상태를 복원할수있음
	만약 재시작이라면 여기에 이전실행정보가 있고,처음실행이라면 비어있는상태로 전달됨
	
	파일리더에서는 ExecutionContext에서 maxItemCount(몇개읽을건지)과 itemCount(이전에 몇개읽었는지)를 복원해서 사용함
	그리고 itemCount만큼 jumpToItem()을 사용해 파일의 현재 읽기위치를 이동시킴
	
	jdbcCursorItemReader에서도 똑같이 maxItemCount와 itemCount를 가져오는건 동일하고,
	jumpToItem()이 커서를 해당 위치로 즉시 이동시킴
	추가적으로 사용중인 jdbc드라이버가 ResultSet.absolute()를 지원한다면
	  JdbcCursorItemReaderBuilder.driverSupportsAbsolute(true)
	설정으로 재시작 커서이동속도를 크게올릴수있긴한데
	그러나 JdbcCursorItemReader는 ResultSet.TYPE_FORWARD_ONLY 고정이라 저걸 사용할수없음
	그래서 false(기본값)을 유지하는게 좋고,만약 필요하다면 JdbcCursorItemReader의 openCursor()메서드를 오버라이드해야함
	
	false라면 moveCursorToRow()로 한줄씩 이동함(파일처럼)
	커서가 아닌 JdbcPagingItemReader같은경우엔 바로 페이지번호 바꿔서 알아서 움직임
	
	update()도 ExecutionContext를 파라미터로 받고,여기다가 현재 실행정보를 저장함
	update()는 매 트랜잭션의 커밋 직전에 호출되고,트랜잭션이 롤백되면 호출되지않음
	얘는 
	  currentItemCount//현재까지 읽은 데이터수
	  maxItemCount//읽을수있는 최대 데이터갯수
	를 저장함(오픈에서 사용할거 그대로)
	
  3.재시작불가사례:RedisItemReader
    물론 레디스처럼 재시작을 지원하지않는 아이템리더도 있음
	그런애들은 update메서드를 구현하지않고,오픈클로즈에서 자원관리하고 자원해제만 함

  4.ItemStream의 위임 구조
    위임을 할때 open(),close(),update()메서드는 받은 모든 리더의 해당메서드를 for문돌려서 실행시킴
	이떄 스프링배치 5.2이전버전에선 위임받는 애들의 close중 하나에서 예외터지면 나머지애들이 클로즈되지않는 문제가 있었고,5.2.2에서 수정됨

  5.다른 위임 패턴들의 ItemStream 구현
    CompositeItemWriter, MultiResourceItemReader, MultiResourceItemWriter들도 이런식으로 ItemStream을 구현해서 위임함
	단,ClassifierCompositeItemWriter는 ItemStream을 구현하지않아서 위임하지못함
	그래서 이런경우엔 위임대상의 open(),update(),close()메서드를 배치스탭에서 직접 호출할수있게 별도구성을 해야함
		@Bean
		public Step systemLogProcessingStep() {
			return new StepBuilder("systemLogProcessingStep", jobRepository)
					.<SystemLog, SystemLog>chunk(10, transactionManager)
					.reader(systemLogProcessingReader())
					.writer(classifierWriter())
					.stream(criticalLogWriter()) // ItemStream 구현체 직접 전달
					.stream(normalLogWriter()) // ItemStream 구현체 직접 전달
					.build();
		}
	이렇게 사용할 해당 위임대상들을 stream으로 직접 밀어줘야함
	기본적으론 알아서 자동으로 체크하기때문에 할필요없는데,문제는 @StepScope를 사용할때임
	이러면 stream을 떼버리면 npe가 터지는데,
	배치동작로직상 프록시를 생성하는데,이때 생성된 프록시객체는 ItemStream인터페이스를 아예 구현하지않음
	그래서 해당객체를 스탭의 관리대상 ItemStream에 포함시키지않아서 open(),close(),update()가 호출되지않음
	
	그래서ItemStream을 구현한 컴포넌트에 @StepScope를 적용할떈 반드시 구체클래스를 반환타입으로 잡아야함
	
*2.ItemProcessor 
  아이템프로세서는 비즈니스로직을 넣는곳이고,대부분은 그래서 직접 클래스를 만들어야함
  단 아이템프로세서도 반드시 알아야할 기본동작원리들이 있음
  
  1.null 반환을 통한 데이터 필터링
    ItemProcessor의 process메서드가 null을 반환할경우,해당 item은 itemWriter로 전달되는 청크에서 완전히 제외됨
	이게 ItemProcessor를 통한 데이터필터링임
	즉 그래서 아이템라이터에 전달되는 청크는,리더로부터 받은 청크보다 작거나 같음
	
	보통 유효하지않은 데이터를 제거하거나,처리가 불필요한데이터를 제외하거나,특정조건에 맞지않는 데이터를 제외할때 사용됨
	
	이런경우같은건 ValidatingItemProcessor라는 구현체가 이미 있음
	이건 org.springframework.batch.item.validator.Validator를 사용해 데이터필터링을 수행함
	저건 저 Validator 클래스 안에서 데이터가 검증을 만족하지못하면 ValidationException을 던지게 구현하면됨
	그리고 주의점이,이걸쓸땐 
		@Bean
		public ItemProcessor<Command, Command> commandProcessor() {
			ValidatingItemProcessor<Command> processor = 
				new ValidatingItemProcessor<>(new CommandValidator());
			processor.setFilter(true);  // ValidationException 발생 시 필터링 수행
			return processor;
		}	
	이렇게 setFilter를 true로 해야함,기본값은 false인데 이러면 ValidationException을 잡지못하고 스탭으로 던져서 스탭이 터짐
	그리고 중요한건 스프링의 Validator가 아닌 스프링배치의 Validator을 사용해야한다는것
	물론 스프링의 Validator가 이미 있다면 그걸
	  org.springframework.batch.item.validator.SpringValidator
	를 사용해서 그대로 활용할수있는데,새로만든다면 타입을 신경써야함

  2.데이터 검증을 통한 실패 처리
    setFilter(false)를 두면,문제가 있는 데이터가 들어왔을때 아예 배치를 실패하게만들수도있음
	이게 데이터검증을 통한 실패처리임
	이게 좀 그렇다면,skip을 활용해서 몇개이상의 예외가 발생했을때만 터지게할수도있음

  3.데이터 변환
    ItemProcessor의 핵심은 데이터 변환임
	리더에서 읽어온 원본 데이터는 그대로 쓰기힘든경우가 많고,이걸 가공하는게 ItemProcessor의 주요목적임
	보통
	  주문정보에서 정산에 사용할 데이터만 추출해서 정산객체생성
	  레거시시스템의 데이터를 신규포맷으로 변환
	이런걸 하는거임
	
	ItemProcessor의 타입을 보면
	  ItemProcessor<I,O>
	이렇게 리더에서 받을것과,라이터에 보내줄타입이 두개가 있고,저걸 맞춰주는게 ItemProcessor의 핵심업무임

  4.데이터보강
    읽어온데이터만으로 충분하지않고,외부시스템이나 db에서 추가정보를 가져와 기존데이터를 보강해야할때가 있음
	보통 외부 api등으로 정보를 추가하거나 할때 사용됨
	
	문제는 외부시스템통신을 할땐 성능이슈가 발생할수있음
	각 아이템마다 api호출을 하는데 ItemProcessor는 로직상 아이템별로 처리를 해서 호출횟수를 줄일수가 없음
	그럴떈 ItemWriteListener의 beforeWrite에서 처리를 하는식으로 청크단위로 외부api통신을 묶어서 처리하는식으로 문제를 줄일수있음
	
	물론 ItemWriter에서 처리하는것도 생각해볼수있지만,라이터는 쓰기작업을 처리하는애라서 단일책임원칙이 깨짐

  5.CompositeItemProcessor
    이건 여러 위임대상 아이템프로세서를 순차적으로 실행하는 위임 ItemProcessor임
	각 ItemProcessor는 순차적으로 실행되고,이전 ItemProcessor의 반환값이 다음 ItemProcessor의 입력으로 사용됨
	
	이걸 쓸땐 생성자에 위임대상 ItemProcessor들을 전달하거나,CompositeItemProcessorBuilder.delegates()를 사용할수있음
	
  6.ClassifierCompositeItemProcessor
    이건 여러 ItemProcessor중 하나를 선택하는 방식임
	이떄 주의할점은,이건 순서대로 하는게 아니니까 모든 ItemProcessor들의 타입이 입력과 출력이 리더출력,라이터입력으로 같아야함
	
	
*3.FaultTolerant-견고한 배치 스텝 만들기
  배치에서 실패는 피할수없고,이걸 대응하는게 중요함
  배치의 기본 오류처리동작은,스탭중 하나의 예외라도 발생하면 바로 잡을 실패로 만듬
  그런데 천만개의 데이터처리중 하나가 실패했다고 전부 실패시키는건 비효율적임
  그래서 만약 예외가 허용가능하거나,재시도등으로 복구가능하다면 그렇게 처리하는게 나음
  
  1.청크지향처리의 구조적한계
    문제는 청크지향처리에서 우리는 리더,프로세서,라이터를 스탭에 구성만 할뿐 실제 실행은 스탭이 담당함
	즉 아이템처리중 예외가 발생해도 우리가 직접 개입할수없음
	
	그래서 배치가 내결함성(FaultTolerant)기능을 제공함
	이걸 사용해서 재시도나 건너뛰기등 청크지향처리에서 발생할수있는 다양한 예외상황을 다룰수있음
	
	즉 일시적네트워크오류는 재시도,잘못된형식의 데이터 예외는 건너뛰기로 무시하는등을 할수있음
	
	단,태스크릿처리는 FaultTolerant기능의 대상이 아님
	얘는 우리가 직접 코드내에서 try-catch할수있는형태기때문에 지원하지않음

  2.재시도(retry)
    재시도는 말그대로 실패한작업을 다시 시도하는것
	일시적 네트워크문제같을때 매우 효과적임
	
	1.RetryTemplate
	  내결함성기능을 활성화하면 스탭에 RetryTemplate이 장착되게됨
	  얘는 작업이 실패하면 정해진 정책에 따라 다시 시도하는 컴포넌트임
	  이건
	    canRetry():재시도가능여부를 판단
		retryCallback:우리가 실행할 핵심비즈니스로직,단 재시도때만 사용되는게 아닌 첫시도때부터 이걸로만 호출함
		recoveryCallback:재시도가 불가능하다고 판단될경우(횟수초과등),이게 호출됨
	  이렇게 구조가 나옴
	  핵심은 첫시도때부터 retryCallback을 사용해 처리해서,실패횟수를 셀수있게만든것
	  
	  만약 실패가 발생한다면,retryTemplate의 canRetry를 사용해 재시도가능여부를 확인하고,
	  가능하다면 retryCallback으로 프로세서와 라이터를 재실행함
	
	2.RetryPolicy
	  재시도가능여부판단은 RetryPolicy를 사용해 재시도가능여부를 결정함
	  이건
	    발생한 예외가 사전에 지정된 예외유형에 해당하는지
		현재 재시도횟수가 최대횟수를 초과하지않았는지
	  를 바탕으로 재시도여부가 판단됨
	  
	  단 리더의 경우 재시도가 없음
	  왜없냐면 mutable,즉 읽었을때 데이터가 사라지는 메시지큐같은것도 고려되었기때문
	  문제는 대부분의 경우 unmutable한 데이터소스가 많은데,이렇게되면 불편해서 6.0에선 개선예정임
	
	3.내결함성 최적화 - Input Chunk 재활용
	  롤백이 발생하면 매번 아이템을 다시읽는게 아닌(리더는 애초에 뒤로가기가 없는게 기본값임),
	  내결함성기능이 활성화되면 청크를 버퍼링해뒀다가 그걸 다시넣어서 처리함
	
	4.재시도설정
	  이건
		@Bean
		public Step terminationRetryStep() {
			return new StepBuilder("terminationRetryStep", jobRepository)
					.<Scream, Scream>chunk(3, transactionManager)
					.reader(terminationRetryReader())
					.processor(terminationRetryProcessor())
					.writer(terminationRetryWriter())
					.faultTolerant() // 내결함성 기능 ON
					.retry(TerminationFailedException.class)   // 재시도 대상 예외 추가
					.retryLimit(3)
					.listener(retryListener())
					.build();
		}	  
	  이런식으로 내결함성기능을 킨다음,기본재시도정책(SimpleRetryPolicy)을 사용할떈 예외를 던져주고 반복횟수를 던져주는식으로 킬수있음
	  그리고 이때 상위예외클래스의 특정하위예외만 재시도에서 제외하고싶을떈 noRetry()에 추가하면됨
	  보통 http에러코드를 세분화해서 캐치할떄 사용함
	  
	  그리고 listener()를 사용해,재시도과정을 모니터링할수있음
	  이건 RetryListener을 구현해서 재시도의 전과정을 후킹할수있음
	
    5.내결함성 동작
	  프로세서와 라이터는 RetryTemplate이 같더라도 동작이 다름
	  하나는 아이템기반이고 하나는 청크기반이기때문
	  1.processer  
	    프로세서의 경우 아이템단위로 재시도를 관리하게됨
	    단 이때,이미 처리한 아이템들도 다시 시도를 하게되고,
	    아이템 단위로 재시도컨텍스트가 관리되기때문에 retryCount가 3이라도 한청크내에서 4번이상 재시도를 하는게 가능해짐
	    
	    즉 청크전체가 다시처리되지만,재시도횟수는 아이템단위로 개별관리됨
	    
	    그리고 이미 처리된 아이템들에 대해서 계속 process를 하는게 비효율적이라서,
	    processorNonTransactional() 설정을 제공함
	    이걸 사용하면 아이템프로세서를 비트랜잭션으로 표시해서 한번처리된 아이템의 결과를 캐시에 저장하고,
	    이미 성공한애들은 캐시를 재사용하고 실패한애들만 process를 호출함
	    이건 스탭빌더에서
	      .processorNonTransactional()
	    만 추가해주면됨
	    
	    이걸 사용한다고 ItemProcessor의 실패가 청크트랜잭션에 영향을 안미치는건 아니고,
	    이건 ItemProcessor가 현재 트랜잭션의 상태에 영향을 받지않고 같은 입력에 대해 같은 결과를 반환한다는걸 의미함
	    내결함성기능을 사용할땐 ItemProcessor가 멱등해야하는데,멱등하지않다면 processorNonTransactional()을 사용하라는걸로 나온거임
	    단,이떄 최초처리결과가 재사용되므로 멱등하지않은 로직의 원래의도와 결과가 달라질수있다는걸 유의해야함
	  2.writer
	    라이터에서는 재시도횟수가 청크단위로 관리됨,즉 특정 하나만 재시도할수없음
		또한 라이터에서 예외가 발생하면 프로세서부터 처리가 재개됨(즉 프로세서부터 다시 로직을 돌림)
		
		그리고 아이템단위로 재시도를 체크하지않아서,retryCount가 3일경우 한청크내에서 3을 넘길수없음
	
	6.다양한 재시도정책 적용하기
	  재시도정책은 기본값말고 다양한것들이 있고,직접만들수도있음
	    .retryPolicy(new TimeoutRetryPolicy(Long.MAX_VALUE))//일정시간내에 성공하지못하면 실패
		.retryPolicy(new MaxAttemptsRetryPolicy(Integer.MAX_VALUE))//예외구분없이 재시도횟수기반정책
	  또한 예외별로 다른 재시도정책을 설정할수도있음,ExceptionClassifierRetryPolicy를 사용하면됨
	  
	  그리고 .retryPolicy를 여러개 둘경우,and방식으로 둘다 통과해야 재시도함
	  가급적 헷갈리니까 하나만두는게좋음
	
	7.백오프 정책
	  재시도를 그냥 무지성으로 날리면,안그래도 아파서 힘들어하는 상대서버가 더힘들어함
	  기본값은 NOBackOffPolicy로 실패 즉시 재시도함
	  만약 여기에 간격을 주고싶다면 
		.backOffPolicy(new FixedBackOffPolicy() {{ //1초단위재시도
		  setBackOffPeriod(1000); // 1초
		}})	  
		.backOffPolicy(new ExponentialBackOffPolicy() {{//지수단위재시도
		  setInitialInterval(1000L);  // 초기 대기 시간
		  setMultiplier(2.0);        // 대기 시간 증가 배수
		  setMaxInterval(10000L);     // 최대 대기 시간
		}})	
	  들을 사용할수있음
	  
	  어지간하면 백오프정책을 설정해두는게 매너임
  
  3.Skip
    canRetry()가 false를 반환,즉 재시도가 불가능할경우 별도 구성이 없으면 스탭이 실패함
	그런데 이때 skip를 활성화하면 recoveryCallback 을 통해 예외를 건너뛸수있음
	즉 skip은 recoveryCallback 을 통해 작동하게됨
	
	skip는 개별레코드처리보다 전체작업이 더 중요할때 사용됨
	즉
	  비즈니스적으로 중요도가 낮은 데이터를 처리할때
	  일부 실패 레코드 처리보다 전체배치프로세스 완료가 더 중요할때
	  입력데이터품질이 균일하지않을때
	사용됨
	
	이것도 그냥
		.skip(TerminationFailedException.class)
		.skipLimit(2) // 두 번까지만 참아주겠다 💀
	를 추가하면되고,여기도 SkipPolicy를 기반으로 작동함
	기본값은 LimitCheckingItemSkipPolicy이고,이건 SimpleRetryPolicy와 유사하게 동작함
	지정된 예외타입에 대해 스탭에서 지정된 횟수만큼의 건너뛰기를 허용함
	이건 아이템이나 청크가 아닌,스탭 전체에서 발생한 총 건너뛰기횟수를 카운팅한다는걸 알아둬야함
	커스텀을 쓰려면
	  .skipPolicy(new AlwaysSkipItemSkipPolicy())
	을 쓰면되는데,이걸 쓰고 skip()를 넣으면 기본값과 둘다 동작하니(or로 동작) 주의
	
	1.ItemProcessor에서의 건너뛰기
	  이건 단순하게 건너뛰기가 가능하다면 null을 반환하고,그렇지않다면 예외를 던짐
	  
	  필터링과 건너뛰기는 동작상 동일하지만,배치는 이 두경우를 구분해서,필터의 경우는 필터카운트에,건너뛰기는 스킵카운트에 각각 기록해서 
	  스탭실행결과를 추적할때 구분할수있음
	
	2.ItemWriter에서의 건너뛰기
	  이건 좀 복잡한데,얘는 청크단위로 동작하기때문임
	  그래서 예외 발생시 어떤아이템이 문제인지 즉시 알수없음
	  그래서 배치는 재시도가 불가능해질땐 스캔모드로 돌입하는데,스캔모드는 청크내 아이템을 하나씩 개별처리하는 모드임
	  즉 단일아이템만 쓰는방식임
	  당연히 청크사이즈만큼 쿼리가 나가서 비용이 큼
	  
	  처리데이터품질이 좋지않아 스킵이 빈번하게 발생할거같다면
	    전처리스탭에서 불량데이터를 미리 걸러내기
		ItemProcessor에서 철저한 검증로직으로 문제데이터를 사전필터링
	  하는방식으로 최대한 라이터에서 스킵이 일어나지않게하는게 좋음
	  물론 외부네트워크오류는 어쩔수없지만..
	  
	  또한 스킵이 자주발생할거같지만 데이터 전처리가 현실적으로 어렵다면 스킵을 포기하는것도 고려해볼만함
	  느린배치보단 실패하는배치가 나을수있음
	  
	  또한 여기서도 .processorNonTransactional()을 사용하면 프로세서의 중복실행을 방지할수있음(스캔도 똑같이 캐시결과재사용함)
	
	3.ItemReader에서의 건너뛰기
	  재시도와 달리 리더에서도 건너뛰기는 가능함
	  이건 단순하게 read()중 예외가 발생하면 이 예외를 catch하고 건너뛰기가능하다면 예외를 무시하고 다음아이템을 읽음
	  그래서 청크트랜잭션롤백이 발생하지않고,그냥 무시하기때문에 청크사이즈도 그대로 4라면 4개아이템이 보장됨
	  즉 건너뛴 아이템은 카운트에서 제외됨
	
	4.SkipListener
	  건너뛴 아이템들을 어떻게 추적하고 후속처리할지에 대한 리스너임
	  이건 건너뛴아이템을 추적하고,해당 아이템에 대한 로직을 넣을수있음
	  각 리드,라이트,프로세스에 대한 메서드들이 있고 여기에 넣으면됨
	  
	  단 리드의 경우 읽기가 실패했기때문에 어떤아이템인지 알수없고,예외정보만 전달받을수있음
	  
	  또한 스킵리스너의 호출은 리드든 라이트든 프로세스든 라이터의 청크쓰기 이후(afterWrite)에 일괄적으로 진행됨
	  이것도 6.0에서는 바뀔예정

  4.noRollback()
    이건 해당 예외가 발생했을때,트랜잭션롤백을 방지하는 설정임
	
	1.ItemReader에서의 NoRollback처리
	  여기서는
	    1.건너뛰기가 가능하다면 건너뛰기(건너뛰기를 우선처리함)
		2.불가능하거나 skipLimit를 모두 소진했다면 noRollback판단을 하고,noRollback예외라면 트랜잭션롤백없이 해당예외를 무시하고 다음아이템리드
	  건너뛰기나 noRollback()이나 비슷하지만,통계정보가 스킵카운트에 들어가냐,아예 무시되냐차이가 있음
	
	2.ItemProcessor에서의 NoRollback처리
	  여기서는
	    1.noRollback 예외여부확인후 아니라면 청크트랜잭션롤백
		2.noRollback예외라면 skip을 하는데,만약 스킵설정에 없다면 시스템이 터짐
	  즉 아이템프로세서의 noRollback은 롤백하지말고 건너뛰어라라는 명령이라서,건너뛸수없다면 모순이라서 터지는거
	  즉 반드시 프로세서의 noRollback설정예외는 스킵에도 포함되어야함
	  
	3.ItemWriter에서의 NoRollback처리
	  여기서는 스캔모드의 개별쓰기작업중에만 발생함
	    1.예외가 건너뛰기가능한지를 검사후,건너뛰기가능한 예외라면 건너뛰기를 따름(즉 카운트다떨어졌으면 스텝실패)
		2.건너뛰기 불가능한 예외라면 noRollback 인지 체크후,맞다면 그냥 무시하고 넘어감,즉 롤백도없고 스킵카운트증가도 없이 그냥 없던일이 됨
	  이런식
	
	즉 모든경우에서 스킵은 noRollback 보다 우선순위가 높고,같은예외를 skip과 noRollback 에 둘다 설정하면 건너뛰기만 작동함
	스킵리미트가 소진되어도 noRollback 은 실행되지못함
	
	재시도는 일시적인 오류(네트워크문제)를 극복하는 매커니즘이고
	스킵은 영구적인오류를 무시하고 진행하는 방법임
	
	
*4.스탭 해부	
  상세내용은 스킵하고 쓸만한거만 적음
  내부구현은 코드찾아가던가 다시강의보자
  1.구성요소
    청크지향스탭이든 테스크릿스탭이든 내부적으로는 태스크릿스탭으로 구현돼있음
	태스크릿스탭은 우리가 내용물을 만든거고,청크지향스탭은 배치가 만들어둔 태스크릿스탭구현체를 사용하는거의 차이일뿐임
	
	하나의 객체가 여러타입의 Listener를 구현하는경우엔 리스너를 빌더에 넣으면
	  "error: reference to listener is ambiguous" 
	이런에러가 나는데,이러면 반환타입을 특정인터페이스로 넣어야함
	이때 더 상위 인터페이스로 넣는게 좋음(더 넓은범위를 처리할수있는 타입으로)
	이러면 내부에서 하위거도 구현했는지 확인하고 알아서 넣어줌
	즉
	  StepExecutionListener > ChunkListener > ItemListener
	순으로 넣으면됨
	물론 리더,라이터,프로세서에는 각각 따로 등록해야함
	
	배치의 읽기반복에는 RepeatTemplate를 내부적으로 사용하는데,얘는 CompletionPolicy라는 완료정책을 바탕으로 반복여부를 결정함
	이건 RepeatStatus를 사용해서 반복여부를 결정함
	청크기반도 결국 태스크릿이라서 이것도 마찬가지임
	
	ChunkProvider는 ItemReader를 호출하는 역할을 하고,ChunkProcessor는 ItemProcessor와 ItemWriter를 호출하는 역할을 함
	SimpleChunkProvider에는 ItemReader와 chunkOperations(읽기반복실행조건체크)을 매개변수로 받고,
	SimpleChunkProcessor에는 ItemProcessor와 ItemWriter를 매개변수로 받음
	즉 읽기반복을 시키는건 ChunkProvider가 담당함

  2.실행흐름추적
    스탭은 StepHandler(구현체 SimpleStepHandler)가 실행시킴
	얘는 StepExecution과 ExecutionContext를 생성하고,이를 바탕으로 Step의 execute를 실행함
	또한 여기서 @StepScope에 해당하는 빈들을 받아오는데,이게 스탭이 생성되기전에 받아오기때문에 스텝에 스텝스코프를 붙이면안되는거
	
	그리고 스탭이 실행되면 가장먼저 스텝스코프를 활성화시키고,beforeStep을 실행시키고 itemStream의 open()을 호출한다음
	doExecute로 스탭을 실행시킴
	이 실행된 스탭에서 반복처리를 하고 트랜잭션을 실행한후에 우리가 넣은 스탭구현체를 돌리는거
	이후에 각 실행마다 통계정보를 StepExecution에 누적하는식임
	 
	
5.스프링 배치 정복	
*1.Job과 메타데이터 저장소
  배치를 작성할떈 반드시 중간에 배치가 죽어버렸을때를 대비해야함
  이떄 필요한게
    어떤작업이 실행되었는지 추적
	어디까지 진행되었는지 상태저장
	실패지점부터 재시작
  이렇게 3가지 기능이 필요함
  
  1.메타데이터 저장소
    그래서 배치는 저 3가지를 해결하기위해 메타데이터 저장소를 사용함
	얘는 각 작업의 실행상태,처리항목수,실패지점등을 db에 기록하고 추적할수있음

  2.배치의 핵심 도메인
    1.JobInstance
	  JobInstance는 Job의 논리적 실행단위를 나타냄
	  Job이 설계도(클래스)라면 JobInstance는 실제 객체라고 할수있음
	  
	  즉 같은 잡이라도 잡파라미터에 따라서 다른 잡이 되고(보통 날짜시간등),잡의 이름도 그래서 넣는거임
	  잡이름을 가지고 여러 잡중 하나를 선택할수있게되고,같은 잡 내에서 잡파라미터를 가지고 잡인스턴스를 구분하는식임
	
	2.JobParameters 
	  JobParameters는 배치를 동적으로 실행하기 위한 용도만이 아니라,잡을 서로 다른 잡인스턴스로 구분하는 핵심요소기도 함
	  즉 Job+JobParameters가 하나의 JobInstance를 결정함
	
	3.JobExecution 
	  JobExecution은 잡의 실제 실행 시도를 나타냄
	  JobInstance가 잡의 논리적인 실행을 나타낸다면,JobExecution은 잡이 실제로 실행된 이력을 나타내는거임
	  
	  JobInstance는 여러번 실행될수있음(실패등의 이유로)
	  이걸 각 재시도를 구분하기위한 목적임
	  
	  JobExecution에 포함된 실행정보로는
	    현재상태(BatchStatus):작업이 어떤상태인지(COMPLETED, FAILED, STOPPED등)
		시작시간:Job이 실행을 시작한 시점
		종료시간:Job이 실행을 종료한 시점
		종료코드:Job실행의 최종 결과코드
		실패원인:Job이 실패했을때 실패이유
	  등이 들어있음
	  
	  이때 BatchStatus로는 여러 값을 가질수있는데,
		COMPLETED: 배치 작업이 성공적으로 완료됨
		STARTING: 배치 작업이 실행되기 직전 상태
		STARTED: 배치 작업이 현재 실행 중인 상태
		STOPPING: 배치 작업이 중지 요청을 받아 중지 진행 중인 상태
		STOPPED: 배치 작업이 요청에 의해 중지된 상태
		FAILED: 배치 작업이 실행 중 오류로 인해 실패한 상태
		ABANDONED: 배치 작업이 비정상 종료되어 재시작할 수 없는 상태
		UNKNOWN: 배치 작업의 상태를 확인할 수 없는 불확실한 상태	
	  등이 있음
	4.JobInstance의 재실행 제한
	  배치는 완료된 JobInstance를 재실행할수 없게 제한함
	  즉 배치는 잡을 실행할때마다 같은 JobInstance(Job+JobParamaters)의 실행이력(JobExecution)중
	  BatchStatus가 COMPLETED인게 있는지를 검사함
	  만약 존재한다면 예외를 뱉고 작업재실행을 거부하고,같은게 있어도 COMPLETED가 아니라면 다시 실행할수있음
	
	5.JobParametersIncrementer로 동일 작업 여러 번 실행하기
	  그런데 작업에 따라서 같은걸 여러번 시도해야하는 작업들이 있음(멱등성이 보장되거나,덮어쓰기가 가능하거나 등)
	  이럴떈 JobParametersIncrementer를 사용할수있음
	  이건 JobParamaters에 쓸데없는값들을 추가,수정하여 같은잡을 여러번 실행하게 해주는 컴포넌트임
	  
	  대표적으로 RunIdIncrementer가 있음
	  이건 run.id라는 파라미터값을 자동으로+1하는방식임
	  이건
		@Bean
		public Job brutalizedSystemJob() {
			return new JobBuilder("brutalizedSystemJob", jobRepository)
					.incrementer(new RunIdIncrementer()) // 이 한 줄이 전부다💀
					.start(brutalizedSystemStep())
					.build();
		}
	  이런식으로 사용할수있음
	  
	  단 모든 JobParamater가 식별용도로 사용되는건 아니고,identifying 속성이 true인거만 식별용도로 사용됨
	  기본값으로는 JobParamater는 다 identifying =true임
	  이걸 false로 두는건 로깅수준제어,성능튜닝변수,출력경로지정등의 경우는 false로 두고 사용하게됨
	
	6.Job의 재실행 통제: restartable 설정
	  기본적으로 잡은 실패했을때 재시작이 가능하지만,모든 잡이 재시작가능해야하는건 아님
	  이런상황을 위해 preventRestart()메서드가 있음
		@Bean
		public Job brutalizedSystemJob() {
		  return new JobBuilder("brutalizedSystemJob", jobRepository)
				  .start(brutalizedSystemStep())
				  .preventRestart() // 이 한 줄로 재시작을 막는다 💀
				  .build();
		}
	  이걸 키면 설정된 잡이 재시작불가능하게 만듬
	  이게 설정되면 어떤이유로든 실패후 같은파라미터로 다시 실행할수없으니,적용전에 이 작업을 재시작하면 안되는 명확한 이유가 있을떄만 추가해야함
	  또한 RunIdIncrementer와 이걸 같이쓰면의미없으니까 주의해야함
	
	7.StepExecution
	  StepExecution은 단일 스탭의 실행시도를 나타냄
	  하나의 잡이 여러 스탭을 가질수있으니,하나의 jobExecution은 여러 StepExecution를 포함할수있음
	  StepExecution은 스탭실행시 생성되고,해당스탭이 실제로 시작해야지만 생성됨
	  즉 첫스탭이 실패하면 두번째스탭의 StepExecution은 생성되지않음
	  이건 이런 정보들을 포함함
		현재 상태: Step이 현재 어떤 상태인지를 나타내는 BatchStatus
		읽기/쓰기 카운트: 성공적으로 읽거나 쓴 아이템의 수
		커밋/롤백 카운트: 트랜잭션 처리 횟수
		스킵 카운트: 청크 처리 중 건너뛴 아이템의 수
		시작/종료 시간: Step 실행의 시간적 정보
		종료 코드: Step 실행의 최종 결과 코드
		예외 정보: 실패 시 발생한 오류에 대한 상세 정보
	  여기서도 BatchStatus가 있음
	
	8.JobExecution과 StepExecution의 BatchStatus
	  JobExecution의 최종 BatchStatus는 해당 JobExecution의 마지막 StepExecution의 BatchStatus값을 따라감
	
	9.실패와 재시작 시 StepExecution의 동작
	  잡이 실패후 다시 실행될때 JobExecution이 새로 생성되는거처럼,스탭을 다시 실행할때도 새로운 StepExecution이 생성됨
	  각 실행시도가 독립적으로 추적되어야하기때문
	  또한 이때 실패한Job을 재시작할땐 실패한 Step부터 다시 시작된다는걸 알아야함
	  즉 이미 성공한 Step은 재실행되지않아서 StepExecution이 생성되지않음
	  
	  StepExecution정보는 단순한 로깅 이상의 가치가 있음
	  이걸 사용해서
	    성능모니터링:각 스텝의 처리시간과 처리량을 분석해서 병목지점식별
		오류패턴분석:특정데이터나 조건에서 반복적으로 오류발생하는부분을 파악
		자원할당최적화:읽기/쓰기/처리비율을 분석해서 리소스할당조정
	  을 할수있음
	
	10.ExecutionContext
	  ExecutionContext는 배치작업의 상태정보를 저장하는 데이터컨테이너고,Map형태로 데이터를 저장하고 관리함
	  특히 사용자정의데이터를 넣는데 사용됨
	  또한 이것도 메타데이터 저장소에 영구저장됨
	  
	  ExecutionContext는 Job와 Step 두가지 범위가 있고,실제메타데이터 저장소에서도 물리적으로 분리되어 관리됨

  3.배치 메타데이터 테이블 구조
    배치 메타데이터 테이블은
		BATCH_JOB_INSTANCE
		BATCH_JOB_EXECUTION
		BATCH_JOB_EXECUTION_PARAMS
		BATCH_JOB_EXECUTION_CONTEXT
		BATCH_STEP_EXECUTION
		BATCH_STEP_EXECUTION_CONTEXT
	이 6개의 테이블로 구성됨
	
	JobInstance는 BATCH_JOB_INSTANCE에 저장되고
	이테이블은
	  JOB_INSTANCE_ID  Job 인스턴스의 고유 식별자,PK    
	  JOB_NAME         Job 이름. JobInstance 식별에 반드시 필요             
	  JOB_KEY          JobParameters의 해시값,실제객체에는 존재하지않음                          
	  VERSION          낙관적 락(Optimistic Lock) 버전.JobInstance의 경우 항상 0으로 유지됨                 	
	이렇게 구성되어있음
	즉 실제 JobInstance객체는
	  JobInstance {
	    Long id;             --> JOB_INSTANCE_ID (PK)
	    String jobName;      --> JOB_NAME
	    Integer version;     --> VERSION
	  }
	이렇게 구성됨
	
	JobExecution는 BATCH_JOB_EXECUTION 에 저장됨
	  JOB_EXECUTION_ID  작업 실행의 고유 식별자,PK     
	  VERSION           낙관적 락 버전                                
	  JOB_INSTANCE_ID   연관된 JobInstance의 ID,FK      
	  CREATE_TIME       JobExecution 생성 시간                       
	  START_TIME        JobExecution 시작 시간                       
	  END_TIME          JobExecution 종료 시간                       
	  STATUS            JobExecution 현재 상태(BatchStatus)          
	  EXIT_CODE         JobExecution 종료 코드                       
	  EXIT_MESSAGE      JobExecution 종료 메시지(오류 포함)             
	  LAST_UPDATED      마지막 업데이트 시간                            	
	이렇게 구성되고,JOB_INSTANCE_ID가 외래키로 등록되어있고,이건 1:N관계로 한 JOB_INSTANCE_ID엔 여러 JobExecution이 존재할수있음
	실제 객체는
	  JobExecution {
	  	Long id;                            --> JOB_EXECUTION_ID (PK)
	  	JobParameters jobParameters;        --> BATCH_JOB_EXECUTION_PARAMS 테이블에 저장
	  	JobInstance jobInstance;            --> JOB_INSTANCE_ID (FK)
	  	Collection<StepExecution> stepExecutions; --> BATCH_STEP_EXECUTION 테이블 레코드
	  	BatchStatus status;                 --> STATUS
	  	LocalDateTime startTime;            --> START_TIME
	  	LocalDateTime createTime;           --> CREATE_TIME
	  	LocalDateTime endTime;              --> END_TIME
	  	LocalDateTime lastUpdated;          --> LAST_UPDATED
	  	ExitStatus exitStatus;              --> EXIT_CODE
	  	ExecutionContext executionContext;  --> BATCH_JOB_EXECUTION_CONTEXT 테이블에 저장
	  	List<Throwable> failureExceptions;  --> EXIT_MESSAGE
	  	Integer version;                    --> VERSION
	  }
	이렇게 구성됨
	
	JobParameters는 BATCH_JOB_EXECUTION_PARAMS 테이블에 저장됨
	  JOB_EXECUTION_ID  작업 실행의 ID,FK      
	  PARAMETER_NAME    파라미터 이름                          
	  PARAMETER_TYPE    파라미터 타입                          
	  PARAMETER_VALUE   파라미터 값                           
	  IDENTIFYING       JobInstance 식별에 사용 여부            
	이런식으로 구성됨,이떄 특이점은 JOB_INSTANCE_ID가 아닌,JOB_EXECUTION_ID을 참조한다는것
	이건 IDENTIFYING이 N,즉 false인거때문임
	이거의 객체는
		JobParameters {
		  Map<String, JobParameter> parameters;  
			
		}
		JobParameter {
		  T value;                    --> PARAMETER_VALUE
		  Class<T> type;              --> PARAMETER_TYPE
		  boolean identifying;        --> IDENTIFYING
		}
	이런식으로 표현됨
	
	StepExecution은 BATCH_STEP_EXECUTION 에 저장되고
		 STEP_EXECUTION_ID  StepExecution 고유 식별자,PK    
		 VERSION            낙관적 락 버전                                
		 STEP_NAME          Step 이름                                  
		 JOB_EXECUTION_ID   연관된 JobExecution의 ID,FK    
		 CREATE_TIME        실행 레코드 생성 시간                           
		 START_TIME         StepExecution 시작 시간                     
		 END_TIME           StepExecution 종료 시간                     
		 STATUS             StepExecution의 현재 상태(BatchStatus)       
		 COMMIT_COUNT       커밋 횟수                                   
		 READ_COUNT         읽은 아이템 수                                
		 FILTER_COUNT       필터링된 아이템 수                             
		 WRITE_COUNT        쓴 아이템 수                                 
		 READ_SKIP_COUNT    읽기 건너뛴 수                                
		 WRITE_SKIP_COUNT   쓰기 건너뛴 수                                
		 PROCESS_SKIP_COUNT 처리 건너뛴 수                                
		 ROLLBACK_COUNT     롤백 횟수                                   
		 EXIT_CODE          StepExecution 종료 코드                     
		 EXIT_MESSAGE       StepExecution 종료 메시지                    
		 LAST_UPDATED       마지막 업데이트 시간                            
	이렇게 저장됨
	이떄 JOB_EXECUTION_ID를 FK로 사용해서 StepExecution이 어떤 JobExecution에 속하는지를 나타냄
	객체로는
	  StepExecution {
	    Long id;                      --> STEP_EXECUTION_ID (PK)
	    JobExecution jobExecution;    --> JOB_EXECUTION_ID (FK)
	    String stepName;              --> STEP_NAME
	    BatchStatus status;           --> STATUS
	    long readCount;               --> READ_COUNT
	    long writeCount;              --> WRITE_COUNT
	    long commitCount;             --> COMMIT_COUNT
	    long rollbackCount;           --> ROLLBACK_COUNT
	    long readSkipCount;           --> READ_SKIP_COUNT
	    long processSkipCount;        --> PROCESS_SKIP_COUNT
	    long writeSkipCount;          --> WRITE_SKIP_COUNT
	    long filterCount;             --> FILTER_COUNT
	    LocalDateTime startTime;      --> START_TIME
	    LocalDateTime createTime;     --> CREATE_TIME
	    LocalDateTime endTime;        --> END_TIME
	    LocalDateTime lastUpdated;    --> LAST_UPDATED
	    ExecutionContext executionContext; --> BATCH_STEP_EXECUTION_CONTEXT 테이블에 저장
	    ExitStatus exitStatus;      --> EXIT_CODE
	    List<Throwable> failureExceptions; --> EXIT_MESSAGE
	    Integer version;            --> VERSION
	  }
	이렇게 표현됨,또한 객체에선 StepExecution이 JobExecution을 직접참조,즉 양방향참조를 하고있음
	
	ExecutionContext은 
	JOB의 경우는 BATCH_JOB_EXECUTION_CONTEXT 테이블에,
	STEP의 경우는 BATCH_STEP_EXECUTION_CONTEXT 테이블에 저장됨
	둘다 비슷하게 저장됨
	JOB은
	  JOB_EXECUTION_ID JobExecution의 ID,PK, FK
	  SHORT_CONTEXT    직렬화된 ExecutionContext의 문자열 버전        
	  SERIALIZED_CONTEXT 전체 컨텍스트, 직렬화된 형태                  
	스텝은
	  STEP_EXECUTION_ID  StepExecution의 ID,PK, FK
	  SHORT_CONTEXT      직렬화된 ExecutionContext의 문자열 버전        
	  SERIALIZED_CONTEXT 전체 컨텍스트, 직렬화된 형태                    
	이렇게 저장됨
	이때 직렬화문자열이 2500자보다 짧으면 SHORT_CONTEXT에 저장되고,SERIALIZED_CONTEXT는 null로 설정됨
	만약 2500자보다 길다면 SHORT_CONTEXT에는 잘린버전(2492자정도)이 저장되고,전체내용은 SERIALIZED_CONTEXT에 CLOB형태로 저장됨
	이 ExecutionContext의 객체는
	  ExecutionContext {
	  	Map<String, Object> map;  --> SHORT_CONTEXT, SERIALIZED_CONTEXT
	  }	
	이렇게 표현됨

  4.Job Squad	
    1.JobLauncher
	  JobLauncher는 Job을 실행시키는 발사대임
	  이건 잡과 잡파라미터를 받아 실행하고,그 결과로 JobExecution을 반환함
	  
	  여기서 재시작일경우(같은잡네임과 잡파라미터로 생성된 잡인스턴스가 있을때)는 배치스테이터스를 확인하고 완료나 ABANDONED일땐 예외를 던지고,
	  또한 식별파라미터가 잡파라미터에 하나도없다면 이 상태검사를 건너뜀
	  
	  즉 매번 새로운 JobExecution을 만들고,재시작일땐 이전 ExecutionContext를 그대로 가져오고,모든정보를 즉시 메타데이터저장소에 기록함
	
	2.AbstractJob & SimpleJob
	  잡에서는 가장먼저 잡스코프를 활성화시키고,JobExecution과 JobContext를 스레드에 바인딩시킴
	  이후 JobParamaters를 한번더 벨리데이팅하고,
	  JobExecution의 상태를 STARTED로 설정한후에 시작시간을 설정한후 테이블에 저장함
	  그리고 beforeJob을 호출하고 
	  잡을 실행시킴(이후 스탭들이 실행되고 순서대로 진행함)
	  이후 JobExecution의 잡 종료시간을 기록하고
	  afterJob을 호출한후
	  최종상태를 메타데이터저장소에 저장하고
	  JobScope를 비활성화함
	
	3.JobInstance의 생성과 저장
	  JobInstance는 JobLauncher가 Job을 실행할떄 생성됨
	  이건
	    기존 JobInstance존재여부를 확인하고(같은 JobName+JobParamaters 중복이 있는지)
		없다면 새 JobInstance를 실행
		있다면 새로 생성하지않고 기존걸 재사용함
	
	4.JobExecution의 생성과 저장
	  JobExecution은 Job이 실행될떄마다 무조건 생성되고,생성된 JobExecution은 지연없이 즉시 메타데이터저장소에 기록됨
	  그리고 Job실행과정에서 상태가 변경된다면 변경사항이 즉시 메타데이터저장소에 반영됨
	
	5.ExecutionContext(JobExecution 수준)의 생성과 저장
	  이것도 첫실행이면 새로만들고,재시작이면 이전 JobExecution의 ExecutionContext를 메타데이터저장소에서 가져오고,
	  결정된 ExecutionContext는 즉시 메타데이터저장소에 저장됨
	  그리고 스탭실행이 끝낼떄마다 변경된 ExecutionContext를 메타데이터저장소에 업데이트함
	  
  5.Step Squad
	1.SimpleStepHandler
	  Step의 실행은 StepHandler에 의해 관리됨,기본값은 SimpleStepHandler임
	  얘는 가장먼저
	    마지막 실행 StepExecution을 JobRepository를 통해 가져옴(재시작시 이전실행상태복구)
		shouldStart()로 lastStepExecution의 STATUS를 사용해서 Step 실행가능여부를 결정함
		  이전실행이 없다면 무조건 실행가능
		  이전실행이 COMPLETED라면 allowStartIfComplete옵션이 Step에없다면 실행하지않음(이옵션은 StepBuilder로 할수있음)
		  이전실행이 ABANDONED이라면 절대 재시작하지않음
		  재시작가능하다면 startLimit를 초과했는지 검사함
		이렇게 함
		여기서 allowStartIfComplete옵션은 후속스탭이 이전 스탭의 실행결과에 의존하는경우엔 true로 설정해야함
		이걸 true로 두면 Job재시작시 이미 완료된 스탭을 건너뛸지 다시실행할지를 결정할수있음(기본값 false)
		
		이렇게 Step이 실행가능하다면
		  JobExecution에서 새로운 StepExecution을 생성함(반드시 새거)
		  재시작여부를 확인하고,
		    재시작이라면 이전 StepExecution의 ExecutionContext를 현재 StepExecution에 설정
		    첫실행이라면 빈ExecutionContext를 설정
		  이후 즉시 JobRepository로 메타데이터저장소에 저장
		  Step을 실행
		  실행완료후 ExecutionContext에 batch.executed 플래그를 설정하고 메타데이터저장소에 저장
		이렇게 동작함
		
		그리고나서 ExitStatus가 EXECUTING으로 초기화됨
		여기서 중요한건 ExitStatus는 Step실행중에는 반영되지않고,스탭이 완료된이후에만 저장됨
		이건 실행이 종료될때 설정되는 코드이기때문
		
		이후 스탭을 실행하기위해 ExecutionContext에 태스크릿타입과 스탭타입을 기록하고 저장함
		이후 청크트랜잭션을 걸어야함
		  StepContribution를 생성하고(청크처리 읽기/처리/쓰기 카운트같은 정보를 담는 컨테이너)
		  태스크릿실행
		  StepContribution에 저장된 통계정보를 StepExecution에 반영(즉 중간에 깨져도 다날아가지않음)
		  청크처리 성공시에 Stream.update()로 ItemStream의 상태를 ExecutionContext에 저장하고,
		  커밋카운트를 증가시키고 업데이트된 StepExecution을 메타데이터 저장소에 저장함
		  이후 BatchStatus를 COMPLETED로 설정하고,ExitStatus도 COMPLETED로 변경함
		이런순선임
	  2.예외처리와 롤백
	    청크처리중 예외가 발생하면 롤백메서드를 호출해 rollbackCount를 증가시킴(실제로 롤백은 TransactionTemplate에서 하고 여기는 카운트만증가)
		즉 예외를 캐치한후 카운트만 올리고 다시 예외를 던짐
		이 예외는 AbstractStep까지 전파되고,여기서 예외종류에 따라 적절한 BatchStatus가 결정되어 StepExecution에 반영됨
	
	  3.Step 종료 처리
	    정상이든 아니든 StepExecution에 종료시간을 설정하고,
		ExitStatus를 설정하고
		afterStep을 호출함(이떄 ExitStatus를 받고 변경할수있음)
		이후 ExecutionContext 업데이트를 시도하고,이때 예외가 발생하면 BatchStatus와 ExitStatus가 둘다 UNKNOWN으로 설정됨
		완료결과를 메타데이터저장소에 저장하고
		마지막으로 ItemStream.close()를 호출함
		이후 JobExecution의 ExecutionContext를 메타데이터 저장소에 업데이트한후
		마지막 처리된 StepExecution을 Job에 반환하고
		이걸 JobExecution에 반영하고,
		마지막 스탭이었다면 잡도 종료시간설정하고 리스너호출후 저장하고 종료시킴

	  4.Step Squad 핵심 요약
		StepExecution 생명주기
		  SimpleStepHandler는 매번 새로운 StepExecution을 생성한다.
		  재시작 시에도 새 StepExecution이 생성되지만, 이전 ExecutionContext를 복구한다.
		  생성된 StepExecution은 즉시 메타데이터 저장소에 저장된다.
		
		BatchStatus와 ExitStatus
		  BatchStatus는 실행 상태가 변경될 때마다 즉시 메타데이터 저장소에 반영된다.
		  ExitStatus는 Step 실행이 완료된 후에만 StepExecution에 설정되어 저장된다.
		  예외 발생 시 적절한 BatchStatus와 ExitStatus가 설정된다.
		  
		StepContribution과 ExecutionContext
		  각 청크마다 StepContribution이 생성되어 처리된 항목 수, 읽기/처리/쓰기 카운트 등을 기록한다.
		  청크 처리 후 StepContribution의 정보가 StepExecution에 반영된다.
		  청크 처리가 실패해도 중간까지의 처리 정보는 안전하게 기록된다.
		  
		청크 및 롤백 처리:
		  청크 단위로 트랜잭션이 관리된다.
		  예외 발생 시 StepExecution에 롤백 카운트가 반영되며, 실제 트랜잭션 롤백은 TransactionTemplate이 담당한다.
		  
		마무리 단계:
		  Step 실행이 종료되면 종료 시간이 설정된다.
		  StepExecutionListener.afterStep()을 통해 ExitStatus를 조정할 수 있다.
		  모든 ItemStream이 닫히며 리소스가 정리된다.
		  최종 상태가 메타데이터 저장소에 업데이트된다.  
		
		이런식으로 처리됨 
	
*2.배치 실행의 진입점과 Spring Batch 자동구성
  1.JobLauncherApplicationRunner: 시스템 명령 체인의 시작점
    ./gradlew bootRun 을 하면 배치작업이 자동으로 실행됨
	이떄 우리는 잡네임과 잡파라미터를 주는데,이걸 처리하는게 JobLauncherApplicationRunner임
	이건 우리가 전달한 문자열을 분석하여 Job와 JobParameters로 변환하고,이를 JobLauncher에 전달하여 실행시킴
	
	즉 모든 파라미터들을 파싱해서 맵에 넣고 그걸가지고 JobParamaters객체를 만듬(JobParamatersBuilder이 처리)

  2.executeLocalJobs(): 애플리케이션 컨텍스트 내의 Job 실행
    이건 @Configuration에서 빈으로 등록한 Job을 직접 찾아 실행함
	즉 로컬잡을 대상으로함	
	즉 현재 빈으로 생성된 모든 Job을 이름으로 Map에 넣고,Map을 순회돌면서 해당하는 이름이 있는지 찾아서 돌리는거임

  3.executeRegisteredJobs(): JobRegistry를 통한 실행
    이건 JobRegistry라는 잡전용 저장소에 등록된 Job들을 찾아서 실행함
	이건 정적인 단일실행에서는 executeLocalJobs과 차이가 없지만,동적으로 Job을 등록해야할때,
	즉 앱시작시점에 외부설정파일이나 db로부터 설정을 읽어서 잡을 만들어야할때 사용됨
	또한 두개중에서는 executeLocalJobs이 우선순위를 가지고,둘다 처리할수있더라도 한번만 처리되니까(로컬에 이미 있다면 무시) 신경안써도됨
	
  4.JobExplorer: 배치 메타데이터 탐색기
    이건 배치가 제공하는 메타데이터 탐색에 최적화된 컴포넌트임
	이건 실행중이거나 완료된배치작업의 내부상태를 샅샅히 들여다볼수있는 메서드를 제공함
	얘는 오직 조회에만 집중해서 풍부한 조회기능을 제공함
	
	이걸 사용해서 배치작업추적 및 관리용 대시보드나 어드민툴등에 사용하기좋음

  5.배치실행 정리
	애플리케이션 진입점
	  JobLauncherApplicationRunner는 Spring Boot 애플리케이션 실행 시 자동으로 등록되어 커맨드라인 파라미터를 분석하고 처리한다.

	파라미터 변환
	  우리가 입력한 문자열 파라미터는 JobParametersConverter를 통해 JobParameters 객체로 변환된다.
	  파라미터 형식(chaos=true,java.lang.Boolean)에서 값과 타입 정보를 추출하여 적절한 Java 타입으로 변환한다.

	잡 실행 경로
	  JobLauncherApplicationRunner는 두 가지 경로로 잡을 실행한다.
	    executeLocalJobs(): Spring 컨텍스트에 등록된 로컬 Job 빈을 실행
	    executeRegisteredJobs(): JobRegistry에 등록된 Job을 실행 (분산 환경이나 동적 Job 생성 시 유용)  
	
	파라미터 가공
	  실행 전 파라미터는 getNextJobParameters() 메서드를 통해 한 번 더 가공된다.
	    이미 존재하는 JobInstance인 경우, 실패/중단된 작업을 재시작할 수 있도록 파라미터 처리
	    JobParametersIncrementer를 통해 매번 다른 JobInstance로 인식될 수 있도록 파라미터 증가 
 
	최종 실행
	  준비된 Job과 JobParameters를 JobLauncher.run() 메서드에 전달하여 실제 Job을 실행한다.	
	

  6.BatchAutoConfiguration: 시스템 자동 세팅
    BatchAutoConfiguration은 Spring Boot의 자동 구성 메커니즘의 핵심 클래스임
	스프링부트는 배치를 감지하면 이걸 자동으로 활성화시키고,얘가 모든 배치코어컴포넌트들을 구성함
	JobLauncherApplicationRunner ,JobLauncher, JobExplorer, JobRepository등등
	이후 프로퍼티에서 jobName을 추출해 세팅하고,
	JobExecution의 ExitCode를 세팅함
	얘의 기본값은 DefaultBatchConfiguration임

  7.DefaultBatchConfiguration: 배치 컴포넌트 구성의 심장
    얘는
		JobRepository
		JobExplorer
		JobLauncher
		JobRegistry
		JobOperator
		JobRegistryBeanPostProcessor
		StepScope
		JobScope	
	를 자동으로 생성하고 빈으로 등록함
	또한 얘는 대부분의 내부메서드들이 protected로 구성되어있어서 필요한거만 오버라이딩하기 편하게 되어있음

  8.@EnableBatchProcessing
    이건 더 간단하게 어노테이션 한줄로 자동구성을 할수있음
	DefaultBatchConfiguration를 상속받지않고 그냥 저걸 Config클래스에 붙이면됨
	그리고 커스터마이징이 필요하다면 커스텀빈을 직접 정의하고 어노테이션 속성에 해당 빈 이름을 지정하면됨

  9.ScopeConfiguration
    빈에 배치스코프를 달면 스프링은 프록시를 생성함
	그래서 빈에 접근하면 프록시가 실제 스코프빈을 조회하기위해,빈팩토리를 통해 스코프의 get을 호출하고 이게 getContext를 호출하게됨
	이떄 getContext내부에서 StepSynchronizationManager를 사용해 Context를 반환하는데,
	그래서 StepScope 활성화란 StepSynchronizationManager에서 유효한 StepContext를 가져올 수 있는 상태를 말하는 것
	즉 이게 null이라면 아직 StepSynchronizationManager.register()가 호출되지않은,활성화되지않은 상태라는것

  10.JobContext/StepContext: Late-Binding
    여기서 컨텍스트는 배치스코프 활성화만을 나타내는게 아닌,런타임 배치정보를 담고있는곳임
	즉 여기서 
	  getJobParameters()
	  getStepExecutionContext()
	  getJobExecutionContext()
	  getStepName()
	  getJobName(),
	  getJobInstanceId() 
	  getSystemProperties()
	등이 다 들어있음(단 JobScope에서는 stepName과 jobInstanceId를 조회할순없음,이건 JobContext를 사용하기떄문)

  11.BatchAutoConfiguration 비활성화
	BatchAutoConfiguration은 다음의 경우에 비활성화됨
	  DefaultBatchConfiguration을 상속한 커스텀 configuration 클래스를 등록하는 경우
	  @EnableBatchProcessing 어노테이션을 사용하는 경우 
	즉 DefaultBatchConfiguration를 커스터마이징하거나 @EnableBatchProcessing를 사용하면 세부설정은 가능하지만 자동구성을 포기해야함
	이럴떈 직접 자동구성 컴포넌트들을 수동으로 구현해야함
	
	그런데 BatchAutoConfiguration은 DefaultBatchConfiguration을 상속한 SpringBootBatchConfiguration을 사용하는데
	얘는 빈을 정의하는식으로 오버라이드할수있으니 이런식으로 쓰는게 좋음

  12.@BatchXXX 어노테이션의 역할
    이게 @Bean으로 생성하는곳에 붙어있으면,이건 배치에서만 사용하는거라고 표시하는거임
	@BatchDataSource, @BatchTransactionManager, @BatchTaskExecutor 등이 있음

  13.TransactionManager 분리하기
    기본적으로 스프링부트가 자동구성해준 트랜잭션매니저를 사용하면,하나의 트랜잭션매니저로 배치메타데이터와 비즈니스db를 같은 트랜잭션으로 처리함
	그런데 실제 운영환경에서는 배치메타데이터와 비즈니스데이터의 분리가 필수적임
	보통 배치 메타데이터용 db와 비즈니스db를 따로두니까
	이렇게 db를 분리하게되면 각각에 맞는 트랜잭션 전략을 적용하기위해 TransactionManager를 별도로 구성하는걸 고려할수있음
	실제 세팅은 강의보면되고,이렇게하면 각 트랜잭션이 독립적으로 동작해서 상호간에 충돌을 방지할수있음
	
	단 주의점이 있는데,이러면 트랜잭션이 두개라서 데이터불일치가 발생할수있음(하나실패하나성공)
	그래서 해결법이 두가지가 있는데,
	  XA DataSource와 JTA TransactionManager를 사용하여 분산트랜잭션구현,일관성보장하지만 설정이 복잡하고 성능오버헤드발생가능
	  불일치를 감안하고 모니터링 및 복구 메커니즘 구축
	가급적 두번째를 하는게 비용면에서 싼경우가 많음

  14.ExitCode
    보통 배치는 cicd도구나 스케줄링도구등을 통해 실행하는경우가 많은데,얘네들은 프로세스의 종료코드를 확인해 작업의 성공여부를 판단하고 후속조치를 취함
	보통 실제운영환경에서는 jar을 실행시키는식으로 동작함
	  java -jar kill-batch-system-0.0.1-SNAPSHOT.jar --spring.batch.job.name=brutalizedSystemJob chaos=true,java.lang.Boolean
	이때 실행완료후 echo $?를 찍어보면 프로세스exit코드가 출력됨(0은 정상종료)	
	그런데 이때 System.exit()를 사용하지않으면 정상종료가 아니라도 0이 뜨게되니 배치를 실행할때 이걸로 감싸는건 필수임
	  System.exit(SpringApplication.exit(SpringApplication.run(KillBatchSystemApplication.class, args)));
	이렇게
	
	이 exitcode는 JobExecutionExitCodeGenerator가 기본적으로 생성함
	  COMPLETED = 0
	  STARTING = 1
	  STARTED = 2
	  STOPPING = 3
	  STOPPED = 4
	  FAILED = 5
	  ABANDONED = 6
	  UNKNOWN = 7
	이렇게 Job의 statusCode에 따라서 다른 숫자를 던짐
	일반적으로는 이정도로 사용해도되지만,특정실패유형에 따라 다른 종료코드를 반환하거나,추가적인 정보를 주거나 할순없고,그래서 ExitStatus를 사용할수있음
	
  15.ExitStatus	
    ExitStatus는 고정값인 BatchStatus와 달리 문자열 기반이라 자유롭게 정의,확장이 가능함
	그래서 예외유형에 따라 다른 커스텀 종료상태를 정의할수있고,거기에 맞는 종료코드를 매핑할수있음
        } catch (IllegalStateException e) {
            contribution.setExitStatus(new ExitStatus("SKULL_FRACTURE", e.getMessage()));
            throw e;	
	이건 이렇게 contribution에 세팅하고 던지면되는데,보통 예외를 잡거나,특정 조건을 만족할때 넣거나 할수있음
	그리고 JobExecutionExitCodeGenerator대신 커스텀 ExitCodeGenerator를 사용하면됨
	
	ExitCodeGenerator 구현체는 ExitCodeGenerator는 당연히 구현해야하지만 JobExecutionListener도 같이 구현하면 좋음
	커스텀 ExitCode를 만들고,
	afterJob을 오버라이드해서 ExitStatus를 가져와서 이걸 정수형종료코드로 변환하면되기때문

  16.순수 스프링배치 CommandLineJobRunner 
    스킵
	
	
*3.REST API를 통한 Job 실행과 JobOperator
  커맨드라인으로만 잡을 실행시킬수있는건 아님
  RestApi등으로 원격으로 Job을 제어할수도있음
  이때 
	spring:
	  batch:
		job:
		  enabled: false	
  설정으로 잡 자동실행을 막을수있음(이러면 JobLauncherApplicationRunner가 자동으로 구성되지않음)
  
  1.JobLauncher를 활용한 REST API 구현
    그냥 JobLauncher,JobRegistry등을 di받고,jobName을 받은다음 그걸가지고 JobRegistry에서 빈의 이름으로 Job을 조회하고,
	이걸가지고 JobParametersBuilder와 JobExplorer를 활용해 이전 실행 이력을 기반으로 JobParamaters를 생성하고,
	JobLauncher의 run으로 잡을 실행하고 그결과로 JobExecution을 받아서 클라에게 응답하면됨
	결국
		JobLauncher로 잡을 실행하고
		JobExplorer로 이전실행이력을 바탕으로한 잡파라미터를 생성하고
		JobRegistry로 등록된 모든 잡을 이름으로 조회하는
	이건 똑같이 사용할수있고,그냥 원격으로 동작시킬수있는거임
	단,한가지 세팅해줘야하는게 있는데 기본 잡런처구현체인 TaskExecutorJobLauncher는 기본적으로 동기방식의 SyncTaskExecutor을 사용함
	그런데 웹요청은 빠른 응답을 기대하기때문에 동기기반은 타임아웃등 좋지못한 경험을 줄수있음
	그래서 @BatchTaskExecutor를 사용해서 TaskExecutor을 비동기기반 구현체로 변경해야함
		@Configuration
		public class BatchCustomConfiguration {
			@Bean
			@BatchTaskExecutor
			public TaskExecutor taskExecutor() {
				SimpleAsyncTaskExecutor executor = new SimpleAsyncTaskExecutor();
				return executor;
			}
		}	
	이걸 등록하면 알아서 저걸가져다쓸거임
	이러면 즉시 응답을 주고,배치는 별도의 스레드에서 동작하게됨
	응답은 JobExecution을 주니까 여기서 id를 빼서 주는식으로 하면됨

  2.JobOperator: 배치 작업의 완전한 제어
    JobOperator를 사용하면 잡을 중지하거나,실패한 잡을 재시작하거나,실행상태를 모니터링하는등 더 정교한 제어를 할수있음
	얘는 운영에 초점을 맞춘 인터페이스로,잡런처보다 한단계 높은 수준의 추상화를 제공함
	이걸 사용하면 실행,중지,재시작,실행이력조회등을 restapi로 할수있음
	
	이때 중지에 대해 알아둬야할게있는데,이 중지는 즉시 중지하는게 아닌,해당 청크를 마무리한다음에 중지됨
	그리고 재시작할땐 해당 청크 다음청크부터 재시작되고
	왜냐면 중지는 실제로 Job을 즉시 중단시키는게 아닌,중단요청을 메타데이터에 기록하는것에 불과하기때문임
	그리고 실제로 중단을 감지하고 처리하는건 TaskletStep인데,얘는 청크처리가 정상완료된후 트랜잭션커밋전에만 확인하기때문임
	이렇게해서
	  데이터일관성
	  리소스효율성
	을 얻을수있음
	또한 카프카처럼 롤백이 불가능한 ItemWriter도 있으니 안전한걸 선택한거

  3.Spring Batch의 Job 중지 흐름 요약
	중지 요청 단계
	  JobOperator.stop() 호출 시 JobExecution의 상태가 STOPPING으로 설정
	  이 상태는 메타데이터 저장소에 기록되지만, 실제 작업은 아직 중단되지 않음
	중지 감지 단계
	  청크 처리 완료 후 JobRepository.update(stepExecution) 호출
	  checkForInterruption() 메서드에서 메타데이터에서 JobExecution의 현재 상태를 동기화
	  JobExecution이 STOPPING 상태면 StepExecution의 terminateOnly 플래그를 true로 설정
	중지 실행 단계
	  StepExecution의 terminateOnly 필드가 true인 경우 JobInterruptedException 발생
	  AbstractStep에서 예외를 포착하여 StepExecution 상태를 STOPPED로 변경
	  SimpleStepHandler에서 StepExecution의 STOPPED 상태를 감지하고 JobInterruptedException 예외 발생
	  최종적으로 AbstractJob에서 JobExecution 상태를 STOPPED로 설정

  4.JobOperator Job 재시작 메커니즘	 
	재시작은 그냥 id를 받아서 JobExecution을 추적하고,Job빈을 뽑아낸후 JobLauncher로 run하는게 다임
	재시작의 실질적 매커니즘은 JobLauncher에서 처리하기때문에,여기선 별내용이 없음

  5.메시지 기반 Job 실행 
    Spring Batch Integration은 배치에 Spring Integration의 메시징기능을 결합한 확장모듈임
	이걸 사용하면 RabbitMQ나 kafka같은 외부메시징 시스템으로부터 수신한 메시지를 트리거로 배치Job을 자동실행할수있음
	실제예제는 쓸일있을때 참고하면될듯
	결국 진입점이 좀 달라진다는거뿐이고 내부동작은 같아서
	
	
*4.Flow:배치의 흐름 제어
  기본적으로 배치는 정직하게 순차실행됨
  그런데 가끔 분기를 타야하는경우가 존재함(앞 스탭의 실행결과에 따른 분기)
  보통 실패처리를 해야한다던가,앞에거가 전부 성공해야 할수있다던가
  
  이럴때 사용하기위해 flow가 있음
  
  1.Flow란?
    flow는 Job안에서 각 Step들이 언제,어떤조건으로 실행될지 우리가 직접 설계하고 지휘하는 핵심 컨트롤타워임
	즉 ExitCode같은 상태를 기반으로 분기를 태울수있어지는거

  2.Flow 3대 핵심 요소
    가장 중요한건
	  state:현재 실행이 머무르거나 도달할수있는 모든 논리적지점
	    실행상태:Flow내에서 실제 특정작업을 수행하는 지점을 나타냄(각 스탭들)
		종료상태:Flow실행의 최종도착점,Job의 최종결과는 Flow가 어떤상태로 종료되었는지에 따라 결정됨
	  ExitCode(전이조건):분기의 기준,ExitStatus내의 ExitCode가 기준이 됨,ExitStatus를 사용하니 커스텀도 가능
	  Transition(전이규칙):특정 전이조건에 따라 어디로 이동할지를 정하는 이동규칙임
	이렇게 3가지임
	
	FLow는 시작상태에서 출발해서 여러 실행상태들 사이를 이동하고,각 실행상태의 결과인 ExitCode와 사전의 전이규칙에 따라 다음에 진행할 상태가 결정되고,
	이걸 반복하다가 특정 종료상태에 도달해서 Flow의 실행을 마치게됨
	
	실제로 구현을 할떈,
	  .start(스탭)//시작상태설정
	  .on(상태)//종료상태조건지정("COMPLATED")등
	  .to(스탭)//전이될 다음상태(스탭) 정의
	  .from(스탭)//추가분기필요시 기준점 다시설정하고 새조건과 전이 정의
	  .end()//플로우정의 완료
	이걸 섞어서 만들면됨
	즉 start로 시작하고 end로 끝내고,그사이에 on과 to로 어디로갈지 각각 정하고,다음스탭에서 분기세팅할땐 from으로 넘어가서 세팅하는식임

  3.커스텀 ExitCode를 활용한 Flow 분기 제어
    태스크릿에서 커스텀 ExitStatus를 만드는건
		} catch (IllegalStateException e) {
		  contribution.setExitStatus(new ExitStatus("SKULL_FRACTURE", e.getMessage()));
		throw e;
	이렇게 하면되지만
    청크지향처리에서 커스텀ExitStatus를 만들땐 직접 StepContribution에 접근하기가 힘듬
	그래서 StepExecutionListener의 afterStep()을 구현하는식으로 돌아가면됨
	여기서 ExitStatus를 조건에 따라 반환하면되는거

  4.Flow의 빠진 조건
    만약 특정 상태로 종료되었는데,여기에 해당하는 전이규칙이 없으면 예외가 발생함
	정확히는 
	  한 번이라도 .on() 을 사용한 상태(State)가 있다면, 반드시 해당 상태에서 발생 가능한 모든 조건(ExitStatus)에 대한 전이를 정의해야함
	즉 조건부분기를 사용한다면,모든 가능한 ExitCode에 대한 경로가 정의되어있어야함
  
  5.암시적 전이  
	단,.on()->.to()을 아예 사용하지않으면 암시적상태로 분류되어서 자동으로 암시적 전이규칙을 생성해줌
	  종료코드가 "COMPLATED"라면 COMPLATED상태의 endState로 전이
	  종료코드가 "COMPLATED"가 아니라면 FAILED상태의 endState로 전이
	이때 on을 한번이라도 쓰면 동작하지않으니 직접 명시적으로 정의해줘야함

  6.와일드카드
    이렇게 모든 종료코드에 대응해서 조건을 넣는건 너무 힘드니까,와일드카드를 사용해서 패턴매칭을 할수있음
	*는 어떤 문자열과도 매칭되고
	  "*"는 모든문자열
	  "ERROR_5*"는 ERROR_5로 시작하는 모든문자열
	?는 특정 한개의 문자와 매칭되는 와일드카드
	  "ERROR_50?"는 ERROR_50?로 끝나는 문자열
	  "ERROR_???"는 ERROR_???로 끝나는 문자열
	이것도 너무 복잡하게하면 가독성박살나니 주의
	
  7.전이 우선순위
	여러 와일드카드가 동시에 만족될경우엔
	  정확한거
	  ? 
	  ???  //?갯수따라서 밀림
	  *
	이렇게 더 구체적인 조건으로 먼저 들어감

  8.EndState
    Flow가 EndState에 도달하면 Flow의 실행이 종료되고,Job도 종료됨
	이때 Job의 최종상태는 기본값으로는 EndState를 그대로 반환함
	
  9.EndState 전이 메서드: end(), stop(), fail()
	자동전이말고 우리가 직접 원하는 EndState로 전이하고싶을땐 end(), stop(), fail()를 사용할수있음
	이걸 사용하면 해당 스탭의 결과와 상관없이 해당 상태로 전이됨
	  end():COMPLATED상태로 전이됨(여기서 end는 Flow완성의 end와 다른거임)
		.from(analyzeLectureStep)
		.on("TOO_EXPENSIVE").to(priceGougerPunishmentStep).on("*").end()
	  stop():STOPPED상태로 전이
		.from(analyzeLectureStep)
		.on("666_UNKNOWN_PANIC").to(adminManualCheckStep).on("*").stop() 
	  fail():FAILED상태로 전이(즉시 실패처리됨)
		.from(analyzeLectureStep)
		.on("PLAGIARISM_DETECTED").fail()	

  10.커스텀 EndState로의 전이: end(String status)
    커스텀 상태로 전이할땐 end(String status)를 사용할수있음
	이러면 해당상태로 전이됨
	단,이렇게 사용할때,커스텀 EndState가 BatchStatus열거형값(COMPLATED,FAILED등)으로 시작하는 문자열이 아니라면 UNKNOWN을 반환하니 주의
	  "COMPLETED_BY_SYSTEM" //이런식
	BatchStatus는 내부적으로 사용되는 중요한상태값이고,일반적으로는 개발자가 수정하면안됨
	그래도 건드릴수있게 해주는대신,기존 정의된 BatchStatus값과 매칭을 할수있게 강제하는거

  11.JobExecutionDecider
    ExitCode로 분기하는건 간단한분기에는 충분하지만,복잡해지면 문제가 생김
	  Step내부에 ExitCode로직이 증가하고,핵심비즈니스로직은 뒤로 밀려남
	  청크지향에서는 ExitCode설정을 위해 StepExecutionListener를 별도구현해야하는데,이러면 흐름제어로직이 리스너에 숨어서 전체흐름이해가 어려워짐
	그래서 사용되는게 JobExecutionDecider임
	
	JobExecutionDecider를 사용하면 전이조건설정로직을 Step외부로 뺄수있음
	여기서 얻는 이점은
	  관심사의 분리:Step의 데이터처리,흐름제어는 JobExecutionDecider
	  더 명확한 Flow정의:Flow구성에서 의사결정포인트가 명시적으로 드러남
	StepExecutionListener는 원래 Step의 생명주기이벤트를 감지하는목적이라서 Flow제어에 사용하는게 사도임
	그런데 JobExecutionDecider는 Flow분기만을 위한 컴포넌트고,이게 등장하면 여기서 분기제어를 하는구나 하는게 바로 나타나게됨
	
	이걸 사용할떈 JobExecutionDecider를 구현한 클래스를 만들고,decide를 오버라이딩하면됨
	여기서 JobExecution과 StepExecution을 기반으로 FlowExecutionStatus를 리턴해주면됨
	그리고 잡빌더에선
		.start(analyzeStudentReviewStep)
		.next(studentReviewDecider) // 여기서 선언
		.on("EXCELLENT_COURSE").to(promoteCourseStep)
		.from(studentReviewDecider).on("AVERAGE_COURSE").to(normalManagementStep)
		.from(studentReviewDecider).on("NEEDS_IMPROVEMENT").to(improvementRequiredStep)
		.from(studentReviewDecider).on("666_SPRING_BATCH").to(springBatchMasterStep)
		.end()
		.build();
	이런식으로 사용하면됨
	
  12.Flow 외부화
	만약 여러 Job에서 같은 Flow패턴이 반복된다면 Flow를 빈으로 정의할수있음
	Flow빌더로 만들면됨

  13.Flow 재사용 방법
    가장 간단한방법은 Job에 Flow를 주입하는것
		return new JobBuilder("newCourseReviewJob", jobRepository)
			.start(lectureValidationFlow)  // Flow를 Job의 시작점으로 사용
			.next(notifyInstructorStep)    // Flow 완료 후 추가 Step 실행
			.end()
			.build();	
			
	또다른 방법은 Flow를 단일스탭처럼 랩핑하는것
		@Bean
		public Step validationStep(JobRepository jobRepository, 
			 PlatformTransactionManager transactionManager,
			 Flow lectureValidationFlow) {
			return new StepBuilder("validationStep", jobRepository)
				.flow(lectureValidationFlow)  // Step 내에 Flow 주입
				.build();
		}
	이러면 스탭처럼 취급할수있음
	이건 Flow전체에 Step단위리스너를 붙이거나,ItemStream을 적용하고싶을때 사용할수있음
	
	Flow를 실행 상태로 활용하는방법도 있음
		return new JobBuilder("courseUpdateJob", jobRepository)
				.start(checkModificationStep)
				.on("MAJOR_UPDATE").to(lectureValidationFlow)  // 조건에 따라 Flow 실행
				.from(checkModificationStep)
				.on("MINOR_UPDATE").to(publishUpdateStep)
				.end()
				.build();	
	이렇게 to에 Flow를 넣어서 하나의 실행상태로 취급하는것
	단,이런구조는 복잡도가 매우 올라가니까 가급적 피하는게 좋음
	
	그리고 Flow자체도 복잡도가 올라가니까 항상필요할때만 사용하는게좋음
	

  14.부록:5.1에서의 취약점
    배치5.1에서는 "*"에서 버그가 있음
	그래서 *대신 **를 사용해야함
	왜냐면 두 패턴의 *문자의 개수가 같다면,단순히 두패턴의 알파벳순서를 비교해 우선순위를 정해버리게됨
	그래서 "*"가 "666_*"보다 우선적으로 잡혀버리는 버그가 있음
	이럴때 "**"를 사용하면 와일드카드가 2개라서 *갯수가 적은 "666_*"이 먼저 잡히게됨
	즉 5.1에서는
	  APPROVED > APPROVE? > ??? > * > 666_* > **
	이런우선순위를 가지게됨
	
	
6.스프링배치 응용
*1.Multi-threaded Step	
  1.멀티스레드스텝이란
    이건 하나의 StepExecution을 여러스레드로 동시실행하는 기법임
	이건 간단하게 스탭빌더에서
        .taskExecutor(taskExecutor())
        .throttleLimit(5)//동시에 실행가능한 스레드 최대갯수,maxPool이랑 똑같이세팅하는게좋음,기본값4
        .build();	
	이렇게 taskExecutor를 추가해주면되고,
		@Bean
		public TaskExecutor taskExecutor() {
			ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
			executor.setCorePoolSize(5);//기본스레드수,maxPool이랑 똑같이세팅하는게좋음
			executor.setMaxPoolSize(5);//최대스레드수,CorePool이랑 똑같이세팅하는게좋음
			executor.setWaitForTasksToCompleteOnShutdown(true);//중단신호를 받았을때 처리중인 작업이 완료되기까지 대기하는 설정
			executor.setAwaitTerminationSeconds(10);//중단신호시 최대 대기시간
			executor.setThreadNamePrefix("T-800-");//이름
			executor.setAllowCoreThreadTimeOut(true);//스레드풀특성상 처리할작업이 없어도 유지되는데,아무것도 안하고 n초지나면 jvm종료설정
			executor.setKeepAliveSeconds(30);//유휴상태 타임아웃 시간설정
			return executor;
		}	
	이렇게 빈으로 설정을 만들어서 저쪽에서 불러오면됨
	이걸 설정하면 스탭빌더는 표준 RepeatTemplate 대신 TaskExecutorRepeatTemplate라는 구현체를 사용하는데
	이건병렬처리능력을 가진 확장버전임
	정확히는 청크단위의 실제처리작업만 TaskExecutor의 워커스레드가 처리함
	그래서
		태스크릿 지향 처리 스텝: Tasklet의 execute() 메서드를 여러 스레드가 동시에 병렬로 실행한다.
		청크 지향 처리 스텝: 각 청크 처리(ItemReader → ItemProcessor → ItemWriter 처리 사이클)를 여러 스레드가 동시에 수행한다.
		  각 스레드가 독립적인 청크를 가져와 처리하므로 전체 처리량이 향상된다.
	이렇게 청크지향에선 청크단위로 병렬스레딩을 하고,태스크릿은 그냥 동시에 실행함
	
	그런데 ThreadLocal은 스레드별로 독립적이라서,그래서 각 스레드들을 각각 StepSynchronizationManager.register()를 호출해
	자신만의 ThreadLocal 공간에 StepExecution을 바인딩함

  2.멀티스레드 주의사항
	일단 멀티스레드환경에선 공유상태가 예측할수없게 오염될수있음
	여러스레드가 같은데이터에 접근할때 데이터불일치가 발생할수있음
	그래서 스레드안전성에 대한 이해가 필수임

  3.멀티스레드환경에서의 스레드안전성
    스레드안전성은 두가지로 나뉨
	  데이터처리안전성:데이터를 읽고쓰기관점에서의 스레드안전성
	  실행상태안전성:메타데이터기록 동기화관점에서의 스레드안전성
	이렇게

  4.데이터처리안전성
    이건 아이템리더와 라이터가 데이터를 읽고쓰는과정에서 생기는 스레드안전성문제임
	여러스레드가 동시에 같은데이터에 접근할때,데이터불일치,누락,중복처리등 문제가 발생하는거
	
	보통 아이템라이터는 스레드안전한경우가 많지만,아이템리더는 스레드안전하지않은 구현체들이 많음
	그래서 해당 구현체코드에 들어가보면 스레드안전한지 아닌지를 보통 적어둠
	없다면 실제 코드에서 공유상태변수를 사용하는지(FaltFileItemReader등,보통 싱글톤으로 빈을 쓰니까),
	iterator을 사용하는지(JpaCursorItemReader,자바의 iterator는 스레드안전하지않음)
	등이 문제가 될수있음
	
	JdbcPagingItemReader같은경우엔 조건에따라 스레드안전함
	그래서 페이지중복이 일어나지않는데,이건 doRead() 메서드의 시작과 끝이 lock(ReentrantLock)으로 감싸져 있어서 그럼
	그래서 스레드안전하지않은 ItemReader들도 이렇게 lock을 사용해서 읽기를 동기화시킬수있음
	근데 이미 스프링배치가 제공하고있어서 직접 구현할필요는 없음

  5.SynchronizedItemStreamReader
    이건 스레드안전하지않은 ItemReader을 동기화하는 방법임
	이건 스레드안전 데코레이터로,read에 ReentrantLock을 적용해서 읽기시 스레드간 동시접근을 제어하고 스레드안전성을 보장함
	setDelegate에 위임할 ItemStreamReader를 전달하면됨
	이러면 순서대로 데이터를 읽게됨	
	또한 ItemStream을 구현한애들만 받기때문에,자동으로 배치는 ItemStream으로 관리하는데,그래서 리소스초기화와 상태관리도 걔들을 통해서 할수있게됨
	
	만약 ItemStream을 구현하지않은 아이템리더를 써야한다면 SynchronizedItemReader를 쓸수있음
	이건 그냥 ItemStream관련로직만 제외된 형태임
	
	ItemWriter쪽도 똑같이 SynchronizedItemStreamWriter와 SynchronizedItemWriter를 사용할수있음

  6.실행상태 안전성
    FlatFileItemWriter는 Item을 쓰는작업에 있어서는 스레드안전하지만(공유변수를 사용하지않아서),
	재시작을 위한 실행상태관리측면에서는 스레드안전하지않음
	open으로 파일을 열때 스레드안전관련 로직이 없어서 파일핸들충돌,중복헤더작성,리소스경쟁등 문제가 발생하고
	close로 닫을때도 버퍼플러시,파일정리작업등이 스레드안전하지않음
	데이터쓰기와 별개로 상태관리메커니즘자체가 멀티스레드를 고려하지않은거
	
	그래도 FlatFileItemWriter는 멀티스레드스텝에서는 동작방식상 청크처리만 여러스레드가 처리하고,
	open,close등 리소스 준비 및 정리작업은 메인스레드에서만 호출하기때문에 신경쓰지않아도됨
	단 한가지 ItemStream.update()에서 발생하는 문제는 체크해야함
	얘는 saveState필드가 true라면 현재 처리위치와 기록된 라인수를 ExecutionContext에 저장하는데,
	이걸 켜두면 여러스레드가 동시에 업데이트를해서 데이터불일치가 발생함
	그래서 재시작을 포기하고 저걸 false로 스탭빌더에서 설정해야함
	
	대부분의 멀티스레드환경에서 안전하게 돌아가게할땐,saveState는 false로 둬야하는경우가 매우많음(JdbcPagingItemReader등)
	
	추가적으로 JsonFileItemWriter는 공유상태를 사용하기때문에 반드시 SynchronizedItemStreamWriter로 래핑해야함
	  
	
*2.Local Partitioning	
  멀티스레드스탭을 사용한 멀티스레딩은 성능이 올라가긴하지만,아이템리더와 라이터에서의 동기화병목으로 인한 특정 갯수 이상의 스레드를 사용할때 병목이 있음
  그래서 사용되는게 파티셔닝임
  각 스레드가 독립된 StepExecution으로 작동하게하고,ItemReader과 ItemWriter에 @StepScope를 걸어서,
  각 스레드마다 독립된 아이템리더 라이터를 가지게하는것
  즉 단일스레드스탭을 여러개 실행하는것
  여기서 중요한건,각 스레드마다 독립된 범위의 작업(1년치 데이터를 처리한다면 4스레드일떄 1~3,4~6,7~9,10~12로 나누는식)을 맡겨야한다는거
  이러면 서로간에 간섭이 일어나지않으니 락을 걸지않아도됨
  
  1.파티셔닝
    스프링 배치 파티셔닝은 멀티스레드스텝의 성능한계를 돌파하기위해 생겼음
	이건 전체 데이터를 여러 작은조각,즉 파티션으로 나누는것부터 시작함
	이렇게 쪼갠 파티션을 각각 별도의 스레드로 할당해서 동시에 처리하는것
	
	그래서 ItemReader와 ItemWriter에 @StepScope만 제대로 붙어있다면 각 StepExecution마다 새로 생성되니까 스레드별 공유가 되지않음
	즉 서로 다른 데이터범위를 처리하는 독립적인 여러 스탭을 동시에 수행한다고 보면됨

  2.파티셔닝 구현
    파티셔닝은 ManagerStep과 WorkerStep으로 구성됨
	매니저가 워커를 관리하는형태
	또한 매니저스텝에는 
	  StepExecutionSpliter: 데이터 분할+파티션마다 워커 StepExecution인스턴스 생성하는 객체,보통 기본값사용하고 partitioner만 합성함
	  PartitionHandler:생성된 StepExecution으로 가용한 스레드에 할당하여 병렬실행을 명령+실행결과 취합후 정리,보통 기본값사용함
	하는 두개의 핵심 객체가 있음
	매니저스탭은 직접 작업을 처리하지않고,다른애들한테 맡김
	핵심은 각 워커스탭은 독립적인 아이템라이터와 리더를 가진다는것,그래서 병목이 없음

  3.데이터분할
    데이터의 분할은 StepExecutionSplitter의 Partitioner인터페이스가 처리함
	데이터분할기준은 각 작업마다 다르니 우리가 직접 정의해야함
	배치는 특성상 처리할 입력데이터의 범위가 이미 확정되어있고,이걸 기준으로 우리는 로직을 짜서 분리할수있음
	
	구현은 Partitioner인터페이스를 구현하고,여기의 partition을 오버라이드해서,
	i를 키로,ExecutionContext에 범위정보를 값으로 담아서 Map으로 리턴하면됨
	여기서 i는 반복문i로,각 파티션의 고유식별자임,꼭 저렇게할필요없고 고유값이면됨
	
	그리고 ItemReader에서 SpEL 표현식으로 범위를 ExecutionContext를 사용해서 주입받으면됨
		@Bean
		@StepScope
		public RedisItemReader<String, BattlefieldLog> redisLogReader(
				@Value("#{stepExecutionContext['startDateTime']}") LocalDateTime startDateTime) {
			return new RedisItemReaderBuilder<String, BattlefieldLog>()
				.redisTemplate(redisTemplate())
				.scanOptions(ScanOptions.scanOptions()               
					// 💀 Redis에 저장된 전장 로그의 키가 
					// "logs:[날짜시간]:*" 형식으로 저장되어 있다고 가정 💀 
					.match("logs:" + startDateTime.format(FORMATTER) + ":*")
					.count(10000)
					.build())
				.build();
		}
	이런식으로 아이템리더에서 @Value로 받은 값을 활용해서 데이터를 잘라서쓰면됨
	그리고 스텝은
		@Bean
		public Step managerStep(Step workerStep) {
			return new StepBuilder("managerStep", jobRepository)
					//  핵심 1: 파티셔닝 선언 및 Partitioner 주입 
					.partitioner("workerStep", dailyTimeRangePartitioner)

					// 핵심 2: 실제 작업을 수행할 워커 스텝 지정 
					.step(workerStep)
					.taskExecutor(partitionTaskExecutor()) // 병렬 실행을 위한 TaskExecutor
					.gridSize(4) // 24시간을 4개(6시간)의 파티션으로 분할 
					.build();
		}	
	이렇게 파티셔너를 선언하고,워커스텝을 지정한후에,잡에선 저 매니저스탭을 사용하면됨
	워커스텝의 경우는 그냥 평범한스텝을 사용하면됨
	즉 매니저스탭에서 고려해서 일감을 내려주고,워커스탭은 평범하게 동작하는거
	또한 매니저스텝의 taskExecutor는 최대 스레드수를 결정하는데,파티션갯수와 스레드갯수를 같게 만드는게 가장 좋음

  4.멀티스레드스탭+파티셔닝
    아예 워커스텝을 멀티스레드스탭으로 만들어서 처리하는것도 가능함
	물론 그냥 파티션갯수를 늘리는방법도 있음(애초에 이렇게 나누면 스레드풀도 두개로 관리해야하고 디버깅도 어려움)
	근데 가끔 이렇게 해야할때가 있음
	  논리적 파티션수에 제약이 있을때
	  ItemProcessor단에서 개별아이템처리가 병목일때(외부 api io 병목일때등)
	이런경우
	이떄는 워커스텝에 taskExecutor를 넣어서 사용하고,아이템리더와 라이터에 SynchronizedItemReader writer을 감싸서 처리하면됨
	즉 멀티스레드스탭쓰는거랑 똑같이쓰고,그걸 매니저스텝에서 사용하면됨
	주의할점은 두개의 스레드풀을 별도로 관리해야한다는것

  5.파일시스템에서의 파티셔닝(읽기)
    db같은게 아닌 파일에서의 파티셔닝은 각 파티션이 각각 서로 다른파일을 완전히 독립적으로 처리하게됨
	즉 각 파티션이 데이터소스를 독립적으로 처리하는것
	이건 간단하기때문에 MultiResourcePartitioner라는 클래스가 이미 있음
	파일배열을 받아서 각 파일마다 ExecutionContext를 만들고 그걸 리턴함
	그러니 그냥 빈으로 만들어서 주면됨
	주의점은 얘는 URL형식으로 인지하기때문에 반드시 'file://'을 써야함

  6.파일시스템에서의 파티셔닝(쓰기)
    FlatFileItemWriter는 같은파일에다가 파일을 쓸때 안전하지않음
	그래서 각 워커스탭은 각각 독립된 파일에 쓰기를 해야함(보통 입력파일에 .out를 붙여서 많이씀)
	그리고나서 다음스탭으로 이걸 다시 합치는식으로 만들면됨

  7.StepExecutionAggregator
    매니저스탭이 실행한 워커스탭이 완료될때마다 집계를 하고,여기서 모든워커의 상태,카운트가 매니저로 통합됨
	이때 하나라도 실패했으면 매니저스탭도 실패임
	이 집계에는 기본값으로 DefaultStepExecutionAggregator를 사용하고
	.aggregate()를 사용해서 매니저스탭과 각 워커스텝의 StepExecution을 받을수있음
	기본적으로는 DefaultStepExecutionAggregator가 사용되지만
	단 이것만으로 부족할경우엔 StepExecutionAggregator 를 직접 구현할수있음
	그리고나서 매니저스탭빌더에 등록하면됨
	  .aggregator(battleReportConsolidator)
	
*3.원격파티셔닝
  추천하지않음,대부분 스킵
  
  원격파티셔닝은 여러가지로 문제가 있음
    매우큰 복잡성:
      스프링 인티그레이션을 익혀야함
	  메시지 미들웨어(카프카등)을 반드시 사용해야함
	  분산시스템의 문제점을 가져감
	메타데이터 저장소를 db로 사용해야함:인메모리기반 JobRepository를 사용할수없어서 반드시 db를 사용해야함
  이런 문제가 있음
  
  그래서 대안으로 사용되는건,잡파라미터를 활용한 잡 복제를 주로 사용하게됨
  이건 그냥 배치잡을 여러 컴퓨터에서 실행하되,각각에 다른 범위를 할당하면됨
  로컬/원격파티셔닝에선 ExecutionContext을 통해 처리할 데이터 범위를 잘라서 사용했다면,
  이건 그냥 우리가 각각 범위를 명확히 지정해서 잘라주는거
  이건
    단순함:추가 기술스택이 필요없음
	독립성:각 Job인스턴스는 완전히 독립적으로 실행되고,서로간에 영향을 줄필요가 없어서 관리,디버깅이 용이
	유연성:필요에따라 잡인스턴스를 늘리거나 줄이는식으로 손쉽게 확장축소가 가능함
  정말 원격파티셔닝을 해야하는경우가 아니라면,그냥 각각돌리는게 나음	
  
  팁으로는
  @ConditionalOnProperty를 사용하면 특정 프로퍼티(잡파라미터 입력,처음에 실행시킬때)에 해당하는 입력을 프로그램시작때 받아야지만 빈이 뜨게할수있음
  이건 배치꺼가 아니라 스프링꺼임
	@Configuration
	@ConditionalOnProperty(name = "spring.batch.job.name", havingValue = "battlefieldLogPersistenceJob")
	public class BattlefieldLogPersistenceJobConfiguration {  
  이런식
  이건 한 프로젝트내에 잡이 여러개있고,그중 하나만 로드해서 실행시켜야할때 유용하게 사용됨
  
*4.원격 청킹(Remote Chunking)
  원격청킹은 매니저가 직접 데이터를 읽어서 청크단위로 묶은후,이 청크를 워커에게 보내는 방식임
  원격파티셔닝이 매니저가 StepExecutionContext를 워커에 보내서,거기서 알아서 자기가 할 부분만 처리했다면,
  원격청킹은 올바른 데이터를 분리해서 주는책임(데이터공급자)까지 매니저에 넣은것(즉 리더가 매니저에만 있음)
  이경우엔 워커는 데이터처리의 역할만 수행함(즉 프로세서와 라이터만 워커에 있음)
  이때 통신수단은 매니저의 ChunkRequest로 요청하고,ChunkResponse(처리결과요약)으로 받음

  이건 워커의 프로세싱/라이팅의 부하가 매우 심할때(읽기는 빠르지만 처리나 쓰는게 매우복잡하고 느릴떄)효과적임
  또한 매니저측 데이터소스분할이 불가능할때(단일스트림,순서가 중요한 메시지큐,파티셔닝키가 없는 레거시데이터소스등)을 처리할땐,
  매니저가 전부 순차적으로 읽은다음 분배하는방법밖에없음
  
  단점으로는
    네트워크대역폭:실제데이터를 전송하므로 네트워크부하가 매우심하고,데이터건수가 많거나 크기가 크면 통신자체가 병목이 될수있음
	매니저읽기병목:매니저가 혼자 다읽어야하므로,읽기가 느리다면 원격청킹은 효과가 없음
	복잡성:미들웨어와 스프링 인터그레이션이 필수적
  이라는 단점이 있음
 
  1.구현
    매니저노드는 데이터판독,청크생성,워커에게 발송 및 결과수신/집계를 담당함
	이건 @EnableBatchIntegration선언시 자동으로 di되는 RemoteChunkingManagerStepBuilderFactory를 사용함
	실제 구현은 필요해지면 보면될듯
	그렇게 어렵진않은데 자주 사용하진않을거같아서
	
	
*5.병렬스탭과 AsyncItemProcessor	
  병렬스탭은 병렬스탭쓸바에 그냥 잡을 여러개 실행시키면되고,만약 c스탭이 a,b스탭의 결과물을 다 가져야한다고 해도 그냥 a,b잡을 돌리고나서 완료되면 c잡을 돌리면됨
  어지간하면 쓸일없으니 생략
  
  seda패턴은 reader과 processor이 writer보다 훨씬 빠를때,리더와 프로세서가 라이터를 기다리는 비효율을 해결하기위한 패턴임
  이건 기존 단일멀티스테드스텝을 생산자와 소비자라는 독립적인 스탭으로 분할하고,그사이에 중간버퍼를 배치해서 두 스텝을 디커플링하는것
  메시지큐쓰는느낌으로 분리시킨후 중간버퍼를 브로커로 써서 각각돌리는느낌
  근데 이건 써보면 별로 효율적이지않아서 잘 사용되진않음
  
  
  AsyncItemProcessor는 병목지점이 프로세서일떄,이걸 비동기로 처리하는 프로세서임
  이건 읽기는 빠른데,프로세서가 병목일때,프로세서와 라이터를 비동기로 잡아서 메인스레드가 다음아이템을 즉시 처리할수있게 하는것
  즉 읽기쓰레드는 하나로 잡아서 계속 읽기를 하고,프로세서 스레드를 여러개로 잡아서 각각 일을 한후 writer를 돌리는것
  
  이건
    1. AsyncItemProcessor는 위임된 ItemProcessor를 감싸고, 처리 작업을 별도 스레드 풀에 위임한다
	2. 각 아이템 처리 결과는 Future 객체로 즉시 반환된다 (완료되지 않은 상태)
	3. AsyncItemWriter는 모든 Future 처리가 완료될 때까지 대기한 후, 실제 결과값을 위임된 Writer에 전달한다
  이렇게 일반적인 위임프로세서의 형태로,모든 작업이 완료될떄까지 기다린후 Writer이 처리하는식
  
  얘는 청크지향방식을 유지하면서도 ItemProcessor만을 병렬화해서,청크단위 트랜잭션안전성은 유지됨
  
  이떄 주의할건
    적절한 크기설정:청크가 너무작으면 비동기하는의미가없고,너무크면 메모리사용량이 증가함
	TaskExecutor구성:스레드풀크기는 처리속도와 시스템리소스간의 균형을 고려해야함
	  일반적으로 코어+1개의 스레드가 적절함
	Future 처리:AsyncItemProcessor는 Future<T>를 반환하므로, 스텝 구성시 제네릭 타입에 주의해야 한다.
	  AsyncItemWriter는 자동으로 Future.get()을 호출해 실제 결과값을 추출함
	제약사항:AsyncItemProcessor는 제출 순서대로 Future를 반환하지만, 실제 처리는 병렬로 진행되므로
	  개별 아이템의 처리 완료 순서는 보장되지 않는다. 그러나 AsyncItemWriter가 원래 순서대로 재배열한다.
  즉 AsyncItemWriter는 모든 Future의 처리가 완료될떄까지 대기하고,실제결과를 위임된 ItemWriter에 전달함
  즉 애는 비동기처리가 아닌,Future의 결과를 동기적으로 수집하는 역할을 함
  
  AsyncItemProcessor는 프로세서가 주 병목일때 성능향상이 크고,청크지향의 트랜잭션안전성은 그대로 유지됨
  또한 구현이 단순함
  단점으로는 메모리사용량이 증가하고(Future를 다 받아야하니까),스텝의 일부만 병렬화함(프로세서+라이터)
  
  얘는 원격청킹과 경쟁하는데,원격청킹보단 이게 더 복잡도관리가 쉬움
  대신 원격청킹의 경우 메모리한계를 여러 컴퓨터를 사용하는걸로 벗어날수있고
	
	
7.스프링 배치 테스트
*1.E2E테스트
  배치테스트를 할땐
	testImplementation 'org.springframework.boot:spring-boot-starter-test'
	testImplementation 'org.springframework.batch:spring-batch-test'
	testImplementation 'com.h2database:h2'
  의 의존성을 추가해야함(h2는 상황따라)
  
  또한 h2를 쓸거면 기본설정을 좀 해줘야하고(기본테이블생성등),
    spring.batch.job.enabled: false
  로 배치 자동실행도 막아주는게좋음
  
  e2e테스트는 배치잡을 처음부터 끝까지 실행하면서,모든 컴포넌트가 제대로 연동되는지를 검증함
  배치에서는 e2e테스트가 웹보다 좀 더 중요하다고 볼수있음
  
  e2e테스트를 할떈 @SpringBatchTest를 사용하면 배치테스트에 필요한 핵심 유틸리티들이 자동으로 준비됨
	JobLauncherTestUtils: Job 실행 및 Step 개별 실행을 위한 유틸리티
	JobRepositoryTestUtils: Job 실행 이력 생성 및 정리 등 JobRepository 관련 유틸리티
	StepScopeTestExecutionListener: @StepScope 빈 테스트 지원
	JobScopeTestExecutionListener: @JobScope 빈 테스트 지원	
  즉 저걸 붙이고 이 4가지를 di받을수있음

  1.JobLauncherTestUtils
	이건 잡 전체를 실행하거나,개별스텝만 골라 실행할때 사용됨
	이걸로 잡파라미터를 생성할땐
	  JobParameters jobParameters = jobLauncherTestUtils.getUniqueJobParametersBuilder()
        .addString("filePath", tempDir.toString())
        .toJobParameters();
	이렇게하면되고,잡을 실행할땐
	  JobExecution jobExecution = jobLauncherTestUtils.launchJob(jobParameters);
	이렇게하면됨
	
	이떄 getUniqueJobParametersBuilder를 사용하면,자동으로 고유한 잡파라미터를 생성해줌(random이라는 키에 랜덤값을넣어서)
	또한 어플리케이션컨텍스트안에 잡이 여러개라면,테스트할 잡인스턴스를 직접 지정해줘야함
	  jobLauncherTestUtils.setJob(inFearLearnStudentsBrainWashJob);
	잡이 만약 하나라면 자동으로 잡이 등록됨

  2.개별스텝 테스트
    두개이상의 스텝으로 이뤄진 배치잡을 테스트할땐,각각 따로 테스트하는게 더 견고한 테스트가 됨
	이걸 할땐
	  JobExecution jobExecution =
            jobLauncherTestUtils.launchStep("inFearLearnStudentsBrainWashStep", jobParameters);
	을 사용하면됨(또한 이것도 내부적으로 랜덤파라미터를 생성해서 중복실행방지를 막음)
	이후 JobExecution의 StepExecution을 사용해서 검증을 하면됨
	
*2.단위테스트	
  (마지막에서 StepScopeTestUtils만 봐도됨)
  배치의 단위테스트는 아이템리더,프로세서,라이터등의 개별컴포넌트의 동작을 검증함
  이건 일반적인 스프링 유닛테스트에서는 그냥 di받아서 테스트하면되지만,배치에서는 StepScope등 스코프들때문에 불가능함
  스코프빈은 스텝실행이후에 생성되기때문
  
  그래서 배치는 @SpringBatchTest를 사용해서 테스트를 돌릴수있게함
  이건 테스트인스턴스 초기화단계에서 각각 잡익스큐션과 스텝익스큐션을 자동으로 생성하고,이걸 활용해 배치스코프를 활성화하고,
  완료이후에는 배치스코프를 정리해서 @StepScope와 @JobScope가 있는 빈을 테스트환경에서 정상적으로 사용할수있게해줌

  즉 @SpringBatchTest를 추가하기만 하면 배치스코프빈을 @Autowired로 di받아서 쓸수있다는거
  
  1.배치스코프빈에 JobParamaters설정하기
    또한 테스트클래스에 정의된 getStepExecution을 탐지/호출해서 테스트상황에 맞는 커스텀 StepExecution을 사용할수있게도 해줌
	즉 테스트클래스에 getStepExecution이 있다면,이걸 감지해서 커스텀StepExecution을 생성해준다는것(JobExecution도 마찬가지)	
	잡파라미터도 JobExecution안에 포함된 필드니까,잡익스큐션세팅에 넣어두면되긴함
	
	그런데 이것저것만들기 귀찮아서 쓰는게 MetaDataInstanceFactory임
	이건 JobInstance, JobExecution, StepExecution 등의 Spring Batch 메타데이터 객체들을 간편하게 생성할수있는,
	스프링배치테스트의 유틸리티임
	또한 잡레포지토리등의 의존없이도 잡익스큐션등을 만들수있음
		public StepExecution getStepExecution() {
			writeTestDir = Files.createTempDirectory("write-test");

			// JobParameters 생성
			JobParameters jobParameters = new JobParametersBuilder()
					.addString("filePath", writeTestDir.toString())
					.addLong("random", new SecureRandom().nextLong())
					.toJobParameters();
			
			// MetaDataInstanceFactory로 StepExecution 간편 생성
			return MetaDataInstanceFactory.createStepExecution(jobParameters);
		}
	이렇게 쓰면됨
	
	그리고 테스트가 아닌 실제로직에서는,ItemStream의 open과 close를 배치스텝이 자동으로 호출해줬지만,
	단위테스트에선 실제스텝을 실행하는게 아니기때문에 우리가 직접 ItemStream의 메서드들을 호출해줘야함
		brainwashedVictimWriter.open(new ExecutionContext());
		brainwashedVictimWriter.write(new Chunk<>(brainwashedVictims));
		brainwashedVictimWriter.close();
	이렇게
	
	만약 getStepExecution()을 사용하는게,테스트메서드별로 서로 다른 JobParamater가 필요하다면 문제가 생김
	또한 getStepExecution호출시에는 @TempDir필드의 초기화때문에 별개의 필드를 사용해야하는 문제가있음
	
	그래서 사용되는게 StepScopeTestUtils임
	이것도 StepScopeTestExecutionListener처럼 별도의 스텝실행없이 스코프를 활성화해주고,해당테스트만의 독립적인 스텝익스큐션을 쓸수있음
		// 테스트 메서드 내에서 직접 커스텀 StepExecution 생성
		StepExecution stepExecution = MetaDataInstanceFactory.createStepExecution(jobParameters);


		// StepScopeTestUtils에 stepExecution 전달
		StepScopeTestUtils.doInStepScope(stepExecution, () -> {
			// StepScope 빈 호출
			return null;
		});
	이런식으로 스텝익스큐션을 생성하고,StepScopeTestUtils.doInStepScope로 람다로 넣어주면됨
	마찬가지로 잡스코프빈도 테스트를 동일하게 할수있음(JobScopeTestUtils.doInJobScope())
	
	즉
	    @Test
		void shouldWriteBrainwashedVictimsToFileCorrectly() throws Exception {
			// Given
			List<BrainwashedVictim> brainwashedVictims = createBrainwashedVictims();

			JobParameters jobParameters = new JobParametersBuilder()
					.addString("filePath", tempDir.toString()) // 이제 @TempDir 사용 가능
					.addLong("random", new SecureRandom().nextLong())
					.toJobParameters();
			StepExecution stepExecution = MetaDataInstanceFactory.createStepExecution(jobParameters);


			// When
			// StepScopeTestUtils 활용
			StepScopeTestUtils.doInStepScope(stepExecution, () -> {
				brainwashedVictimWriter.open(new ExecutionContext());
				brainwashedVictimWriter.write(new Chunk<>(brainwashedVictims));
				brainwashedVictimWriter.close();
				return null;
			});


			// Then
			verifyFileOutput();
		}
		// StepScopeTestUtils 사용으로 @TempDir 직접 접근 가능, 더 이상 Path 파라미터 불필요
		private void verifyFileOutput() throws IOException {
			Path expectedFile = Paths.get("src/test/resources/expected_brainwashed_victims.jsonl");
			Path actualFile = tempDir.resolve("brainwashed_victims.jsonl");

			List<String> expectedLines = Files.readAllLines(expectedFile);
			List<String> actualLines = Files.readAllLines(actualFile);

			Assertions.assertLinesMatch(expectedLines, actualLines);
		}
	이런식으로 하면됨
































	
	
  