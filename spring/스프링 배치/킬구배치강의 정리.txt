1.스프링 배치 종결의 서막
*1.서문
스킵
*2.배치처리란
  배치는 일일정산,데이터마이그레이션,리포트생성,데이터정제,데이터통합등 다양한 용도로 사용됨
  배치에서 중요한건 정확성과 완결성임,처리에 시간이 걸려도 완벽하게 처리하는게 목표
  웹에서는 실패시 에러를 반환하고 끝냈지만,배치는 복구를 시도해볼수있음
  
  또한 트랜잭션도 체크포인트 기반으로 할수도있고,처리할 데이터양과 특성에 따라 범위조절도 가능함
  
  그리고 청크지향이라 데이터를 나눠서 순차처리할수있고,병렬처리도 가능함

*3.사전점검
  job은 하나의 완전한 배치 처리를 의미함(일일매출집계,정기결제등)
  step은 job을 구성하는 실행단위로,하나의 job은 하나이상의 step으로 구성됨
  즉 일일매출집계 job은
    매출집계step
	  전일주문데이터 read
	  결제완료된것만 필터링 process
	  상품별로 집계후 저장 write
	알림발송step
	  집계요약정보생성후 관리자에게 전달
	캐시갱신 step
	  집계된 데이터로 캐시정보 업데이트
  이런식으로 구성됨
  
  스프링배치는 배치잡을 만들기위해 필요한 거의 모든걸 제공함
  우리는 이 뼈대위에 스프링배치가 제공하는 컴포넌트들을 @configuration으로 구성만 하면됨
  즉 스프링배치를 스프링배치가 제공하는영역과 개발자가 제어하는 영역으로 나눠보는게 쉬움
  보통 잡과 스탭,잡런처,잡레포,ItemReader구현체,writer구현체등이 스프링 배치가 제공하는 영역임
  우리는 Job과 Step의 구성을 해야하고(configuration),기본적으로 제공되지않는 영역의 로직을 만들어야함(보통 processor은 제공하지않음)
  
  추가적으로 배치에선 시스템 시작의 결과를
    SpringApplication.run(abcApplication.class, args);
  이게 아닌
    System.exit(SpringApplication.exit(SpringApplication.run(abcApplication.class, args)));
  이런식으로 exit로 처리하는게 권장됨
  이러면 배치작업의 성공실패를 exit코드로 외부에 전달할수있어서 배치모니터링과 제어에 필수적임
  
  잡은 잡빌더를 통해 생성되는데,이때 잡의 이름과 잡레포지토리가 파라미터로 들어감
  또한 잡레포지토리와 트랜잭션매니저또한 자동으로 주입받을수있음
  잡이 스탭을 실행시키면,스탭은 순서대로 실행되고,이전스탭이 성공해야지만 실행됨
  
2.스프링 배치 시작
*1.스탭의 두가지 유형
  스탭은 크게 청크지향처리와 태스크릿 지향처리 두가지로 나눠짐
  1.태스크릿
	태스크릿은 가장 기본적인 처리방식으로,복잡하지않은 단순작업을 실행할때 사용함
	즉 대량데이터처리가 아닌,단순히 실행에 초점을 맞춘 오래된로그삭제,이동,알림발송등을 할때 사용됨
	즉 함수호출 하나로 끝날만한걸 할때 사용하면됨
	이걸 쓸떈 그냥 execute()에 원하는로직을 구현하고,구현체를 배치에 넘기기만 하면됨

	또한 RepeatStatus를 사용해서 반복을 할수있음(FINISHED, CONTINUABLE)
	FINISHED는 스탭이 종료됐다는거고,CONTINUABLE는 아직 더 진행해야한다는것
	물론 내부에서 반복문을 쓸수도있지만,이렇게 처리하는 이유는 짧은 트랜잭션을 사용해서 안전하게 배치처리를 하기위해서 이런식으로 처리함
	이러면 실패해도 한번에 처리하는양만큼만 실패하지,전체를 다시할필요는없음

	태스크릿을 구현했다면,이걸 스텝에 테스크릿으로 등록하면됨
	이건 @configuration파일에서 태스크릿과 스탭 빈을 만들고,이걸 잡 빈에 넣으면됨
	이떄 트랜잭션매니저와 잡레포지토리가 필요한데,만약 태스크릿작업이 트랜잭션이 필요없다면 ResourcelessTransactionManager을 사용할수있음
	이건 그냥 해당 클래스를 직접 new로 생성해서 스탭의 태스크릿넣는곳에 같이 넣어주면됨

	ResourcelessTransactionManager를 빈으로 만들고싶을땐 주의가 필요함
	이걸 빈으로 만들어버리면 메타데이터관리 트랜잭션에서도 저걸써버리기때문에 별도의 트랜잭션매니저구성을 해야함

	태스크릿을 사용할때,간단한작업이라면 그냥 스탭에서 람다식으로 정의해도되고,좀 복잡하다싶으면 별도클래스로 빼는게 나음
	보통 태스크릿작업을 할땐 태스크릿에서 생성자di로 처리할때 사용할 파라미터(레포지토리나 파일경로등)을 받아서 처리하는경우가 많은듯
	그리고 포인터같은걸 둬서 일정단위로 밀어내면서 처리하고,빈거만나면 끝내고 이런식
	
	즉 태스크릿은 단순하고 명확한 작업을 수행할때 사용되는 step이고
	  단순작업에 적합(알림발송,파일복사,오래된데이터삭제등)
	  Tasklet 인터페이스 구현후 이를 StepBuilder.tasklet()에 전달
	  RepeatStatus로 실행제어
	  트랜잭션지원(execute단위로 트랜잭션이 걸림)
	이런 특징들이 있음
  2.청크
    일반적으로 배치를 처리할때 사용하는 읽기-처리-쓰기를 할땐 청크지향 처리를 사용하면됨
	청크랑 데이터를 일정단위로 쪼갠 덩어리를 말함(백만데이터를 백개단위로 쪼개서 처리 이런느낌)
	이렇게 처리해야 메모리관리,가벼운 트랜잭션(작은실패)등이 가능해짐
	
	배치에서 읽기-처리-쓰기 패턴은 딱 3가지로 나뉘어짐
	이게 ItemReader,ItemProcessor,ItemWriter임
	
	1.ItemReader
	  이건 한번에 하나씩(db 한 row씩) 읽어서 반환하고,읽을데이터가 없다면 null을 반환함
	  이 null을 반환하는게 청크지향처리스탭의 종료시점임
	  
	  또한 이미 아이템리더의 경우 표준구현체들이 많이있음(FlatFileItemReader,JdbcCursorItemReader등)
	2.ItemProcessor
	  이건 데이터를 원하는 형태로 깎아내는,즉 실제 로직을 처리함
	  이것도 리더처럼 데이터 하나하나씩 입력받아서 반환함
	  얘는 보통
	    데이터가공(입력데이터를 출력데이터의 형태로 변환함)
		필터링(null을 반환해서 해당데이터를 처리흐름에서 제외시킴)
		데이터검증(입력데이터의 유효성을 검사,조건에 맞지않는 데이터를 만나면 예외를 발생시켜 잡을 중단시킴)
	  이런 작업을 처리하고,꼭 프로세서가 있을필요는없음,즉 스탭에서 직접 데이터를 읽고바로쓰게할수도있음
	
	3.ItemWriter
	  이건 프로세서의 결과물을 받아서 원하는방식으로 최종저장/출력함
	  이건 청크단위로 한번에 데이터를 쓰고,이단위로 트랜잭션에 묶임
	  이것도 다양한 구현체들이 이미 만들어져있음(FlatFileItemWriter,JdbcCursorItemWriter 등)
	
	이런식으로 크게 3개단위로 분리해서 처리되는게 기본패턴인데,이걸통해
	  완벽한 책임분리
	  재사용성극대화
	  높은 유연성(데이터소스가 바뀌면 리더만,데이터형식이 바뀌면 프로세서만 변경하면됨)
	  대용량처리의 표준
	를 얻을수있음
	
	이 청크지향스탭을 조립할때도 StepBuilder를 사용하면됨
	이때 
	  return new StepBulder("스탭명",jobRepository)
	    .<리더의반환타입,프로세서의반환타입>chunk(청크사이즈,트랜잭션메니저)
		.reader(itemReader())
        .processor(itemProcessor()) 
        .writer(itemWriter())
		.build()
	이런식으로 하면됨
	이떄 스탭의 동작방식은,
	  청크갯수만큼 리더가 읽어서 청크를 구성하고(단일호출로 n개의 데이터를 가져와 하나의 청크를 생성)
	  구성된 청크의 내용물을 하나하나 프로세서가 처리해서 처리완료된 청크로 만들고(단일처리는 맞음,즉 청크크기가 10이면 한 청크를 처리할때 10번호출)
	  처리완료된 청크를 라이터가 저장(이건 청크 전체를 한번에 처리함)
	이런 방식으로 진행됨
	이걸 더이상 읽을데이터가 없을때(read가 null을 반환할때)까지 반복함
	즉 청크크기만큼 read하고,청크크기만큼 process한다음 한번 write해서 저장하는게 한 청크처리임
	
	청크지향처리에서 트랜잭션관리는 각 청크단위로 진행됨
	즉 실패시 해당 청크만 롤백된다는것
	이떄 청크사이즈는 트레이드오프와 업무요구사항,데이터양을 고려해서 적절히 골라야함
	이때 트레이드오프는
	  청크사이즈가 클때
	    메모리에 많은데이터를 한번에 로드함
		트랜잭션범위가 커져서 롤백데이터양이 많아짐
	  청크사이즈가 작을때
	    롤백데이터가 최소화됨
		읽기쓰기io가 자주발생하게됨
	이런 문제가 있음
  
*2.JobParamater
  1.잡파라미터란?
    잡파라미터는 처리대상과 조건등 배치작업에 전달되는 입력값임 
	이걸 사용해서 같은 잡을 입력값만 바꿔서 유연하게 실행할수있음
	물론 -D로 프로퍼티전달을 할순있지만,목적이 서로 다름
	
  2.프로퍼티와 잡파라미터의 결정적 차이
    1.입력값 동적 변경
	  단순한경우 프로퍼티로 충분하지만,웹요청에 전달된값을 Job의 매개변수로 주입하려는등은 프로퍼티로 해결하기힘듬(배치 프로그램을 띄워두고 잡을 실행시키는형태일떄)
	  프로퍼티는 앱 시작시 한번 주입되고 끝이기때문
	
	2.메타데이터
	  스프링 배치는 잡파라미터의 모든값을 메타데이터 저장소에 저장하고,이걸통해
	    Job인스턴스 식별 및 재시작 처리
		Job 실행이력추적
	  등을 할수있음
	  
	  이때 메타데이터는 JobRepository를 통해 Job과 Step의 실행이력을 저장소에 저장하고,
	  여기엔 잡과 스탭의 시작/종료시간,실행상태,처리레코드수 등이 포함됨
	  
	  반면 프로퍼티는 메타데이터로 저장되지않아서 관리가 불가능해,배치운영과 제어를 제한하게됨
	  즉 재시작이나 처리이력관리등이 안되게됨
  3.JobParameters 전달하기
    1.커맨드라인(CLI)
	  커맨드라인에선
	    ./gradlew bootRun --args='--spring.batch.job.name=dataProcessingJob inputFilePath=/data/input/users.csv,java.lang.String'
	  이런식으로 
		 --spring.batch.job.name=실행잡이름 파라미터1이름=값1,파라미터타입 파라미터2이름=값2,파라미터타입
      이렇게 전달하면됨
	  
	  이때 파라미터타입은 DefaultJobParametersConverter를 통해 적절한 타입으로 변환되는데,
	  기본적인 타입과 LocalDateTime등 시간관련타입등 다양한 타입을 지원함
	2.프로그래밍방식
	  이경우엔 JobParametersBuilder컴포넌트를 사용해서 잡 파라미터를 만들고,이걸 JobLauncher에 넣어서 잡을 실행시킬수있음
	  이때 addJobParameter()의 체이닝을 통해 잡파라미터를 추가하고,toJobParameters()로 빌드를 할수있음
	  이것도 
	    addJobParameter(파라미터명,파라미터값,타입)
	  즉
		JobParameters jobParameters = new JobParametersBuilder()
          .addJobParameter("inputFilePath", "/data/input/users.csv", String.class)
          .toJobParameters();
	  이렇게 넣으면됨
	  
	  여기서 JobLauncher는 잡을 실행하는데 사용되는 핵심도구로,잡파라미터를 입력받아 잡의 실행컨텍스트를 생성하고,잡이 성공적으로 실행되게 관리하는역할을 함
	3.잡파라미터 직접 접근
	  잡파라미터에 직접 접근하려면,어디서 잡파라미터가 관리되는지를 알아야함
	  스프링배치에선 JobExecution이 잡의 실행정보를 쥐고있음,즉 잡파라미터도 이안에있음
	  스탭에서 잡파라미터를 찾으려면 이 JobExecution을 통해서 찾아야함
	  테스크릿에서 잡 파라미터를 찾을땐 
	     public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) {
         JobParameters jobParameters = chunkContext.getStepContext()																			            
            .getStepExecution()																				      
            .getJobParameters();
	  이런식으로 컨텍스트에서 익스큐션을 찾고,거기에 잡파라미터를 찾아야함
	  이때 잡익스큐션이 아닌 스텝익스큐션을 사용하는데,스텝익스큐션안에 부모잡의 잡익스큐션을 참조하고있어서 이렇게 찾는거
	  
  4.다양한 타입의 잡파라미터
    1.기본데이터타입
      기본적으로 잡파라미터를 받아서 사용할땐 di로 하게되는데
	     @Bean
         @StepScope //@Value로 잡파라미터를 전달받을떈 @StepScope등을 선언해줘야함
	       public Tasklet terminatorTasklet(
	  	     @Value("#{jobParameters['terminatorId']}") String terminatorId, 
	  	     @Value("#{jobParameters['targetCount']}") Integer targetCount
	       ) { 
	  이런식으로 di받으면됨
	  그리고
	    ./gradlew bootRun --args='--spring.batch.job.name=processTerminatorJob terminatorId=KILL-9,java.lang.String targetCount=5,java.lang.Integer'
      이런식으로 넣으면되는거
	2.날짜와 시간 타입
	  또한 날짜의 경우도 같은방식으로 di받고 
	    executionDate=2024-01-01,java.time.LocalDate startTime=2024-01-01T14:30:00,java.time.LocalDateTime
	  이런식으로 받으면됨
	  이떄 주의점은 LocalDate는 ISO_LOCAL_DATE,LocalDateTime은 ISO_LOCAL_DATE_TIME형식으로 전달해야한다는점
	  다른것도 마찬가지임 java.util.Date는 ISO_INSTANT 형식으로, java.time.LocalTime은 ISO_LOCAL_TIME형식으로 전달해야함
	  
    3.Enum타입
	  enum의 경우엔 받을땐 똑같이
	    @Value("#{jobParameters['questDifficulty']}") QuestDifficulty questDifficulty
	  이렇게하면되고,줄떈
	    questDifficulty=HARD,com.system.batch.killbatchsystem.TerminatorConfig$QuestDifficulty
	  이렇게하면되는데,이때 $는 중첩클래스일땐 저렇게 표시해야함,만약 중첩클래스가 아니라면 그냥 똑같이 경로설정해주면됨
	
	4.POJO를 활용한 잡파라미터주입
	  만약 여러 Job파라미터를 효율적으로 관리해야한다면,별도의 클래스를 만들어 파라미터를 관리하면 구조화와 재사용성이 높아짐
	  즉 잡파라미터 관리용 컴포넌트를 만들고,여기에 StepScope를 붙인다음 여기서 잡파라미터를 다 주입받은다음에 이걸 가져다가 쓰는거
	  어짜피 싱글톤으로 실행되니까
	
	5.기본파라미터 표기법의 한계
	  문제는,기본파라미터에서 쉼표(,)가 파라미터값에 포함되게되면 문제가 발생함 
	  그래서 사용되는게 Json기반의 파라미터표기임
	6.Json기반 표기법
	  기본적으론
	    implementation 'org.springframework.boot:spring-boot-starter-json'
	  이 의존성이 필요함
	  그리고나서 JsonJobParametersConverter빈을 직접 등록해줘야함
	  그리고나서 잡파라미터로
	    ./gradlew bootRun --args="--spring.batch.job.name=terminatorJob infiltrationTargets='{\"value\":\"판교서버실,안산데이터센터\",\"type\":\"java.lang.String\"}'"
	  이렇게 json을 넘겨주면됨
	  받는건 똑같이
	    @Value("#{jobParameters['infiltrationTargets']}") String infiltrationTargets
	  이렇게받으면됨(파라미터명으로)
    7.커맨드라인 파라미터는 어떻게 실제 Job으로 전달될까?
	  배치를 스프링부트3이랑 쓰면,앱이 시작될떄 JobLauncherApplicationRunner라는 컴포넌트가 자동으로 동작함
	  이건 스프링부트가 제공하는 어플리케이션러너중 한 종류로,얘는 커맨드라인으로 전달된 잡파라미터를 해석하고 이걸바탕으로 실제 잡을 실행하는 역할을 맡음
	  얘는
	    잡목록준비:등록된 모든 잡타입 빈을 자동주입받음
		유효성검증:잡타입빈이 여러개인데 잡네임이 없을경우 검증실패,만약 하나라면 생략가능
		명령어해석:커맨드라인으로 전달된 값(잡파라미터)들을 파싱함,
		  여기서 DefaultJobParametersConverter나 JsonJobParametersConverter을 사용해 문자열을 적절한 잡파라미터로 변환함
		Job실행:첫단계의 잡리스트에서 해당이름의 잡을 찾고,해당잡을 잡파라미터를 넣어서 실행시킴,이과정에서 JobLauncher라는 잡실행컴포넌트가 사용됨
	  이런식으로 동작함

  5.JobParametersValidator
    이건 말그대로 잡파라미터 밸리데이터,즉 검증도구임
	이 인터페이스는 단순하게 하나의 검증메서드만 있음 
		public interface JobParametersValidator {
			void validate(@Nullable JobParameters parameters) throws JobParametersInvalidException;
		}	
    말그대로 잡파라미터를 받아서 검증한후 아니다싶으면 예외를 던지면됨
	이건 잡을 등록할때 JobBuilder에서 validator로 등록해주면됨,이러면 잡실행시점에 호출되어 파라미터검증을 진행함
	
	물론 매번 만들긴 귀찮으니까  DefaultJobParametersValidator를 사용할수도 있는데
	이건 그냥 파라미터의 존재여부만 확인해줌
	이건
	  DefaultJobParametersValidator([필수파라미터명],[선택파라미터명])
	이렇게 넣어두면 검증해줌
	그리고 만약 선택파라미터에 값이 있다면,모든 잡파라미터들은 필수나 선택중 한군데에 들어가야함,만약 저기에없는 파라미터가 들어오면실패함
	선택파라미터에 값이 없다면,그냥 자유롭게 허용함
	
*3.배치 Scope  
  1.Job과 Step의 Scope 이해하기
    배치의 스코프는 스프링의 기본 스코프인 싱글톤과 다른 특별한 스코프를 제공함
	JobScope와 StepScope임
	
	얘들이 선언된 빈은 어플리케이션 구동시점에는 프록시로만 존재하다가,잡이나 스탭이 실행된후에 프록시객체에 접근하면 그때 실제빈이 생성됨
	이러면 이점이 뭐냐면,런타임에서 잡파라미터가 결정되어서 실행시점에 정확하게 주입받을수있고,
	동시에 여러잡이 실행되더라도 각각 독립적인 빈을 사용하게되어 동시성문제도 해결됨
	또한 잡이나 스탭의 실행이 끝나면 해당빈도 같이 제거되므로 메모리적으로도 효율적임
  
  2.@JobScope
    잡스코프는 잡이 실행될때 실제 빈이 생성되고 잡이 종료될떄 함께 제거되는 스코프임
	즉 JobExecution과 생명주기를 같이함
	
	이게 붙은 빈은 구동시점에는 프록시만 생성됨
	그래서 어플리케이션 실행중에 잡파라미터가 변경되거나,잡을 실행 직전에 잡파라미터를 만들어서 잡을 실행시키는등의 일이 가능해짐
	또한 잡이 실행될때 실제 빈이 생성되니까 병렬처리도 가능해짐
  
  3.@StepScope
    스텝스코프는 잡스코프와 유사하지만 스텝레벨에서 동작하는 스코프임
	이건 스텝의 실행범위에서 빈을 관리함
	즉 각각 스텝의 실행마다 새로운 빈이 생성되고,스텝이 종료될때 함께 제거됨
	그래서 동시성이슈가 터지지않음

  4.JobScope와 StepScope사용시 주의사항
    1.프록시대상의 타입이 클래스라면,반드시 상속가능한클래스여야함
	  cglib기반의 프록시생성이기떄문에 상속가능해야함
	2.Step빈에는 @StepScope와 @JobScope를 사용하지말라
	  스텝에 StepScope를 달면 스텝빈생성과 스코프활성화시점이 맞지않아 오류가발생함
	  배치는 스텝실행전 메타데이터관리를 위해 스텝빈에 접근해야하는데,이시점엔 스텝이 실행되지않아 스텝스코프가 활성화되지않은상태임
	  그래서 스코프없이 프록시에 접근하니까 안되는거
	  
	  마찬가지로 @JobScope도 스텝에 선언하면안됨
	  단순한 배치에선 문제가 없지만,복잡한 상황에선 예상치못한 문제가 발생함
	    JobOperator을 통한 Step실행제어
		Spring Integration(Remote Partitioning)을 활용한 배치확장기능등
	  이런거
	  
	  그런데 스텝에서 잡파라미터를 쓸땐 스코프가 필요함
	  이럴땐 스텝단위로 스코프를 받는게 아닌,Tasklet에서 스코프를 달아서 파라미터를 받으면됨
	  또 생기는 문제는 잡빌더에서 컴파일시점에 없는값을 참조할때 생기는 문제임
	  스텝에서 받는 잡파라미터는 컴파일시점엔 값이 존재하지않음
	  가장 깔끔한 방법은 스텝을 빈으로 등록하고 이걸 생성자주입받는것
	  
	  그런데 만약 직접 스탭을 호출해야한다면,해당자리에 null을 넣으면 알아서 잡이 실행될때 입력받은 잡파라미터값으로 교체함(지연바인딩)
	  
  5.ExecutionContext
    잡익스큐션과 스텝익스큐션은 시작시간,종료시간,실행상태등의 메타데이터를 관리하는데,이런 기본적인 정보만으로는 시스템을 완벽하게 제어하기 부족할때가 있음
	비즈니스로직 처리중 발생하는 커스텀데이터를 관리할 방법이 필요한데,이때 사용하는게 ExecutionContext라는 데이터컨테이너임
	기본적으로 잡파라미터는 잡의 실행시점에서 바뀌지않는데(불변),그래서 추가적인 데이터저장수단이 필요한것
	
	얘를 쓰면 커스텀컬렉션의 마지막처리인덱스같은 데이터를 저장할수있고,이건 잡이 중단된후 재시작할떄 특히 유용함
	배치가 재시작할때 이걸 자동으로 복원하기때문(이것도 메타데이터저장소에서 관리함)
	
	JobScope와 StepScope에서 ExecutionContext에 접근할때도 @Value로 접근할수있음
	  @Value("#{jobExecutionContext['previousSystemState']}") String prevState
	  @Value("#{stepExecutionContext['targetSystemStatus']}") String targetStatus
	이런식
	단 jobExecutionContext와 stepExecutionContext는 서로 다른 범위를 가지는데,
	jobExecutionContext는 해당 잡에 속한 모든 컴포넌트에서 접근할수있지만,stepExecutionContext는 해당 스텝에 속한 컴포넌트에서만 접근할수있음
	
	즉 스텝의 ExecutionContext에 저장된 데이터는 jobExecutionContext로 가져올수없고,다른 스탭의 stepExecutionContext데이터를 가져올수없음
	이렇게 스텝간 데이터 독립성이 완벽하게 보장됨
	만약 이전스텝의 처리결과를 다음스텝에서 활용해야한다면 jobExecutionContext에 담아서 사용해야함
	즉 jobExecutionContext는 job재시작시 복원과 step간 데이터공유수단으로도 사용됨
  
	
*4.Spring Batch Listener
  리스너는 배치의 주요순간들에 후킹해서 각 시점에 동작을 끼워넣을수있는 도구임
  잡이 시작하기 직전,완료직후,스텝시작직전,완료직후,청크단위,아이템단위등 다양한 위치에 로직을 끼워넣을수있음
  이때 로깅,모니터링,에러처리등 다양한 작업을 할수있음
  1.리스너 종류  
    1.JobExecutionListener
      이건 잡실행의 시작과 종료시점에 호출되는 인터페이스임
	  잡의 실행결과를 로깅,이메일전송하거나 잡 시작전에 필요한 리소스를 준비하고 끝난후에 정리하는등의 작업을 할수있음
	  afterJob은 잡 실행정보가 메타데이터 저장소에 저장되기전에 호출됨
	  그래서 잡의 실행결과를 완료에서 실패로 변경하거나,그 반대로 처리하는등의 작업도 가능함
    
    2.StepExecutionListener
      이건 스탭의 시작과 종료시점에 호출되는 리스너 인터페이스임
	  스텝의 시작시간,종료시간,처리데이터수를 로그로 기록하는등의 사용자정의작업을 추가할수있음
	  또한 afterStep에선 ExitStatus를 반환하는데,이걸통해 스텝의 실행결과상태를 직접 변경할수있음
    
    3.ChunkListener
      청크지향처리는 청크단위로 아이템읽기쓰기를 반복하는데,ChunkListener는 이런 하나의 청크단위처리가 시작되기 직전,완료된후,에러가 발생했을때 호출됨
	  즉 청크단위로 모니터링하거나 로깅할수있음
	  여기서 afterChunk는 트랜잭션이 커밋된후 호출되고,
	  청크처리중 예외가 발생하면 afterChunkError이 afterChunk대신 호출되는데,이땐 트랜잭션롤백이후 호출됨
    
    4.Item[Read|Process|Write]Listener
      아이템리스너는 아이템의 읽기,쓰기,처리작업이 수행되기 직전,직후,에러발생시점에 호출됨
	  여기서 주의점은
	    ItemReadListener.afterRead()는 read호출이후 호출되지만,null을 반환할떈 호출되지않음
	    ItemProcessListener.afterProcess()는 process가 null을 반환하더라도(필터링하더라도) 호출됨
	    ItemWriteListener.afterWrite()는 트랜잭션이 커밋되기전,그리고 ChunkListener.afterChunk()가 호출되기전에 호출됨
    
  2.배치 리스너, 이런 것들을 할 수 있다
    배치 리스너에선
	  단계별 모니터링과 추적:각 잡이나 스탭의 실행전후 로그를 남길수있고,이떄 언제시작하고 언제끝났는지,몇개데이터를 처리했는지등을 기록하고 추적할수있음
	  실행결과에 따른 후속처리:잡과 스텝의 실행상태를 리스너로 직접 확인하고 그에따른 조치를 할수있음(잡이 실패했을떄 다시돌린다든가)
	  데이터 가공과 전달:실제 처리로직 전후에 데이터를 추가로 정제하거나 변환할수있음
	    StepExecutionListener나 ChunkListener를 사용해서 ExecutionContext의 데이터를 수정하거나 필요한 데이터를 추가하는등
		다음 처리에 필요한내용을 미리 준비할수있음
	  부가기능분리:주요처리로직과 부가로직을 깔끔하게 분리할수있음(aop처럼)

    1.배치리스너 구현방법
      리스너 구현법은 두가지가 있음
	  전용 리스너 인터페이스를 구현하던가,어노테이션을 붙이던가
	  가장 일반적인 방법은 그냥 리스너 인터페이스를 구현한 후에 Builder에서 .listener()에 넣어주면됨
	  이떄 JobExecutionListener는 JobBuilder에,나머지 빌더들은 StepBuilder에 넣으면됨
	  
	  더 간단한방법은 어노테이션기반구현임
	    @BeforeJob, @AfterJob, @BeforeStep, @AfterStep //이런느낌,밑에는 모든 어노테이션들
	    @AfterChunk, @AfterChunkError, @AfterJob, @AfterProcess, @AfterRead, @AfterStep, @AfterWrite, @BeforeChunk, @BeforeJob, @BeforeProcess, @BeforeRead, @BeforeStep, @BeforeWrite, @OnProcessError, @OnReadError, @OnSkipInProcess, @OnSkipInRead, @OnSkipInWrite
	  그냥 리스너클래스를 상속없이 만든다음,저 어노테이션을 붙인 메서드를 만들고,이걸 위에처럼 Builder에 넣으면됨
    
    2.JobExecutionListener와 ExecutionContext를 활용한 동적 데이터 전달
      만약 잡파라미터만으로 전달할수없는 동적데이터가 필요할땐 JobExecutionListener의 beforeJob()를 사용해서 동적데이터를 각 스텝에 전달할수있음
  	  저기에다가 
	    jobExecution.getExecutionContext().put(데이터)
	  이런식으로 데이터를 집어넣는식
	  그리고 스탭에서 값을 꺼내다가 쓰면됨

    3.왜 JobParameters가 아닌 ExecutionContext를 사용할까?
	  잡파라미터는 한번 생성된 이후 변경될수없음(불변)
	  이걸통해 배치의 재현가능성과 일관성을 스프링배치는 보장함
	    재현가능성:같은 잡파라미터로 실행된 잡은 항상 같은결과를 생성해야함,중간에 변경될경우 이를 보장할수없음
	    추적가능성:배치작업의 실행기록과 잡파라미터는 메타데이터저장소에 저장됨,이게 변경가능하다면 기록과 실제작업간 불일치가 발생할수있음
	  그래서 동적으로 생성되거나 변경되어야 하는 데이터는 ExecutionContext를 통해 관리해야함
	
	  단,동적으로 전달하는건 유용하긴하지만 잡파라미터만으로 충분히 처리할수있다면 그걸로 처리하는게 좋음
	  만약 beforeJob에서 localDate.now등으로 현재날짜를 넘긴다면 그날데이터를 재처리하는등의 작업이 불가능해짐
	  이런건 외부에서 파라미터로 받는게 나은방식임
	  즉 가급적 잡파라미터로 di받고,외부에서 값을 받을수없는경우에만 동적으로 생성하던가 하면됨
	
	  그런데 잡수준의 ExecutionContext에선 모든스탭에서 공유되지만,스탭수준의 ExecutionContext에선 다른 스탭과 공유가 불가능함
	  그래서 스탭의 컨텍스트에서 값을 가져와서,잡의 컨텍스트로 밀어넣는 작업이 필요해지는데,이게 코드가 너무 많이필요하니까 배치에서 만들어둔 리스너가 있음
	  ExecutionContextPromotionListener임

    4.ExecutionContextPromotionListener를 활용한 스탭간 데이터 공유
      이건 스탭수준의 컨텍스트데이터를 잡수준 컨텍스트로 등록시켜주는 구현체임
	  즉 스탭데이터를 잡데이터로 프로모션(승격)시켜주는거임
	
	  이걸 리스너로 스탭에 등록해두면,거기에 세팅된 데이터의 키를 바탕으로 자동으로 승격처리를 시킴
	    @Bean
		public ExecutionContextPromotionListener promotionListener() {
			ExecutionContextPromotionListener listener = new ExecutionContextPromotionListener();
			listener.setKeys(new String[]{"targetSystem"});
			return listener;
		}
	  이런식임
	
	  단 가급적이면 각 스텝은 가능한 독립적으로 설계하는게 재사용성과 유지보수성이 올라감
	  불가피하지않다면 스텝간 데이터의존성은 최소화하는게 좋음

    5.Listener와 @JobScope, @StepScope 통합
      리스너와 스코프를 활용하면 리스너에서 잡파라미터를 쉽게 다룰수있음
	   @JobScope
	   public JobExecutionListener systemTerminationListener(
			   @Value("#{jobParameters['terminationType']}") String terminationType
	   ) {  
	  이렇게 받고
        return new JobBuilder("killDashNineJob", jobRepository)
	      .listener(systemTerminationListener(null))  // 파라미터는 런타임에 주입
	  컴파일시점에(빌더에) null을 넘겨주면됨
    
	  이때 JobExecutionListener라면 @JobScope를 붙여야함(잡의 실행과 생명주기를 같이하니까)
	  이렇게 주입받은 파라미터는 리스너의 어느메서드든 자유롭게 사용할수있음

  3.Listener 마지막 훈련: 성능과 모범 사례
    1.리스너를 효과적으로 다루는법
	  범위와 목적에 따라 적절한 리스너를 선택해야함
	    JobExecutionListener:잡의 시작과 종료를 통제
		StepExecutionListener:스탭의 단계를 통제
		ChunkListener:시스템을 청크단위로 제어하거나,반복의 시작과 종료시점을 통제
		ItemLintener:개별아이템 식별 통제
    2.예외처리는 신중하게
	  beforeJob과 beforeStep에서 예외가 발생하면 잡이나 스탭이 실패한걸로 취급되고,
	  afterJob이나 afterStep에서 예외가 발생하면 무시됨
	  만약 before쪽에서의 예외가 중요하지않다면 잡아서 무시할수있음
	
	3.단일책임원칙 준수
	  리스너에선 로깅 모니터링등 각 리스너별 책임만 담당하고,메인로직은 분리해야함
	  리스너가 너무 많은일을하면 유지보수가 어려워지고 시스템동작파악이 어려워짐
	
	4.성능최적화를 위한 경고
	  1.실행빈도를 고려하라
	    JobExecutionListener/StepExecutionListener의 경우엔 잡이나 스탭별로 한번씩만 실행되니까 작업이 좀 무거워도됨
	    그런데 ItemReadListener/ItemProcessListener는 매 아이템마다 실행되니까 치명적인 문제가 될수있음
      2.리소스사용을 최소화하라
	    db연결,파일io,외부api호출을 최소화하고,리스너내 로직은 가능한 가볍게 유지하는게 좋음
		특히 아이템단위리스너에서 더더욱 중요함
  
3.파일처리배치
*1.FlatFileItemReader
  1.청크지향처리
    데이터처리는 단순히 읽어서 가공하고 쓴다 딱 3개임
	청크처리도 마찬가지임
	  데이터읽기(ItemReader):데이터를 하나씩 읽어들임,청크사이즈만큼 반복실행
	  데이터가공(ItemProcessor):읽어들인 데이터를 원하는대로 가공,필터링도 수행
	  데이터저장(ItemWriter):청크단위로 모아진 데이터를 한번에 처리
	이렇게 3개가 다임
  2.파일기반 배치처리
    개발자가 직접 파일처리를 구현한다면 보일러플레이트도 많이생기고 귀찮아짐
	그래서 스프링배치에서는 파일기반 아이템리더와 라이터를 제공해줌
	이걸쓰면 io작업,데이터파싱,유효성검사,예외처리까지 다 처리해줌
	
	1.FlatFile
	  플랫파일이란 단순하게 행과 열로만 구성된파일,즉 csv같은거임
	  얘들은
	    각라인이 하나의 데이터 로우,\n이 레코드의 끝을 의미함
		쉼표나 탭,스페이스등으로 필드를 구분할수있고 고정길이로 구분할수도있음
		거의 모든시스템에서 읽고쓸수있는 표준형식,엑셀,db등 다양한 도구와 호환되고 사람이 읽기도 쉽고 대용량처리도 쉬움
	  이런 특징이 있음
	2.FlatFileItemReader
	  이건 플랫파일로부터 데이터를 읽어오는 클래스임
	  얘는 파일을 한줄씩 읽어서 지정한 도메인객체로 변환해서 반환함
	  즉
	    파일을 한줄씩 읽어온다
		읽어온 한줄의 문자열을 우리가 사용할 객체로 변환해 리턴한다
	  이런식으로 동작함
	  
	  이때 한줄의 데이터를 변환할땐 LineMapper이라는 컴포넌트 인터페이스를 사용하는데,
	  이건 단순하게 구분자로 분리해낸다음 각 분리한 데이터를 각 지정된 객체의 프로퍼티에 정확하게 매핑해야함
	  물론 이 인터페이스를 우리가 구현할필요는 없음
	  DefaultLineMapper이라는 기본구현체를 제공하고있기때문
	  
	3.DefaultLineMapper
	  이건 크게 두단계로 동작함
	    1.토큰화:하나의 구분자나 고정길이단위로 잘린 문자열을 각 토큰단위로 분리함//LineTokenizer사용하고,여기서도 인터페이스로 구분자나 고정길이별 구현체존재
		2.객체매핑:분리된 토큰을 도메인객체의 프로퍼티에 매핑함//FieldSetMapper사용하고,기본값은 BeanWrapperFieldSetMapper사용
	  즉 단순하게 토큰으로 만든다음 매핑하는걸 담당함
	  이때 매핑의 기본값인 BeanWrapperFieldSetMapper는 자바빈규약을 따르는 객체에 데이터를 매핑해줌
	  그래서 필드의 이름과 매핑객체의 프로퍼티명이 동일해야하고 setter가 필요함
	
	4.구분자로 분리된 형식의 파일 읽기
	  이걸 사용할땐 단순하게 빈으로 등록한다음 사용하면됨
		@Bean
		@StepScope
		public FlatFileItemReader<SystemFailure> systemFailureItemReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new FlatFileItemReaderBuilder<SystemFailure>()
					.name("systemFailureItemReader")
					.resource(new FileSystemResource(inputFile))
					.delimited()
					.delimiter(",")
					.names("errorId",
							"errorDateTime",
							"severity",
							"processId",
							"errorMessage")
					.targetType(SystemFailure.class)
					.linesToSkip(1)
					.build();
		}	  
	  이렇게 빌더로 
	    아이템리더 식별자를 넣고,
		타겟위치(파일위치)를 넣어주고
		구분자와 필드명을 넣어주고,
	    타겟클래스를 저장해주고,
		맨첫줄이 필드명이라면 스킵도 넣어주고 
		빌드한후 
	  스탭에서 가져다쓰면됨
	  또한 제네릭은 컴파일시점의 객체타입을 지정하고,타겟클래스는 런타임시점의 객체타입을 지정함
	  그리고 FlatFileItemReader는 #을 기본적으로 주석으로 인식하는데,만약 다른식으로 주석을 썼다면 comments("//")로 지정해줄수도있음
	  
	  그리고 파일 누락시 예외를 발생시키는걸 컨트롤하려면 .strict(true/false)를 사용하면됨
	  기본적으로는 true고 false로 하면 아이템리더가 null을 바로 반환하는식임
	  또한 라인토크나이저에서도 true면 한 row의 토큰리스트의 갯수가 전달된 객체프로퍼티(names)와 다르면 예외를 발생시키고,false면 자동으로 보정함
    
	5.고정길이형식의 파일 읽기
	  고정길이파일은 각 필드가 고정된길이로 맞춰진 텍스트파일임
	  이땐 각 필드의 길이를 스프링배치에 알려야함
		@Bean
		@StepScope
		public FlatFileItemReader<SystemFailure> systemFailureItemReader(
			   @Value("#{jobParameters['inputFile']}") String inputFile) {
		   return new FlatFileItemReaderBuilder<SystemFailure>()
			   .name("systemFailureItemReader")
			   .resource(new FileSystemResource(inputFile))
			   .fixedLength()
			   .columns(new Range[]{
				 new Range(1, 8),     // errorId: ERR001 + 공백 2칸
				 new Range(9, 29),    // errorDateTime: 날짜시간 + 공백 2칸
				 new Range(30, 39),   // severity: CRITICAL/FATAL + 패딩
				 new Range(40, 45),   // processId: 1234 + 공백 2칸
				 new Range(46, 66)    // errorMessage: 메시지 + \n
			   })
			   .names("errorId", "errorDateTime", "severity", "processId", "errorMessage")
				.targetType(SystemFailure.class)
			   .build();
		}
      이런식으로 구분자랑 거의 비슷한데,fixedLength()로 표시한후 .columns로 각 범위를 표시해주면됨	
	  그리고 .strict()도 true면 추가적으로 파일의 읽은길이를 엄격하게 검증함
	  파일의 읽은 라인길이가 range에 지정된 최대길이(마지막필드의 끝위치)와 다르다면 예외를 발생시킴
	  또한 공백의경우 내부적으로 trim을 돌리기때문에 신경안써도됨
	
	6.프로퍼티 타입에 LocalDateTime을 쓰고 싶다면?
	  시간타입을 사용하고싶으면 별도의 변환기가 필요함
	  BeanWrapperFieldSetMapper는 이런 복잡한 변환까진 처리하지않는데,
	  그래서 빌더에서 customEditors()를 사용해서 커스텀 propertyEditer를 등록할수있음
	    .customEditors(Map.of(LocalDateTime.class, customEditor()))	  
	  즉 propertyEditer를 구현하고,이걸 빌더의 .customEditors를 사용해 등록하면됨 
  
  
    7.RegexLineTokenizer
	  이건 복잡한 형식의 파일을 처리할때 사용하는 도구임
	  이걸 등록하는건 아이템리더빌더에서 .lineTokenizer()을 사용하면되고,
        RegexLineTokenizer tokenizer = new RegexLineTokenizer();
        tokenizer.setRegex("\\[\\w+\\]\\[Thread-(\\d+)\\]\\[CPU: \\d+%\\] (.+)");
	  이런식으로 정규식을 사용해서 토크나이저를 할수있음
    8.fieldSetMapper()
	  마찬가지로 fieldSetMapper도 커스텀을 등록할수있음(필드 매핑하는클래스)
		.fieldSetMapper(fieldSet -> new LogEntry(fieldSet.readString(0), fieldSet.readString(1)))
	  단 이걸 사용할땐 targetType를 사용하면안됨
	  스프링 배치 5.2.3이전버전이면 에러도 안뜨고 fieldSetMapper가 무시되고,이후버전이면 예외가 뜸(즉 둘다 설정하려고하면 예외발생)
	
	9.PatternMatchingCompositeLineMapper 
	  만약 하나의 파일안에 여러형식의 라인이 혼재되어있다면 이걸 사용할수있음
	  이건 패턴매칭을 지원해서,각 라인의 패턴을 먼저 파악한후,해당 패턴에 맞는 토크나이저와 필드매퍼를 적용할수있음
		@Bean
		@StepScope
		public FlatFileItemReader<SystemLog> systemLogReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new FlatFileItemReaderBuilder<SystemLog>()
					.name("systemLogReader")
					.resource(new FileSystemResource(inputFile))
					.lineMapper(systemLogLineMapper())
					.build();
		}
	  등록은 이렇게 등록한후에
		@Bean
		public PatternMatchingCompositeLineMapper<SystemLog> systemLogLineMapper() {
			PatternMatchingCompositeLineMapper<SystemLog> lineMapper = new PatternMatchingCompositeLineMapper<>();

			Map<String, LineTokenizer> tokenizers = new HashMap<>();
			tokenizers.put("ERROR*", errorLineTokenizer());
			tokenizers.put("ABORT*", abortLineTokenizer());
			tokenizers.put("COLLECT*", collectLineTokenizer());
			lineMapper.setTokenizers(tokenizers);

			Map<String, FieldSetMapper<SystemLog>> mappers = new HashMap<>();
			mappers.put("ERROR*", new ErrorFieldSetMapper());
			mappers.put("ABORT*", new AbortFieldSetMapper());
			mappers.put("COLLECT*", new CollectFieldSetMapper());
			lineMapper.setFieldSetMappers(mappers);

			return lineMapper;
		}
	  이렇게 각 종류에 맞는 토크나이저와 매퍼를 등록하면됨(앞에있는게 조건임)
	  여기선 맨앞의 값으로 구분했지만 패턴이니까 패턴만 맞추면 어떤형식이든 가능
	
	10.RecordFieldSetMapper: Record 매핑 지원
	  만약 targetType을 record타입으로 잡는다면 자동으로 BeanWrapperFieldSetMapper 대신 RecordFieldSetMapper를 사용함
	  동작자체는 비슷하니까 신경안써도됨
	
	11. MultiResourceItemReader:여러 파일 읽기
	  여러파일을 읽어야할때(여러서버의 로그파일들을 통합분석등),각 파일마다 스탭을 만드는건 비효율적임
	  이때 사용하는게 MultiResourceItemReader임
	  이건 여러파일들을 순차적으로 읽게 해주는 ItemReader구현체임
	  그냥 말그대로 여러파일들은 순서대로 읽음
	  
	  얘가 직접 읽진않고,위임대상 ItemReader에게 실제 읽기를 맡기고,걔한테 일주는일을 담당함
	  그래서 이걸 사용할땐 두가지 구성이 필요함
	    실제로 사용할 ItemReader
		읽어들일 파일의 목록
	  그래서 이런식으로 구성됨
		return new MultiResourceItemReaderBuilder<SystemFailure>()
		  .name("multiSystemFailureItemReader")
		  .resources(new Resource[]{
			  new FileSystemResource(inputFilePath + "/critical-failures.csv"),
			  new FileSystemResource(inputFilePath + "/normal-failures.csv")
		  })
		  .delegate(systemFailureFileReader())
		  .build();	 
	  resources로 파일들을 지정해주고,delegate로 실제 사용할 리더를 넣어주는식
	  단 여기서 중요한건,systemFailureFileReader() 빈생성시에 resource가 없어야함(그건 우리가 넣어줘야하니까),나머진 동일함
	  또한 resources의 파일읽기순서는 파일명의 알파벳순서로 기본적으로 읽고,이걸 바꾸고싶다면 comparator()메서드로 원하는 정렬기준을 지정할수있음
	
*2.FlatFileItemWriter 	  
  이건 데이터를 플랫파일형식으로 쓰는작업을 담당함,즉 도메인객체를 문자열로 변환해서 파일에 써내려감
  이것도 리더랑 거의비슷하게 선언해서 빈등록하면됨
  
  얘가 하는일은 필드추출과 문자열결합 두개로 나눠볼수있고,필드추출은 FieldExtractor,문자열결합은 LineAggregator이 담당함
  1.필드추출과 라인결합
    필드추출은 도메인객체에서 필드를 추출하는역할을 함
	즉 T를 받아서 object[]를 내보내는 역할을 함
	
	이것도 당연히 기본구현체가 있는데 BeanWrapperFieldExtractor 와 RecordFieldExtractor 임
	BeanWrapperFieldExtractor는 빈객체로부터 필드를 추출하는거고,RecordFieldExtractor는 레코드객체에서 필드를 추출함
	배치는 파일의 도메인타입에 따라 이 두개중 하나를 자동선택함
	
	문자열결합은 추출한 데이터들을 하나의 문자열로 결합하는 역할을 함
	이것도 구분자기반으로 읽던가 고정길이로 읽던가가 나뉜거처럼,여기도 2개로 나뉘는데
	DelimitedLineAggregator는 구분자로 구분하는거고 FormatterLineAggregator는 고정길이를 포함한 다른형식으로 쓸때 사용함
	이거 선택은 직접 라이터빌더설정에서 delimited()와 formatted()를 사용해서 선택할수있음
	
	또한 내부적으로는 LineAggregator가 FieldExtractor를 합성한 형태로 구성되어있음
	그래서 객체를 문자열로 변환하는 전과정은 LineAggregator가 담당함
	
	기본적으로 단일객체를 이렇게 바꾸고,라이터는 청크단위로 동작하니까 각 아이템마다 이걸 반복해서처리함

  2.구분자형식의 파일쓰기
    아이템 라이터는 기본적으로
	    @Bean
		@StepScope
		public FlatFileItemWriter<DeathNote> deathNoteWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new FlatFileItemWriterBuilder<DeathNote>()
				.name("deathNoteWriter")
				.resource(new FileSystemResource(outputDir + "/death_notes.csv"))
				.delimited()
				.delimiter(",")//구분자
				.sourceType(DeathNote.class)
				.names("victimId", "victimName", "executionDate", "causeOfDeath")
				.headerCallback(writer -> writer.write("처형ID,피해자명,처형일자,사인")) //파일헤더
				.build();
		}
	이런형태로 만들어짐
	FieldExtractor를 전달할때는 빈등록을 하던가 빌더의 fieldExtractor()메서드로 직접 넣어주면됨
	그리고 FieldExtractor가 객체로부터 어떤 필드를 추출할지를 알려줘야하는데,이게 .names()임
	단 이때 지정한순서대로 쓰여지니까 순서조심해야함
	그리고 names()는 자동구성방식에서만 사용되고,fieldExtractor()메서드로 직접 커스텀 fieldExtractor를 넣었을경우엔 무시됨
	RecordFieldExtractor를 사용할땐 names가 무시되는 버그가 있으니 조심
	Record타입을 sourceType으로 전달할경우에 자동으로 RecordFieldExtractor 가 사용됨
	커스텀FieldExtractor의 경우엔 setNames으로 필드명을 지정할수있음
		
	이때 스텝의 청크는
	  .<DeathNote, DeathNote>chunk(10, transactionManager)
	이런형태로 리더의 출력,라이터의 입력타입을 넣어주면됨

  3.커스텀 포맷 형식으로 파일 쓰기
    커스텀포맷을 사용할땐 FormatterLineAggregator를 사용할수있음
		@Bean
		@StepScope
		public FlatFileItemWriter<DeathNote> deathNoteWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new FlatFileItemWriterBuilder<DeathNote>()
					.name("deathNoteWriter")
					.resource(new FileSystemResource(outputDir + "/death_note_report.txt"))
					.formatted()
					.format("처형 ID: %s | 처형일자: %s | 피해자: %s | 사인: %s")
					.sourceType(DeathNote.class)
					.names("victimId", "executionDate", "victimName", "causeOfDeath")
					.headerCallback(writer -> writer.write("================= 처형 기록부 ================="))
					.footerCallback(writer -> writer.write("================= 처형 완료 =================="))
					.build();
		}	
	이런식으로 formatted()를 사용한후 format로 형식을 지정해주고, .names()의 순서대로 집어넣으면됨

  4.파일 처리 옵션
    현재 
	  .resource(new FileSystemResource(outputDir + "/death_note_report.txt")) 
	이런식으로 하면 실행할때마다 파일이 계속 덮어쓰기됨
	그래서 동작방식을 선택할수있는데,
		shouldDeleteIfExists: 기존 파일의 삭제 여부(FlatFileItemWriterBuilder.shouldDeleteIfExists()를 사용해 지정)
		append: 기존 파일에 데이터 덧붙이기 여부(FlatFileItemWriterBuilder.append() 메서드를 사용해 지정)
		shouldDeleteIfEmpty: 빈 결과 파일 처리 여부(FlatFileItemWriterBuilder.shouldDeleteIfEmpty() 메서드를 사용해 지정)
	이런식으로 여러 옵션들이 있음
	
	shouldDeleteIfExists는 기본값이 true로,겹치는파일이 있다면 삭제하고 새로 파일을 생성함
	만약 false라면 겹칠때 예외를 던짐
	
	append는 기본값이 false로,true로 설정하면 shouldDeleteIfExists는 자동으로 false로 설정되고,기존파일에 데이터를 추가함
	
	shouldDeleteIfEmpty는 기본값이 false로,파일에 헤더와 푸터를 제외한 데이터가 하나도없으면 파일을 삭제할지여부를 나타냄 	
	여기서 주의할건,해당파일에 얼마나 데이터가 있냐가 아닌,이번에 얼마나 데이터를 썼냐가 기준이라 append를 true로 설정해뒀는데 아무것도 안썼으면 지워버리니 주의
	
  5.FlatFileItemWriter의 롤백 전략: 버퍼링을 통한 안전한 파일 쓰기
    파일은 db와 달리 이미 쓰여진 데이터를 롤백할수없음
	그래서 FlatFileItemWriter는 데이터를 즉시 쓰지않고 내부버퍼에 일시적으로 저장해두다가,
	트랜잭션이 커밋될때(beforeCommit()이 호출될때) 버퍼의 데이터를 파일에 씀
	즉 트랜잭션과 같은범위로 성공실패를 시킬수있음
	이건 FlatFileItemWriterBuilder의 transactional()메서드로 설정할수있고,기본값은 true임

  6.파일 쓰기와 OS 캐시: forceSync 옵션
	os는 기본적으로 매번 디스크에 파일을 쓰지않고,메모리캐시에 먼저 저장한후 파일쓰기를 하는데 이 시차때문에 유실이 발생할수있음
    forceSync()를 true로 설정하면 이 시차를 없애서 즉시 동기화할수있음
	단 잦은동기화로 성능저하가 발생할수있음,기본값은 false임

  7.대용량 파일의 분할 처리: MultiResourceItemWriter
    한파일에 적게되는 양이 너무 커질경우,파일을 나눠서 저장해야하는 필요성이 생기는데,이때 사용되는게 MultiResourceItemWriter임
	이건 이름그대로 여러 리소스에 데이터를 분배하는 ItemWriter구현체임
	애는 직접 파일을 쓰지않고,쓰는작업은 쓰기를 할 ItemWriter에 위임함
	
	그래서 이걸 사용할땐
		@Bean
		@StepScope
		public MultiResourceItemWriter<DeathNote> multiResourceItemWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new MultiResourceItemWriterBuilder<DeathNote>()
					.name("multiDeathNoteWriter")
					.resource(new FileSystemResource(outputDir + "/death_note"))
					.itemCountLimitPerResource(10)
					.delegate(delegateItemWriter())
					.resourceSuffixCreator(index -> String.format("_%03d.txt", index))
					.build();
		}
	이런식으로 delegate에 실제로 사용할 아이템라이터를 넣어주고,
	itemCountLimitPerResource로 한 파일에 저장할 갯수를 설정한다음,resourceSuffixCreator로 어떤식으로 구분할지를 적어주면됨
	
	사용은 그냥 아이템라이터처럼쓰면됨

*3.JSON 파일 읽고 쓰기
  1.커스텀 LineMapper를 활용한 json 문자열 읽기
    만약 줄마다 각각 json객체로 이루어진
	  {...} 
	  {...} 
	  {...}
	이런형태가 있다면(jsonl),그리고 만약 특정객체로 파싱해야한다면
	이럴땐 커스텀 라인매퍼를 만들어야함,간단하게는 아이템리더빌더에 람다식을 써서 만들수있음
	  .lineMapper((line, lineNumber) -> objectMapper.readValue(line, SystemDeath.class))
	기본적으로 JsonLineMapper을 제공하긴하지만 특정객체를 요구한다면 이렇게 간단하게 커스텀객체를 만들수있음
	
	그리고 각 줄마다 json객체가 있는게 아닌,제대로 포맷팅된 객체가 올수도있음(하나의 json객체가 여러줄에 걸쳐 기록된경우)
	이럴땐 리더빌더에 recordSeparatorPolicy를 추가해 하나의 레코드가 어디서 끝나는지를 결정하면됨
	  .recordSeparatorPolicy(new JsonRecordSeparatorPolicy())//이건 기본적으로 있는거임
	RecordSeparatorPolicy 인터페이스는
	  public interface RecordSeparatorPolicy {
		boolean isEndOfRecord(String line); // 해당 줄이 레코드의 끝인지 판단.
		String postProcess(String record); // 레코드가 완성된 후 추가 처리.
		String preProcess(String record); // 레코드를 시작하기 전 사전 처리.
	  }
	이런식으로 구성되고
	이걸 구현한 JsonRecordSeparatorPolicy는 여는중괄호와 닫는중괄호의 갯수가 동일하고,현재읽은라인이 닫는중괄호로 끝나면 레코드가 완료된걸로 판단함
	
	그리고 만약 Json배열로 포장된 데이터라면 
	  [{...},{...},{...}]
	JsonItemReader를 사용할수있음
	이건 resource에 json의 소스를 지정하고(파일,url등 다양한걸 지원)
	  .resource(new UrlResource("http://kill-batch-system.com/array_system_death.json"))
	JsonObjectReader구현체를 선택할수있음  
	  Jackson: JacksonJsonObjectReader
	  Gson: GsonJsonObjectReader
	즉
		@Bean
		@StepScope
		public JsonItemReader<SystemDeath> systemDeathReader(
				@Value("#{jobParameters['inputFile']}") String inputFile) {
			return new JsonItemReaderBuilder<SystemDeath>()
					.name("systemDeathReader")
					.jsonObjectReader(new JacksonJsonObjectReader<>(SystemDeath.class))
					.resource(new FileSystemResource(inputFile))
					.build();
		}
	이런식으로 오브젝트리더와 리소스위치를 넣어주고 만들어서 쓰면 알아서해줌
  2.JsonFileItemWriter	
	비슷하게 JsonFileItemWriter를 사용하면 객체를 json배열로 변환해 파일로 저장할수있음
	얘는 내부적으로 JsonObjectMarshaller를 사용해 객체를 json문자열로 변환함
	얘는 
	  WritableResource: JSON 데이터를 저장할 대상 파일을 나타내는 Spring의 WritableResource.
	  JsonObjectMarshaller: 객체를 JSON 형식으로 마샬링하는 JSON 객체 변환기
	    Jackson: JacksonJsonObjectMarshaller
	    Gson: GsonJsonObjectMarshaller
	이렇게 얘도 저장위치와 json매퍼 두가지만 넣어주면됨 
		@Bean
		@StepScope
		public JsonFileItemWriter<DeathNote> deathNoteJsonWriter(
				@Value("#{jobParameters['outputDir']}") String outputDir) {
			return new JsonFileItemWriterBuilder<DeathNote>()
					.jsonObjectMarshaller(new JacksonJsonObjectMarshaller<>())
					.resource(new FileSystemResource(outputDir + "/death_notes.json"))
					.name("logEntryJsonWriter")
					.build();
		}
	이렇게 생성하면되고
	
4.데이터베이스 배치
*1.rdbms 읽고쓰기	
  웹의 경우는 대부분 단일레코드를 조회하는게 일반적이지만,배치의 경우 모든 사용자데이터,특정조건을 만족한 데이터등 대량처리가 일반적임
  그래서 oom을 막기위해 
    SELECT * FROM users WHERE created_at >= '2024-01-01'
  이렇게 단순한 조회가 아닌 페이징이나 커서기반의 처리를 하게됨
  
  1.Spring Batch의 두 가지 생존 전략
    커서기반처리는 JdbcCursorItemReader 를 사용함,특징은
	  데이터베이스와 연결(커넥션)을 유지하면서 데이터를 순차적으로 가져옴
	  하나의 커넥션으로 데이터를 스트리밍하는거처럼 처리
	  메모리를 최소한으로 사용하면서 최대성능을 뽑아냄
	가 있고
	페이징 기반 처리는 JdbcPagingItemReader를 사용함,특징은
	  데이터를 정확한 크기로 잘라서 차근차근처리
	  각 페이지마다 새쿼리를 날려 안정성을 보장함
	이런특징이 있음

  2.JdbcCursorItemReader
    커서기반처리는 db와 커넥션을 유지한채 데이터를 resultSet으로 순차적으로 가져오는방식임
	즉 커서를 한칸씩 내리면서 읽는느낌
	이건 메모리사용량은 최소화할수있지만,db커넥션은 유지되므로 긴 배치작업동안 커넥션이 너무 오래 유지된다는 단점이 있음
	
	얘는
	  dataSource //db연결관리
	  sql //쿼리
	  RowMapper //ResultSet->자바객체변환
	  PreparedStatement //쿼리실행 및 결과조회
	  PreparedStatementSetter //파라미터 동적바인딩,옵셔널
	대략적으로 이렇게 구성됨
	
	dataSource는 db의 접속통로로,application.yaml에 적힌값을 자동으로 주입받아서 사용됨
	sql은 데이터조회에 사용할 쿼리
	
	RowMapper는 결과데이터를 객체로 변환하는,LineMapper와 비슷한애임
	스프링배치에서 RowMapper를 다루는 방법은
	  BeanPropertyRowMapper:setteer기반 매핑,자바빈객체에 db칼럼명과 필드명이 일치하면 자동매핑
	  DataClassRowMapper:data클래스나 record같은 불변객체를 위해 설계된 구현체,생성자로 매핑하고 생성자가 없는건 setter로 넣음
	  Custom RowMapper:복잡한 변환로직이 필요할때 직접 구현
	이렇게 3개로 나뉨
	
	PreparedStatement는 쿼리실행기로,이걸로 sql을 실행하고 그 결과를 ResultSet으로 가져옴
	PreparedStatementSetter는 동적으로 파라미터값을 주입할때 사용됨,기본값은 ArgumentPreparedStatementSetter로,?위치에 순서대로삽입함
  
    1.JdbcCursorItemReader 구성하기
 	  이것도 빌더로 빈만들면됨
		@Bean
		public JdbcCursorItemReader<Victim> terminatedVictimReader() {
		  return new JdbcCursorItemReaderBuilder<Victim>()
			.name("terminatedVictimReader")
			.dataSource(dataSource)
			.sql("SELECT * FROM victims WHERE status = ? AND terminated_at <= ?")
			.queryArguments(List.of("TERMINATED", LocalDateTime.now()))
			.beanRowMapper(Victim.class) //목표객체클래스,빈이라서 beanRowMapper를 사용했고,커스텀이라면 rowMapper를 사용
			.build();	
	  이렇게 그냥 그대로 넣어주면됨
	  여기서 dataSource는 클래스레벨로 DI받은다음 사용하면됨(DataSource dataSource)
	  만약 ResultSet을 데이터클래스나 Record를 사용하려면 DataClassRowMapper를 사용하면됨 
	    .rowMapper(new DataClassRowMapper<>(Victim.class)) //배치 5.2 이전
	    .dataRowMapper(Victim.class) //5.2이후
	  이렇게

    2.JdbcCursorItemReader: 데이터 가져오기, 정말 한 행씩일까?
      기본적으로 보이기는 한행씩 db에서 값을 가져오는거처럼 보이지만,여러개의 row를 미리 가져와서 내부버퍼에 저장해두고 next마다 하나씩 밀어주는방식임
	  그러다가 다떨어지면 다시 db가서 가져오고
	  여기서 주의점은,mysql의 경우엔 
	    useCursorFetch=true
	  를 설정하지않으면 조회결과를 전부 메모리에 적재하니 반드시 저걸 설정해서 분할로딩해야함
	  
	  기본적으로 fetchSize만큼의 값을 한번에 가져옴(이거자체는 힌트로 꼭 저거랑 똑같이 가져오진않음,대충 비슷하게자져옴)
	  이건 빌더의 fetchSize() 설정으로 값을 조정할수있음(FlatFileItemReader도 BufferedReader()로 똑같이 조절가능)
	
    3.커서 연속성
      청크처리마다 트랜잭션이 새로 시작되고 커밋되지만,커서는 Step트랜잭션과 별도의 커넥션을 사용하기때문에 괜찮음
	  그래서 스탭커밋의 영향을 커서는 받지않고,처음열렸을때 상태 그대로 데이터를 끝까지 읽을수있음

    4.스냅샷읽기
      아이템리더가 조회하는 데이터를 스텝에서 변경했을때도 아이템리더는 그 변화를 볼수없음
	  즉 처음 조회했던값을 그대로 계속 읽는게 보장됨
	  그래서 변경을 신경쓰지않고 전체처리를 할수있음

    5.JdbcCursorItemReader의 SQL ORDER BY 설정
      ORDER BY는 스탭이 실패했을때를 대비해서 꼭 설정하는게 좋음
	  이래야 고정된 순서로 처리가 가능해서,스탭이 실패했을때 해당 위치로 가서 처리를 할수있고,
	  만약 Order by가 없다면 매번 순서가 달라질 가능성이 있어서 이전 실패위치라고 보장할수없음(누락 및 중복)
	  그래서 pk같은걸로 Order by 해두는게좋음

  3.JdbcPagingItemReader
	이건 데이터를 페이지 단위로 나눠서 읽어오는 아이템리더임
	이건 Keyset 기반의 페이징을 수행함
	
	페이징엔 두가지방식이 있는데
	  offset:db가 결과셋을 정렬하고 offset만큼 건너띄고 limit갯수만큼 데이터를 가져옴
	    정렬하고 가지고있어야한다는 문제때문에 db메모리를 많이사용하고,페이지번호가 뒤로갈수록 성능이 저하됨
		  SELECT * FROM victims ORDER BY id LIMIT 10 OFFSET 20
		이런식
	  keyset:이전 페이지의 마지막키를 기준으로 그 다음데이터를 가져옴
	  정확히 이전에 가져온 마지막값 이후부터 읽어오므로 성능이 일정하게 유지됨
	    SELECT * FROM victims WHERE id > 1000 ORDER BY id LIMIT 10
	  만약 uuid를 사용한다면 생성시간과 uuid를 둘다 사용하고,복합인덱스를 걸수있음
	
	1.JdbcPagingItemReader 해부
	  얘도 별차이없음
	    dataSource
		rowMapper
		NamedParameterJdbcTemplate  //sql실행+파라미터바인딩
		PagingQueryProvider //쿼리생성 및 페이징전략,db별 sql최적화
	  이렇게 구성됨
	  데이터소스와 rowmapper은 커서와 동일함
	  
	  NamedParameterJdbcTemplate 는 :abc처럼 이름있는 파라미터를 사용해서 값을 삽입할수있게해줌
	  PagingQueryProvider 는 페이징을 위한 쿼리생성도구인터페이스고, 각 dbms종류에 맞는 페이징구현체를  제공해줌(MySqlPagingQueryProvider)
	  그래서 기본적으론 쿼리설정메서드를 사용하면되고,커스텀할필요가 있다면 커스텀을 사용할수있음(커스텀을 쓰면 쿼리설정메서드값은 무시됨)
		selectClause(): 가져올 컬럼을 지정한다
		fromClause(): 데이터를 가져올 테이블을 지정한다
		whereClause() (선택): 필요한 데이터만 필터링한다
		groupClause() (선택): 데이터 집계가 필요한 경우 사용한다
		sortKeys(): ORDER BY 절에 사용될 정렬 키를 지정한다. 
		  이는 keyset 기반 페이징의 핵심으로, ORDER BY 절 뿐만 아니라 keyset 기반 페이징을 위한 WHERE 절 조건에도 사용된다. 
		  이 키는 반드시 유니크한 값이어야 한다. 
		  정렬 키가 유니크하지 않으면 동일한 값을 가진 데이터들의 순서가 보장되지 않아 일부 데이터가 누락되거나 중복될 수 있다. 
		  그래서 보통 PK나 인덱스가 있는 컬럼을 사용한다.	  
	  ketset페이징 특성상 처음호출쿼리와 그이후쿼리는 다른데,처음은 정렬조건과 limit만 필요하지만,
	  두번째부턴 이전페이지의 마지막보다큰 이라는 조건이 들어가야하기때문 
	  그래서 내부적으로 firstPageSql과 remainingPagesSql같은 필드들로 이걸 구분함(알필욘없음)
	
	2.JdbcPagingItemReader 구성하기
	이건 데이터소스와 로우매퍼는 동일하고,페이지사이즈를 설정해줘야하고 sql대신 PagingQueryProvider설정이 들어가는게 다름
		@Bean
		public JdbcPagingItemReader<Victim> terminatedVictimReader() {
			return new JdbcPagingItemReaderBuilder<Victim>()
					.name("terminatedVictimReader")
					.dataSource(dataSource)
					.pageSize(5)
					.selectClause("SELECT id, name, process_id, terminated_at, status")
					.fromClause("FROM victims")
					.whereClause("WHERE status = :status AND terminated_at <= :terminatedAt")
					.sortKeys(Map.of("id", Order.ASCENDING))
					.parameterValues(Map.of(
							"status", "TERMINATED",
							"terminatedAt", LocalDateTime.now()
					))
					.beanRowMapper(Victim.class)
					.build();
		}	
	이렇게 select절과 from절,(where절과 group절)을 넣은다음 sortKeys로 키를 지정해주면됨
	이렇게 3개만 지정해주면,현재 db에 맞는 구현체를 자동으로 생성해서 페이징쿼리를 구성해줌
	커스텀 PagingQueryProvider를 사용하려면 queryProvider()에 직접 넣어주면됨
	
	그리고 .parameterValues()로 쿼리파라미터를 채워주면됨
  3.JdbcBatchItemWriter
    JdbcBatchItemWriter는 가장 기본적인 rdbms 쓰기도구임
	이건 NamedParameterJdbcTemplate를 사용하고,jdbc템플릿의 batchUpdate를 사용해서 청크단위 아이템을 효율적으로 저장함
	즉 벌크인서트로 저장함
	이때 배치쿼리는 하나의 트랜잭션내에서 수행되고,한청크가 전부 성공하거나 전부실패함
	
	mysql과 PostgreSql은 
	  # MYSQL
	  url: jdbc:mysql://localhost:3306/mysql?rewriteBatchedStatements=true 
	  #POSTGRESQL
	  url: jdbc:postgresql://localhost:5432/postgres?reWriteBatchedInserts=true 	
	설정으로 벌크인서트를 킬수있음
	
	1.JdbcBatchItemWriter 해부
	  얘는 이렇게 구성됨
	    NamedParameterJdbcTemplate:sql실행엔진,네임드파라미터바인딩을 지원
		sql:실행할 sql구문(insert,update,delete등)을 정의함,네임드파라미터 사용가능
		ItemSqlParameterSourceProvider or ItemPreparedStatementSetter:
		  ItemSqlParameterSourceProvider는 자바객체를 sql네임드파라미터에 매핑(userName은 :userName에 매핑)
		  ItemPreparedStatementSetter 는 자바객체의 데이터를 ?에 매핑
	  즉 쿼리를 받고,청크의 모든값을 넣은 벌크처리 sql로 변환한다음 그걸 한번 던져서 성공 or 실패를 받는식임
	
	2.JdbcBatchItemWriter 빌더
	  이건
		@Bean
		public JdbcBatchItemWriter<HackedOrder> orderStatusWriter() {
			return new JdbcBatchItemWriterBuilder<HackedOrder>()
					.dataSource(dataSource)
					.sql("UPDATE orders SET status = :status WHERE id = :id")
					.beanMapped()
					.assertUpdates(true)
					.build();
		}
	  이런식으로 구성됨
	  beanMapped()는 빈기반으로 매핑하겠다는거고,
	  assertUpdates()는 기본값이 true고,하나라도 업데이트나 실패하면 바로 예외던져서 트랜잭션실패처리,
	  false면 일부데이터가 업데이트되지않아도 계속 진행함
	  중복데이터처리할때나 조건부update를 할땐 false를 걸어야함
	
  4.JPA ItemReader / ItemWriter
	implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
	를 추가하면 jpa를 사용할수있음(web에서 쓰던 그거맞음)
	
	1.JpaCursorItemReader
	  이건 커서기반 jpa 아이템리더임
	  이게 JdbcCursorItemReader와 다른점은 내부적으로 entityManager을 통해 데이터를 읽는다는게 다름
	  그래서 sql이 아닌 엔티티중심의 처리가 가능해짐
	  
	  1.JpaCursorItemReader 해부
	    얘는
		  querySting(jpql)  or JpaQueryProvider:데이터조회쿼리
		  EntityManager :핵심컴포넌트,엔티티의 생명주기 관리 및 실제 db작업을 수행함
		  query:실행가능한 쿼리인스턴스,얘를통해 데이터를 스트리밍으로 읽어옴
		    하이버네이트꺼는 좀 유도리있게 스트리밍하는데,기본구현은 전체데이터를 메모리에 한꺼번에 로딩하니까 스트리밍이 아니고,그래서 주의가 필요함
		로 구성됨
	  2.JpaCursorItemReaderBuilder
		이건
			@Bean
			@StepScope
			public JpaCursorItemReader<Post> postBlockReader(
					@Value("#{jobParameters['startDateTime']}") LocalDateTime startDateTime,
					@Value("#{jobParameters['endDateTime']}") LocalDateTime endDateTime
			) {
				return new JpaCursorItemReaderBuilder<Post>()
						.name("postBlockReader")
						.entityManagerFactory(entityManagerFactory)
						.queryString("""
								SELECT p FROM Post p JOIN FETCH p.reports r
								WHERE r.reportedAt >= :startDateTime AND r.reportedAt < :endDateTime
								""")
						.parameterValues(Map.of(
								"startDateTime", startDateTime,
								"endDateTime", endDateTime
						))
						.build();
			}	
		이런식으로 사용됨
		엔티티매니저는 클래스레벨에서 di받으면되고, queryString에 있는거를 파라미터를 넣어서 조회하고 현재 제네릭타입의 객체에 넣어서 리턴해줌
		queryString말고 다른방식도 있는데,JpaQueryProvider 를 사용하면 동적쿼리생성등 유연한처리가 가능함
		
		기본적으론 
		  JpaNamedQueryProvider:엔티티에 미리 정의된 네임드쿼리를 사용
		  JpaNativeQueryProvider:네이티브sql로 데이터 조회
		이렇게 두개의 구현체를 기본제공해주는데,필요하다면 커스텀을 만들수도있음
		보통 동적쿼리할떄나 쿼리힌트제공할때 사용했는데,쿼리힌트는 5.2부터 .hintValues()를 사용할수있음
		  .hintValues(Map.of("org.hibernate.fetchSize", 100))
		이렇게
		
	2.JpaPagingItemReader
	  이건 offset기반으로 페이징을 수행함(limit와 offset절사용)
	  
	  1.데이터정합성붕괴
	    문제는 실시간 데이터변경에 취약하다는것
		  페이지1번을 읽고 새데이터가 맨앞에 추가되면 마지막하나 중복처리
		  페이지1번을 읽고 하나 삭제하면 다음페이지거 하나 누락됨
		일반적으로 고정된 입력 데이터셋을 대상으로 하면 괜찮지만 아닐경우 문제가될수있음
		이럴땐 offset기반 페이징은 쓰지않는게좋음
	
	  2.성능문제
	    아까말했던거처럼 매 페이지 요청마다 db는 전부 메모리에 올려야하니 문제가됨
		또한 db 메모리 사용량도 많이커짐
		그래서 작은 데이터셋에선 괜찮은데,좀 크다면 안쓰는게좋음
	
	  3.JpaPagingItemReader 해부
	    이건
		  queryString
		  EntityManager
		이렇게 JpaCursorItemReader와 별차이없지만,데이터를 읽는방식에서 차이가 있음
		커서기반은 초기화시점에 쿼리를 한번 실행하고 순차조회하지만,얘는 페이지별로 계속 읽음
		그리고 pageSize로 페이지크기 설정해주면됨
		
		또한 여기서도 orderby는 필수임
		
		또한 JpaPagingItemReader에서는 페치조인을 사용할수없음(커서는 상관없음)
		하이버네이트에서 페치조인과 페이징을 같이쓰면 쿼리에서 페이징하지않고 전체데이터를 로드한후 어플리케이션에서 페이징해버림(oom)
		
		그런데 문제가 있는데,n+1임		
		배치에서는 FetchType을 Eager로 설정하고 @BatchSize로 데이터를 가져오는식으로 n+1을 완화시켜야함
		JpaPagingItemReader 트랜잭션처리로직상 Lazy에선 @BatchSize가 적용되지않아서 eager로 설정해야함
		트랜잭션 범위때문에 생기는문제임
		이건 transacted 필드가 true일때 발생하는데,추가적인 문제가 더있음
		만약 이전청크의 itemProcessor에서 엔티티를 수정했다면,읽기를 하는게 더티체킹돼서 다시 변경될수있음
		그래서 빌더에서
		  .transacted(false)
		이렇게 설정해두고 쓰는게좋음
		
		이때 주의할건 지연로딩이 불가능해지니까 eager로 미리로드해두는게좋음
		배치는 eager가 더 맞기도 함,할일이 정해져있으니까
		그리고 연관관계엔티티가 실제로 필요하지않다면 그냥 jdbcPagingItemReader가 더 나음
		
		offset기반 문제를 해결하고 트랜잭션관리도 개선하고 querydsl까지 도입한 커스텀 구현체가 있는데
			jojoldu/spring-batch-querydsl
		조졸두아저씨꺼있음 가져다쓰면됨
	3.JpaItemWriter
	  implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
	  를 하면 PlatformTransactionManager의 구현체가 
	  기본 DataSourceTransactionManager 대신 JpaTransactionManager가 자동으로 사용됨
	  jpa기능(영속성컨텍스트,1차캐시등)을 사용하기위해서 바꾸는거
	  
	  JpaItemWriter는 넘겨받은 엔티티를 영속성컨텍스트에 넣고 db에 넣는일만 함
	  구성도
		@Bean
		public JpaItemWriter<BlockedPost> postBlockWriter() {
			return new JpaItemWriterBuilder<BlockedPost>()
					.entityManagerFactory(entityManagerFactory)
					.usePersist(true) //persist와 merge선택
					.build();
		}
	  이렇게 간단함
	  entityManagerFactory를 주입하고 persist를 사용할지 merge를 사용할지만 결정해주면됨,이건 어떤식으로 엔티티를 저장할지임
	  persist는 새 데이터를 추가할때,merge는 기존데이터를 수정할때 사용하면됨
	  
	  이것도 로그상에는 하나씩 처리되는거처럼 보일수있는데,배치업데이트(벌크인서트)로 처리됨(reWriteBatchedInserts=true)
	  
	  하이버네이트는 시퀀스를 기본값 50개정도씩 할당받아서 사용함(id생성할떄 GenerationType.SEQUENCE)
	  이때 db도 숫자를 맞춰줘야함
	    ALTER SEQUENCE blocked_posts_id_seq INCREMENT BY 50;
	  그리고 다른어플리케이션에서 이테이블을 사용중이라면,allocationSize 을 동일하게 맞춰줘야함
	  
	  이게 persist()방식임
	
	4. IDENTITY 전략 사용 시 배치 처리 제약사항
	  GenerationType.IDENTITY는 엔티티id가 db에서 생성되기때문에,영속화를 하려면 반드시 insert가 먼저 실행되어야함
	  그래서 모든 인서트가 개별실행되어야한다는걸 의미하는데,그래서 배치처리가 불가능해짐
	  그래서 성능이 중요하다면 IDENTITY대신 SEQUENCE를 사용하는게 좋음
	  
	5.merge()	
	  이건 기존데이터를 수정할때 사용되는데,주의점이 있음
	  merge()호출시 영속성컨텍스트는 해당 데이터의 존재여부를 알수없어 db조회를 강제하게되는데,그래서 청크단위처리마다 셀렉트쿼리가 청크크기만큼 추가로 발생함
	  
	  그거말곤 그냥 해당엔티티를 뱉는다고 생각하고 쓰면됨
	  
*2.nosql읽고쓰기
  몽고디비는
    도큐먼트:기본데이터단위,bson으로 저장
	컬렉션:도큐먼트의 그룹,테이블이라고 보면됨
	데이터베이스:컬렉션을 그룹화한 최상위컨테이너,rdb의 데이터베이스와 같음
  로 구성된 기본적인 nosql임
  얘는
    스키마정의가 없어서 도큐먼트구조를 자유롭게 변경가능
	수평확장이쉽고 샤딩을 지원
	대량데이터처리를 쉽게할수있음
  이런 특징이 있음
  얘를 쓸땐
    implementation 'org.springframework.boot:spring-boot-starter-data-mongodb'
  를 넣어줘야함
  
  1.MongoCursorItemReader
    얘도 커서기반의 데이터순차접근방식임
	얘는
	  MongoTemplate:핵심엔진,이걸사용해 데이터를 스트리밍조회함
	  Query :조회할 쿼리를 지정,다른커서리더처럼 얘도 초기화시점에 쿼리를 한번 생성하고,이를기반으로 커서를 가져옴
	  Cursor:read()가 호출될때마다 커서를 사용해 데이터를 하나씩 순차반환
	로 구성되어있음  
	
	얘는
		@Bean
		@StepScope
		public MongoCursorItemReader<SecurityLog> securityLogReader(
				@Value("#{jobParameters['searchDate']}") LocalDate searchDate
		) {
			Date startOfDay = Date.from(searchDate.atStartOfDay(ZoneId.systemDefault()).toInstant());
			Date endOfDay = Date.from(searchDate.plusDays(1).atStartOfDay(ZoneId.systemDefault()).toInstant());

			return new MongoCursorItemReaderBuilder<SecurityLog>()
					.name("securityLogReader")
					.template(mongoTemplate) //DI받은 몽고템플릿
					.collection("security_logs")//읽어올 컬렉션명
					.jsonQuery("""   //쿼리
					{
						"label": "PENDING_ANALYSIS",
						"timestamp": {
							"$gte": ?0,
							"$lt": ?1
						}
					}
					""")
					.parameterValues(List.of(startOfDay, endOfDay)) //파라미터
					.sorts(Map.of("timestamp", Sort.Direction.ASC)) //정렬
					.targetType(SecurityLog.class)
					.batchSize(10)  //변환타입
					.build();
		}
	이런식으로 구성됨
	이거말고도 
	  쿼리의 projection을 위한 fields()
	  쿼리 최적화를 위한 인덱스 힌트를 지정하는 hint()
	  조회할 도큐먼트 수를 제한하는 limit()
	  쿼리 수행 시간을 제한하는 maxTime()
	등등 다양한 메서드들이 있음
	
	1.MongoCursorItemReader의 쿼리 구성, 또 다른 방법
	  Query객체를 직접 사용할수도 있음
	  이러면
	    타입세이프
		ide자동완성가능
		체이닝으로 명확하게 구성가능
	  이런 이점이 있음
	  이건
		  Query query = new Query()
			.addCriteria(Criteria.where("label").is("PENDING_ANALYSIS"))
			.addCriteria(Criteria.where("timestamp")
					.gte(Date.from(searchDate.atStartOfDay(ZoneId.systemDefault()).toInstant()))
					.lt(Date.from(searchDate.plusDays(1).atStartOfDay(ZoneId.systemDefault()).toInstant())))
			.with(Sort.by(Sort.Direction.ASC, "timestamp"))
			.cursorBatchSize(10);
	  이렇게하면됨
	  이때 배치버그가 있는데,query객체에서 정렬조건이 있더라도 아이템리더빌더에서 sorts를 반드시 넣어줘야함
	  이걸 사용하진않는데 없으면 예외띄움

  2.MongoPagingItemReader
    이건 페이징방식 아이템리더임
	이건 skip와 limit를 사용해 페이징을 수행하고,이것도 offset기반 페이징임
	그래서 성능이 당연히 안좋고,특히 몽고db의 샤딩환경에선 처참함
	
	또한 오프셋기반 페이징이라서 프로세서에서 쿼리조건에 해당하는 값을 수정하는경우 계속 결과값이 바뀌어서 문제가됨	
	그래서 이건 실무에서 사용하지않는게 좋고,정 페이징이 필요하다면 아이템리더를 직접 구현해 keyset기반페이징을 해야함
	
	꼭 써야한다면
	  별도필드활용//쿼리조건필드는 절대 건드리지말고 임시필드에만 결과 저장후,배치완료후 일괄업데이트
	  id기반처리
	  스냅샷//임시컬렉션에 처리할거 복사한후에 원본대신 임시컬렉션에서 읽기
	등 우회할수있긴한데,가급적 커서쓰는게나음

  3.MongoItemWriter
    얘는 MongoTemplate을 사용해 청크의 아이템을 추가,수정,삭제함
	얘는 batchUpdate와 비슷한 bulkWrite를 사용함
	이때 각 연산은 순차실행되고,하나가 실패하면 후속작업은 실행되지않음
	
	얘는
	  MongoTemplate //핵심엔진
	  collection  //테이블비스무리
	  mode  //인서트,업서트(기존도큐먼트수정,없으면추가,기본값),리무브를 설정해야함
	로 구성됨
	얘는 뭘할질 미리 정해줘야함
	얘는
		@Bean
		public MongoItemWriter<SecurityLog> securityLogWriter() {
		   return new MongoItemWriterBuilder<SecurityLog>()
				   .template(mongoTemplate)
				   .collection("security_logs")
				   .mode(MongoItemWriter.Mode.UPSERT)  // 기존 문서 수정
				   .build();
		}
	이렇게 간단하게 만들수있음
	이때 업서트는 기본동작은 id를 사용해서 검색하고,없으면 새로생성함
	만약 로직을 추가하려면 5.2.3부터는 primaryKeys()를 사용해서 복합키기반 도큐먼트선택이 가능함
		public MongoItemWriterBuilder<T> primaryKeys(List<String> primaryKeys) {
			this.primaryKeys = List.copyOf(primaryKeys);
			return this;
		}
	단 이때 지정한 필드들의 조합이 반드시 단일도큐먼트를 유니크하게 식별할수있어야함

  4.MongoTransactionManager
    몽고아이템라이터의 벌크인서트에서 하나의 작업이 실패하면 후속작업이 실행되지않는다는건,바꿔말하면 이미 성공한데이터들은 남아있게됨
	즉 트랜잭션이 되지않는다는거
	
	만약 이때 이미 성공한애들을 롤백하려면 MongoTransactionManager를 스텝의 트랜잭션매니저로 별도구성해줘야함
	단 이때 주의할점은
	  MongoDB의 버전이 4.0이상이어야하고,
	  샤드클러스터나 레플리카셋환경에서만 지원함,즉 단일몽고에서 지원하징낳음
	  이건 자동빈등록안되니까 수동구성해야함
	이런 문제가 있음
	
	만약 단일환경일경우엔 파일라이터에서 쓰던 버퍼링을 사용해서 처리할순있음
	그렇지만 이건 위험도를 감소시키는거지 이미 쓰여진 데이터를 원자적으로 롤백해주진못함
	또한 MongoTransactionManager를 빈으로 정의할땐 배치메타데이터 트랜잭션매니저와 분리를 해야해서 별도구성이 필요함

  5.RedisItemReader
    5.1부터 레디스전용 아이템리더와 라이터를 지원함
	RedisItemReader는 scan을 사용하는 일종의 커서기반 아이템리더임
	
	그런데 스캔특성상 다른 커서랑 차이가 있음
	scan은 조회대상키들의 목록만을 반환하고,실제값을 얻을땐 get명령이 필요함
	이게 다른점임
	또한 scan은 스트링타입전용명령이고,다른타입은 HSCAN, SSCAN, ZSCAN등을 사용한 커스텀아이템리더를 직접 구현해야함
	
	얘의 동작은
	  초기화시점(open())에 scan을 최초호출해 커서를 생성함,커서엔 조회대상키목록이 담겨있고,그것도 전체가 아닌 일부키만 담겨있음
	  read로 커서로부터 키를 하나씩 받아와서 get함
	  커서의 키가 다 소모되면 추가스캔으로 새 키목록을 확보후 반복
	이런식으로 동작함
	이래서 문제가 있는데
	  아이템 하나당 get 한번을 하기때문에 청크크기만큼 get이 날아감
	    인메모리 db특성상 괜찮긴한데 네트워크왕복시간때문에 조회시간이 길어질수있고,레디스에도 부담
	  레디스 특성상 scan이 중복된 키를 반환할수있고,이거처리는 어플리케이션에서 해야함
	    즉 사소한 중복이 문제가 되지않거나(통계),별도의 중복처리방법이 없다면 반드시 멱등한 배치에서만 사용해야함
	  재시작불가
	    스캔은 순서를 보장하지않아서 재시작이 불가능함(order by 없으니까)
	이런문제들이 있음
	
	얘의 구성은
	  RedisTemplate  //핵심엔진
	  Cursor //스캔의 결과물,키가 다떨어지면 내부적으로 추가스캔함
	  ScanOptions //스캔할때 필요한 설정정보가 있음,key패턴이나 한번에 몇개가져올지등
	이렇게 구성되고,실제 빌더는
		RedisItemReader redisItemReader = new RedisItemReaderBuilder<String, AttackLog>()
			.redisTemplate(template)
			.scanOptions(ScanOptions.scanOptions()
				.match("attack:*")  //키패턴
				.count(10)  //가져올갯수
				.build())
			.build();	
	이렇게 하면됨
	단 여기서의 카운트는 힌트라서 엄격하지않음
	
	그리고 레디스템플릿을 기본제공해주는걸 써도되지만,json문자열형태를 객체로 변환할필요가 있다면 직접 생성해야함
	Jackson2JsonRedisSerializer를 별도로 구성해줘야하기때문
	그리고 match()는 저 패턴을 만족하는 모든 키를 대상으로 삼겠다는거
	추가적으로 레디스아이템빌더는 재시작이 불가능하기때문에 이름이없음
		
	추가적으로 배치에서 청크지향처리를 할때,전체데이터에 대한 집계나 요약정보가 필요할땐 별도의 집계컴포넌트가 필요함
	이떄 JobExecutionListener 를 상속받은 클래스를 만들어서 모으는식으로 집계나 요약정보를 생성할수있음
	이걸 잡빌더에서
		.listener(attackCounter)
	로 등록하면됨
	
	그리고 RedisItemReader는 value만 반환하는데,만약 키의 값이 필요할경우가 있음
	현재기준으로는 이걸 사용할수없지만 미래에 수정될예정같음

  6.RedisItemWriter
    이건 청크의 각 아이템을 하나하나 추가하거나 삭제함
	별도의 최적화없이 단순히 한건씩 처리함
	또한 얘도 String타입의 데이터만 쓰기를 지원함
	얘의 빌더는
		@Bean
		public RedisItemWriter<String, AttackLog> deleteAttackLogWriter() {
			return new RedisItemWriterBuilder<String, AttackLog>()
					.redisTemplate(redisTemplate)
					.itemKeyMapper(attackLog -> "attack:" + attackLog.getId())
					.delete(true)
					.build();
		}
	이렇게 됨
	여기서 .delete(true)로 두면 삭제모드로 동작하고(키를 추출해 해당키를 삭제),
	false로 두면 키를 추출해서 키로쓰고,AttackLog객체를 value로 삽입함

  7.RepositoryItemReader
    이건 레포지토리추상화를 가지고 동작하는 아이템리더임
	근데 문제는 얘는 페이징방식으로 오프셋으로 동작함
	그래서 데이터가 많아지면 성능이슈발생가능
	얘는
		public RepositoryItemReader<HackNote> reader() {
		return new RepositoryItemReaderBuilder<HackNote>()
				.name("hackNoteItemReader")
				.repository(hackNoteRepository)
				.methodName("searchNotesByMessageAndSentimentIsNull")
				.arguments(List.of("PWN"))
				.pageSize(10)
				.sorts(Map.of("timestamp", Sort.Direction.DESC))
				.build();
		}
	이렇게 레포지토리를 넣고 검색메서드를 넣어주고 파라미터넣고 페이징사이즈 넣고 정렬넣고 쓰면됨

  8.RepositoryItemWriter
    이건 라이터라서 기본적으로는 saveAll을 사용해서 벌크연산을 사용하고,
	methodName()을 사용해 다른메서드를 사용할수있긴한데 이런경우엔 아이템마다 개별호출되니까 주의
	가급적 기본값쓰는게나음
	
	빌더는
		@Bean
		public RepositoryItemWriter<HackNote> logWriter() {
			return new RepositoryItemWriterBuilder<HackNote>()
					.repository(hackNoteRepository)
					.build();
		}
	진짜 레포만 넣으면되고,필요하다면 중간에  .methodName("save") 같은걸 넣어서 메서드지정하면됨
	또한 이렇게 메서드가 진짜 있는지없는지와 파라미터(즉 시그니처)를 RepositoryMethodReference를 사용해서 검증할수있음
	이때 프록시로 검증하기때문에 레포지토리가 final이 아니어야함
	
*3.위임 ItemWriter와 ItemReader	
  CompositeItemReader/CompositeItemWriter를 사용하면 여러 아이템리더를 순차실행할수있고,
  ClassifierCompositeItemWriter를 사용하면 서로 다른 아이템라이터를 선택적용할수있음
  
  1.CompositeItemReader
    이건 여러 ItemReader를 순차적으로 실행하는 리더임
	이건 5.2에서 추가됐고,전용빌더는 없고 직접 생성해서 써야함
		List<ItemStreamReader<Customer>> readers = List.of(
			shard1ItemReader,  
			shard2ItemREader   
		);

		CompositeItemReader<Customer> compositeReader = new CompositeItemReader<>(readers);	
	이렇게 같은타입을 반환하는 아이템리더 리스트를 넘겨주기만하면됨
	
	얘는 이전 아이템리더가 읽을게 없다고 null을 반환하면 자동으로 다음 아이템리더로 넘어가는형식임
	
	이건 db가 여러샤드로 분산되어있을떄 유용함
	별도 샤드관리없이 각 샤드별로 아이템리더만들고 연결하면됨

  2.CompositeItemWriter
    이건 여러 아이템라이터에 같은 데이터를 전달하는거
	즉 여러종류의 db,이메일,알림등을 같이 보내야할때 사용됨
		CompositeItemWriter<Hacker> writer = new CompositeItemWriter<>(
		   List.of(
			   firstWriter,
			   secondWriter,
			   thirdWriter
		   )
		);
	이렇게 하던가,전용빌더를 써도되긴함
		CompositeItemWriter<Hacker> writer = new CompositeItemWriterBuilder<Hacker>()
		   .delegates(List.of(
			   firstWriter,
			   secondWriter,
			   thirdWriter
		   ))
		   .build();	
	이건 여러곳에 값을 보내야하거나,한곳에는 update,한곳에는 insert를 하는등의 작업을 해야할떄도 사용할수있음(백업이나 동기화등)
	
	단 얘는 동작특성상 트랜잭션문제가 생길수있음(전체실패나 전체성공이 되지않을수있음)
	그래서 일단 트랜잭션외부작업은 beforeCommit에서 버퍼링으로 실행하는식으로 처리하긴함

  3.ClassifierCompositeItemWriter
    이건 여러 라이터중에 하나를 선택해서 사용하는거
	write메서드에 청크가 전달되면,각 아이템은 classifier의 규칙에 따라 각 데이터를 위임라이터를 선택해서 걔한테 처리를 맡김
	얘도 당연히 이런구조니까 같은타입의 데이터를 처리해야함
	
	당연히 데이터위임규칙클래스는 직접 작성해야하고 Classifier클래스를 상속해서 classify를 구현해서 사용할수있음
	또한 아이템라이터는 같은 타입의 데이터를 처리해야하지만,인터페이스를 타겟으로 잡고 밑을 세분화시키면 실제 인스턴스타입에 따라 알맞은 대상에게 처리시킬수도있음
	단 이경우엔 SubclassClassifier라는 이미 있는구현체 쓸수도있음
	
	그리고 라이터에선 그냥 저거 등록만 하면됨
		@Bean
		public ClassifierCompositeItemWriter<SystemLog> classifierWriter() {
			ClassifierCompositeItemWriter<SystemLog> writer = new ClassifierCompositeItemWriter<>();
			writer.setClassifier(new SystemLogClassifier(criticalLogWriter(), normalLogWriter()));
			return writer;
		}	
	이렇게
	
	ClassifierCompositeItemReader는 없는데,PatternMatchingCompositeLineMapper처럼 이런식으로 커스텀으로 만들수있음
	
	
5.배치 스탭
*1.ItemStream	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
  