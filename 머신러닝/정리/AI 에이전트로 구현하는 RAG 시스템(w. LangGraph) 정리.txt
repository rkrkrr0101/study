1.들어가며
  1.강의개요
    요즘쓰는 에이전트는 보통 리액트패턴임,라우터구조도 쓸수있는데 좀 섞어서쓰는듯 커서생각해봐도
    워크플로우를 설계하고,그래프를 사용해 노드와 노드를 연결하는 엣지로 구현하는 방식
    
    랭체인은 좀 복잡한 로직(에이전트같은)을 만들기는 어렵고,llm통합만을 위해,즉 api통신을 편하게 하기위해 사용됨
    랭그래프는 좀 복잡한 로직(멀티에이전트 등)을 상태기반 그래프를 통해 구현하기위해 만들어짐

2.랭체인 tool calling/agent 활용방법
  1.툴콜링 개념 이해
    툴콜링은 llm이 외부기능이나 데이터에 접근할수있게 해주는 메커니즘임
	llm이 특정 기능이 필요할때 미리 지정된 함수형태(aa(int,int))로 응답을 반환하면,
	이 응답에 해당하는 함수를 실행해서 결과값을 돌려주고 llm은 그걸 사용해서 최종응답을 만드는 기법임
	
	기본적으로 툴콜링을 사용할수있는 llm을 에이전트라고 부름

  2.랭체인 (LangChain) 내장 도구 생성하기
    랭체인은 기본적인 도구들을 가지고있음(랭체인 커뮤니티 툴)
	검색,코드인터프리터,생산성도구등이 있음
	
	무료검색에는 덕덕고가 있고,유료로는 TavilySearch가 있음
	
	툴은 4가지 요소를 가짐
	  name:툴의 이름
	  description:도구가 수행하는 작업에 대한 설명
	  json schema:도구의 입력을 정의하는 스키마(매개변수에 대한 설명,이게뭔지와 타입정도?)
	  function:실행할 함수(일반적으로는 동기,비동기도 가능하긴함)
	이렇게
	이름과 설명이 매우 중요함,llm이 고를지말지를 이거보고 정하기때문,스키마도 제대로쓰려면 잘써줘야함,특히 필수필드같은거도 표시해줘야함

  3.랭체인 (LangChain) 내장 도구 호출하기
    툴콜링을 할땐 llm에
	  llm_with_tools=llm.bind_tools(tools=[넣을,툴,배열])
	이렇게 넣고 그냥 invoke하면됨
	그러면 쿼리와 툴의 목록을 보고,툴을 쓰는게 더 편하겠다 싶은건 llm이 알아서 툴을 사용하겠다고 선언함(툴을 호출함)
	그러면 이걸보고 툴을 실행시켜주면됨(이땐 컨텐츠에는 값이 비어있고,tool_calls배열에만 값이 들어있음)
	
	이런 툴콜링을 쓸거면 gpt-4o정도의 성능이 괜찮은 모델을 써야 제대로함

  4.랭체인 (LangChain) 내장 도구 실행하기
    llm이 함수를 호출해주면 이걸 실행하는 방법이 크게 3가지정도가 있음
	기본적으로 반복문으로 ai메시지.tool_calls배열에서 하나씩 뽑아서,해당 도구로 invoke를 하는것(도구를 직접 실행)은 공통이고
	
	첫번째는 툴 메시지에서 args만 뽑아서 던지는방식
	  tool_output=web_search.invoke(tool_call["args"])
	두번째는 툴콜 메시지 전체를 직접 던지는 방식(위랑 거의같은데 args를 뽑지말고 통째로 던지는거)
	  tool_message=web_search(tool_call)
	세번째는 사용자정의한 ToolMessage를 만드는방식
	  tool_message=ToolMessage(content=tool_output,tool_call_id=tool_call["id"],name=tool_call["name"])
	세번째는 좀 정교하게 컨트롤할때 쓰고,보통은 두번째처럼 던지게됨
	
	또한 실행해야할게 여러개라면(tool_calls에 값이 여러개라면),batch로 실행하면 여러개를 동시에 실행할수있음
	  tool_messages=web_search.batch(aimsg.tool_calls)
	이렇게

  5.Tool Calling (도구 호출) 결과로 LLM 답변 생성하기
    실제로 우리가 원하는건 Tool Calling이지 도구를 뭘 실행할지 정하는게아님(근거를 검색하고 응답을 만들어내는게 목표)
	즉 도구의 실행요청을 가져와서,실행한뒤,다시 체인에 넣어서 llm을 돌려야함
	
	이떈 @Chain을 사용하는게 좋음
	대충
	  사용자 요청을 llm에 전달후 1차 응답메시지를 받음
	  1차응답메시지의 툴을 배치실행(툴실행이 있든없든 던지고,없으면 빈값리턴)
	  최종응답을 생성하기위해 사용자요청과 툴결과메시지를 messages에 담아서 llm에 다시 전달후 최종응답을 받음
	이런 루틴임
	즉 사용자요청과 무슨툴을 실행시켰는지,툴의 실행결과가 뭔지까지 전부 넣어서 최종응답을 생성하는것
	  
  6.랭체인 (LangChain) 사용자 정의 도구 활용 - Custom Tool
    도구를 직접 정의해서 쓰려면,@Tool데코레이터를 사용하면됨
	이떄 주의해야할건,명확한 입출력을 정의해야하고,단일책임원칙을 준수해야함,또한 도구설명을 도구를 명확히 이해하고 사용할수있게 작성해야함
	
	만들어진 함수에 @Tool만 붙이면 일단 되는데,이떄
	  name:함수명
	  description:docstring,함수의 최상단에 있는 스트링이 디스크립션이 됨
	  매개변수:매개변수명과 타입
	이런식으로 지정이됨

  7.LLM 모델의 Tool Calling 성능 비교 (OpenAI, Gemini, Llama 70B)
    예전모델이나 작은모델은 툴콜링을 못함(툴인식을 못함),또한 한국어를 못하는경우가 많음

  8.Runnable 객체를 도구로 변환하기
    문자열이나 dict 입력을 받는 Runnable객체를 tool로 변환시킬수있음
	이건 as_tool 메서드를 사용하면됨
	
	즉 일반함수를 RunnableLambda로 싸고,이걸 as_tool로 싸면 툴로 변환시킬수있음
	이떄 이름과 설명을 적고,입력스키마(args_schema)도 넣어주는게 좋음,필수는 아닌데 필드의 설명이 있는게 응답이 더 좋아짐

  9.LCEL 체인을 도구로 변환하기
    체인 자체도 툴로 변환할수있음,어짜피 Runnable 인터페이스니까 당연히 되는거지만
	이걸 사용하면 특정 llm요청 자체를 툴로 사용할수있어짐(요약같은)
	llm을 사용한 전처리나 후처리 자체를 툴로써 접근할수있어진다는거

  10.벡터 저장소 (Vector Store)를 도구로 변환하기
    벡터스토어 자체도 툴로 바꿀수있음
	벡터스토어에 데이터를 저장하는거 자체야 그냥 임베딩박아서 박아넣으면되니까 쉬움(임베딩모델만 연결해주면 알아서넣음)
	이떄 청크 스플릿을 칠때 그냥 딱 끊지말고 단어단위로 끊는게 좀 더 좋긴함
	
	툴로 만들땐 vectorStore.as_retriever(...)로 만들어서 Runnable로 만들어도 되고,
	그냥 @tool을 써서 함수를 만들고 그안에서 검색하고 리턴해도됨(이게 더 편함)

  11.여러 개의 도구를 사용하여 호출하기
    준 도구가 많아도 별로 다를건 없음
	그냥 사용할 툴 목록을 받아서 반복문을 돌려서 케이스문이나 if문으로 분기를 타거나 해서 메시지를 모아서 다시 던지면됨
	
  12.Few-shot 프롬프트를 활용하여 ToolCalling 성능 개선하기
    도구를 사용하는 예제들을 프롬프트에 담아두면,llm이 좀 더 잘 사용하고,내가 원하는 형태로 가이드해줄수있음
	
	이때 각 도구의 용도를 구분해서 예제로 제시하는게 좋고,랭체인에서는 메시지를 구분해서 보내주는게 좋음
	
	랭체인의 한계는,체인이 끝나고나서 결과가 제대로 나오지않았을때,이걸 평가해서 다시 돌린다던가 하는게 좀 만들기 귀찮음
	그래서 사용되는게 랭그래프임

  13.랭체인 (LangChain) 에이전트(Agent) 활용하기
    에이전트란 llm을 추론엔진으로 사용하여,어떤행동을 할지,그행동의 입력은 무엇일지를 결정하는 시스템
    이때 에이전트는 텍스트를 출력하는걸 넘어,실제 행동을 취하게 만듬
	또한 행동의 결과를 다시 피드백해서 추가행동할지,작업을 완료할지를 결정함
	
	랭체인에선 create_tool_calling_agent라는 에이전트를 생성하는 함수와,이 에이전트를 실행할때 사용하는 AgentExecutor함수가 있음
	또한 verbose=true로 하면,중간과정도 볼수있음

  14.그라디오 (Gradio) 챗봇 인터페이스 적용하기
    에이전트를 챗봇인터페이스에 붙이려면,그라디오를 사용할수있음
	이건 붙이기 매우쉬움
	  gradio.ChatInterface(함수,..)
	를 사용하면됨
	함수는 챗봇의 입력과 출력을 처리해주는 함수임
	대략
	  def echo(message,history):
	    return message
	이런 형태를 띄고,메시지는 입력,히스토리는 이때까지의 메시지흐름임(이떄 히스토리는 사람입력과 ai입력을 구분하는게 좋음,list[tuple[str,str]]로)
	즉 저렇게 리턴 메시지를 에코하는게 아닌,저기서 llm을 사용해서 메시지를 만들어내면됨
	
	또한 히스토리를 어디까지 llm에 줄건지도 선택해서 던지면됨
	
	
3.랭그래프 기본기 다지기
  1.랭그래프 개요	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
  