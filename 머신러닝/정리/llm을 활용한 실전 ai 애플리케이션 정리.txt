1.llm지도
  스킵
2.트랜스포머
  1.트랜스포머 기본 구조(적당히 아는거제외하고 요약함)
	트랜스포머의 핵심은 어텐션이고,트랜스포머의 주요 구조는 인코더와 디코더

	당연히 텍스트는 임베딩되어야하고,임베딩하는층도 모델에 포함되어있고,이것도 같이 학습됨
	또한 어텐션이니까 문장 전체에서 가중치를 가져오는건 맞지만,그래도 근처에있으면 영향을 줄 확률이 높으니 위치정보를 더해주는 층이 있음

	쿼리는 입력하는 검색어
	키는 쿼리와 관련성을 찾기위해 문서가 가진 특징
	값은 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬해서 문서를 제공할떄 문서

	즉 우리가 검색하면서 원하는건 값임
	어텐션이 동작할때,특정 단어가 쿼리로 주어졌을떄,해당 문장내에서의 관계도(키-값)에서 키는 해당 단어고,값은 임베딩값임
	이 임베딩값을 각각 위치에따라 0.x를 몇을 곱해주냐(관련도)가 어텐션의 핵심임

	그런데 동음이의어나,나는,다녀왔다 이런 단어들은 토큰 자체로만 보면 관련성을 찾기 힘듬
	그래서 트랜스포머에서는 토큰임베딩을 변환하는 가중치Wq와 Wk,Wv를 도입함
	각각 Wq는해당 쿼리단어에 대해 학습하는거고,
	Wk는 키에대해 학습하는거임
	Wv는 값을 출력할때 토큰임베딩을 텍스트로 변환할떄의 가중치임

	또한 이 어텐션을 하나만 쓰는게 아닌,여러 어텐션을 동시에 사용하고,그걸 합산한게 효과가 더 좋음(파라미터가 늘어나는효과)
  2.인코더와 디코더
    인코더는 자연어 이해를 할떄 사용하고,디코더는 자연어 생성을 할때 사용함
	두개를 같이 쓰는건 더 넓은 범위의 작업을 할때 사용함
	
	gpt같은건 디코더만 사용한모델임
	자연어를 이해하는거만 필요하다면 인코더만 사용하는 BERT같은걸 사용할수있음(리뷰이해한후 ox표시같은거)

  3.주요 사전학습 메커니즘
    인과적 언어 모델링은 문장의 시작부터 끝까지 순차예측하는 방식임,보통 gpt같은데서 사용하는 학습방법
	마스크 언어 모델링은 단어 하나를 감추고 해당 단어를 예측하는 방식임,BERT같은 인코더쪽에서 주로 사용(양방향예측이 필요할때)
	
3.허깅페이스 트랜스포머 라이브러리	
  1.허깅페이스 라이브러리
	허깅페이스 트랜스포머 라이브러리는,다양한 트랜스포머 모델을 같은 인터페이스로 사용할수있게 지원하는 오픈소스 라이브러리임
	이건 크게 트랜스포머모델과 토크나이저를 활용할떄 쓰는 transformers라이브러리와,데이터셋을 가져다쓸수있는 datasets라이브러리로 구성됨

	쓰는것도 그냥 프리트레인모델가중치를 불러오고,토크나이저를 불러오고,입력을 토큰화한후 모델에 넣으면됨(책참고)
	거의 모든 모델을 거의 같은 코드로 사용할수있음
	도커쓰는거랑 거의 비슷한느낌임
  2.허깅페이스 허브 탐색
    이건 그냥 허브사이트 들어가서 모델보는법
	모델과 데이터셋 허브가 따로있음

  3.라이브러리 사용법 익히기
    모델은 바디와 헤드로 구성됨
	바디는 은닉층이고,헤드는 최종출력층임
	즉 기본적으로 제공되지않는 헤드를 가져가 끼우려면,추가적으로 파인튜닝을 시켜야함
	또한 각 모델에는 config.json이 주어지는데,이걸 열어보면 어떤모델인지,뭘하는모델인지를 알수있음
	
	토크나이저는 텍스트를 토큰단위로 나누고,각토큰을 대응하는 토큰아이디로 변환함
	이것도 학습하면서 어휘사전을 구축하기때문에,보통 모델과 함께 저장함(그래서 같은 모델과 토크나이저를 쓰려면 같은 모델아이디로 불러오면됨)
	
	데이터셋을 사용하면 데이터셋을 가져올수있음

  4.모델 학습시키기
    허깅페이스에는 모델학습을 쉽게 할수있게 트레이너api를 제공함
	그냥 
	  데이터를 준비하고(학습,평가)
	  토크나이저를 불러오고
	  모델을 불러오고
	  학습파라미터를 넣어주고
	  트레이너를 돌리면 끝임
	즉 예전에 텐서플로로 하던방식에서,레이어를 까는작업을 그냥 불러오는거로 대체하고,학습파라미터랑 데이터셋,토크나이저를 알아서 붙여넣고 돌리면 되는식으로 바꾼것
  
  5.모델 추론하기
	이것도 그냥 파이프라인이라고 쉽게 추론할수있게 해줌
	이건
	  작업종류
	  모델
	  설정
	을 받아서 모델파이프라인을 만들고,거기에 데이터를 넣으면 값을 리턴해줌
	
	
4.말잘듣는 모델 만들기
  1.사전학습과 지도미세조정
    사전학습은 모델의 기본 체급을 만드는 과정임
	인터넷상의 다양한 텍스트데이터로 사전학습을 보통 하는데,보통 다음에 올 단어의 확률을 예측하는식으로 학습함
	
	또한 사용자의 요청에 적절히 응답하려면,요청의 형식을 적절히 해석하고,응답의 형태를 적절히 작성하고,둘이 잘 연결되어야함
	이렇게 만드는걸 지도미세조정이라고 함
	이름에 지도가 들어간거부터 알겠지만,학습데이터에 정답이 포함된 데이터로 학습하는거임
	그래서 이걸통해 llm은 사용자의 요청에 맞춰 응답하도록 학습하는데,이를 정렬이라고 함
	이때 사용되는 데이터셋은 지시데이터셋이라고 부름(지시에 맞춰 응답한 데이터셋)
	
	이런 지시데이터셋에 비해 사전데이터셋은 형식이 매우 다양하고,요청에 응답하는 형식의 데이터는 적고,특히 양질의 데이터는 훨씬 적음
	그래서 이걸 보완하기위해 지시데이터셋을 사용하는것
	
	지시데이터셋은 사용자의 요구사항을 표현한 문장임
	입력에는 답변을 하는데 필요한 데이터가 들어가고,
	출력에는 지시사항과 입력을 바탕으로한 정답응답이 들어감
	
	즉 지시데이터셋이나 사전데이터셋이나 학습과정은 똑같은데(둘다 다음단어를 예측하는 인과적 언어 모델링 사용),
	단순히 학습하는 데이터셋에 차이가 있어서 그런것
	
	지시데이터셋의 크기는 대충 1000개정도면 충분함
	크기보단 지시사항이 다양한 형태로 되어있고,응답데이터의 품질이 높은게 더 중요함
	결국 모델의 지식이나 능력은 사전학습에서 대부분 끝났고,이걸 잘 끌어내는게 지도미세조정이기때문(즉 기초모델을 잘고르면 작은 지시데이터셋써도됨)
	또한 지시데이터셋의 품질이 좋으면 성능이 많이 올라갈수있음
	즉 이미 공개된 데이터셋을 한번 더 필터링하는게 더 나을수있다는것

  2.채점모델
    a와 b중 더 좋은 데이터를 표시한 데이터셋을 선호데이터셋이라고 부름
	각 데이터에 점수를 매긴거랑 비슷한데,이런 데이터는 찾기 힘들어서 단순히 양쪽을 비교만 하는식으로 만들어진 데이터셋
	
	선호데이터셋을 통해 답변의 점수를 매기는 리워드모델을 만들고,llm의 응답을 여러개 만들게해서 리워드모델로 피드백을 먹이는식으로 강화학습을 할수있음
	이게 사람의 피드백을 활용한 강화학습(RLHF)임
	
	단 이런방식은 단순히 점수를 높게받는데만 집중하는 보상해킹이 생기기쉬워서,이게 안생기게 면밀히 만들어야함
	그때 사용하는게 ppo로,현재 응답에서 조금만 수정해서 점수를 높일수있게 하는방식
	
	근데 rlhf는 너무너무 사용하기 어려워서,보통 기피됨
	리워드모델이 성능이 좋지않아서 일관성없는 점수를 뱉으면,학습이 제대로되지않고,
	참고모델,학습모델,리워드모델 3개의 모델이 필요해지고,
	강화학습자체가 하이퍼파라미터에 민감하고 학습이 불안정해서 쓰기가 힘듬
	
  3.강화학습이 꼭 필요할까?
    그래서 사용되는게 기각샘플링과 직접선호최적화임
	
	기각샘플링은 지도미세조정을 마친 llm을 통해 여러 응답을 생성하고,그중 리워드모델이 가장 높은 점수를 준 응답을 모아서 다시 지도미세조정을 하는것
	즉 자기지도학습을 하는거임
	이러면 강화학습을 쓰지않아서 안정적이게됨
	즉 리워드모델과 ppo는 그대로 사용하고,강화학습으로 점수매기는방식이 아닌,
	학습할 llm의 응답을 받아서 그거중 좋은거만 모아서 응답을 다시먹이는식으로 하는것(뱉은거중 좋은거만 다시먹는거임)
	
	직접선호최적화(dpo)는 선호데이터셋을 직접 언어모델에 학습시켜서,리워드모델과 강화학습을 사용하지않는방식임
	즉 llm이 리워드모델의 역할도 같이(은닉층에서) 처리하는것임
	기본적으로 llm은 해당 문제를 처리할 기초체급을 갖추고있고,여기서 현재 처리하려는 문제에 대해 방향을 제시해주는게 지도미세조정임
	즉,해당 방향으로 적당한 학습률을 가지고 선호데이터셋을 먹여버리면,그쪽으로 사고를 유도할수있다 이런방식임
	
	요즘은 dpo를 가장 많이 사용함
	
	
5.gpu 효율적인 학습	
  1.gpu에 올라가는 데이터
    1.딥러닝 모델 데이터타입
      딥러닝을 돌릴때 가장 자주 만나는 에러는 oom임
	  기본적으로 gpu메모리에는 모델 자체가 올라가고,모델은 행렬곱을 위한 파라미터의 집합임
	  
	  일반적으론 한 파라미터는 16비트로 수를 표현함(부호1비트 지수8비트 가수7비트,bf16)
	  딥러닝모델은 학습이나 추론이나 gpu메모리에 전부 다 올라가기때문에,모델의 용량이 얼마인가가 매우 중요함
	  딥러닝모델의 용량은 파라미터수에 파라미터당 비트를 곱하면됨
	  즉 14b모델이 16비트(2바이트)로 저장된다면
	    14b=14*2=28GB
	  가 필요한것
	2.양자화
	  이렇게 14b만 해도 엄청나게 메모리를 잡아먹기때문에,파라미터수는 줄이기 싫은데 메모리사용량은 줄이고 싶어서 나온게 양자화임
	  이건 더 적은 비트로 모델을 표현하는것
	  양자화를 하면 모델의 용량이 줄어서 메모리 사용량이 줄어들지만,대신 정보가 소실되기때문에 모델의 성능이 저하됨
	  즉 양자화의 핵심은 더 적은 비트를 사용하면서 원본데이터의 정보를 최대한 소실없이 유지하는게 핵심과제임
	  
	  이때 가장 중요한건,변환하려는 데이터 형식의 수를 최대한 낭비하지않고 사용하는거임
	  즉 사용되지않는 비트공간을 최대한 없애고,주어진 범위 내에서 수들을 최대한 퍼트리는방법을 찾는게 핵심
	  
	  그래서 사용되는게,데이터들을 k개의 데이터를 묶은 블록으로 만들고,이 블록단위로 절대최댓값을 구해서 변환을 하는거임
	  이러면 이상치들을 최대한 억제하면서 데이터의 형식을 낭비하지않고 양자화를 할수있어짐
	3.gpu 메모리 분해
	  gpu 메모리엔
	    모델 파라미터
		그레이디언트
		옵티마이저 상태
		순전파 상태
	  이렇게 4가지가 올라가고,
	  딥러닝 학습은 간단히 말하면 순전파를 수행하고,그때 계산한 손실로부터 역전파를 수행하고,옵티마이저로 모델을 업데이트함
	  이떄 역전파를 수행하려면 순전파를 했을때의 값이 필요함(순전파상태값),또한 그레이디언트는 역전파 결과 생성됨
	  
	  이떄 최소로 필요한 메모리양은,2바이트x파라미터수(B)를 N으로 잡았을때,
	    모델파라미터=N
		그레이디언트=N
		옵티마이저 상태=2N(상태가 2개라서)
	  즉 4N이 기본적으로 필요하고,순전파상태를 저장하기 위한 메모리가 추가로 필요함
	  즉 모델을 사용하는것보다,모델을 학습시키려면 최소 4배가 필요하다는거,추가적으로 1배치사이즈의 데이터도 메모리에 올려야하고
	  
	  허깅페이스의 모델메모리 계산기를 사용해서 대략적인 메모리양을 알수있음

  2.단일 gpu 효율적으로 활용하기
    1.그레이디언트 누적
	  학습과정에서 배치크기를 크게 가져가면,더 빠르게 수렴하고 성능이 올라가지만,배치크기가 커지면 순전파상태저장에 필요한 메모리가 증가함
	  그래서 사용되는게 그레이디언트 누적임
	  이건 제한된 메모리 안에서 배치크기를 키우는것과 같은 효과를 얻는방법임
	  
	  이건 스탭을 n번 반복하고,n번의 스탭이 지나가고 나서 모든걸 합친다음 n으로 나눠서 역전파를 수행하고,모델을 업데이트함
	  이러면 배치사이즈가 n배 커진 효과를 받을수있음
	  대신 시간은 길어지겠지만
	2.그레이디언트 체크포인팅
	  역전파 계산을 하려면 순전파의 결과를 저장하고있어야함
	  가장 기본적인건 모두 저장하는거지만,이건 비효율적이고
	  가장 메모리를 많이 절약하는건 역전파에 꼭 필요한거만 저장하고,
	  나머지는 다시 계산하는방식인데,이건 한번의 역전파를 위해 순전파를 반복적으로 계산해야한다는 단점이 있음
	  
	  이 두가지를 절충하는 방법이 그레이디언트 체크포인팅임
	  중간중간에 체크포인트를 두고,거기서부터만 다시 계산하는식으로 동작시키는거임
	  이러면 메모리 사용량도 좀 줄고,그렇게 많이 계산할필요도 없음(즉 위에 두개를 반씩 합친느낌)
	  이건 쓰려면 걍 메서드에서 키기만 하면됨

  3.분산학습과 ZeRO
    1.분산학습
	  분산학습은 GPU를 여러개 활용해서 모델을 학습시키는걸 말함
	  이거의 목적은 크게 2가지로,모델 학습속도를 올리거나,1개의 gpu로 학습이 어려운 모델을 다루는것
	  
	  학습속도를 올리는건,각각의 gpu에 각각 모델을 올리고,학습데이터를 병렬로 처리해서 학습속도를 올리는것(데이터 병렬화)
	  여러 gpu에 모델을 나눠서 올리는건,모델병렬화라고 부르는데,이건 모델의 층별로 나누는 파이프라인병렬화와,한층의 모델도 나누는 텐서병렬화가 있음
	  즉 한모델을 세로로 자르냐 가로세로로 둘다 자르냐 차이임
	  
	  파이프라인병렬화는 진짜 직관적으로 이해가 되지만,텐서 병렬화는 행렬을 분리해도 같은 결과를 얻을수있게 행렬곱셈을 정렬함(자동으로 해줌)
	  트랜스포머에서는,셀프어텐션에서 쿼리,키,값 가중치행렬을 열방향으로 나눠서 행렬곱을 수행하는식
	2.데이터 병렬화에서 중복 저장 줄이기(ZeRO)
	  데이터 병렬화를 할때도 같은 모델을 여러 gpu에 올리는건 메모리낭비임
	  ZeRO는,하나의 모델을 하나의 gpu에 올리지않고,
	  모델병렬화처럼 모델을 나눠 여러 gpu에 올리고 각 gpu에서는 자신의 모델 부분만 연산을 수행하면 된다는 컨셉임
	  즉 4개로 나눠서 1의 속도로 처리하지말고,모델병렬화를 하고 4배의 속도로 처리하는게 속도도 비슷한데 메모리효율적이라는 논리

  4.효율적인 학습방법(PEET):LoRA
    기반모델의 크기가 커지면,하나의 gpu로 모든 파라미터를 학습하는 전체미세조정은 하기 어려움
	그래서 개인이 모델의 일부 파라미터만 학습시키는게 PEET임
	그중에서 가장 주목받는게 로라임
	
	로라는 모델파라미터를 재구성해 더 적은 파라미터를 학습해서 gpu메모리사용량을 줄임
	파라미터 재구성이란 행렬을 더 작은 2개의 행렬의 곱으로 표현해,전체 파라미터를 수정하는게 아닌 더 작은 2개의 행렬을 수정하는걸 의미함
	
	이건 원래 모델의 가중치는 그대로 두고,그 모델의 가로 세로 가중치(로라가중치) 한줄만 새로 만들어서,
	원래모델의 가중치와 로라가중치의 가로세로를 곱한걸 더하는방식임
	이러면 로라가중치에 대해서만 학습하면 되기떄문에 gpu사용량이 줄어듬
	이떄 어떤 파라미터위치를 선택할지는 정해야하고,보통 선형연산의 가중치를 선택함(트랜스포머에선 qkv와 피드포워드층)
	전체 선형층에 로라를 적용한게 일반적으론 성능이 가장 좋음

    1.로라 설정 살펴보기
	  로라를 사용할떄 결정해야할 사항은
	    모델파라미터에 더할 행렬 A,B를 만들떄 차원 r을 몇으로 할지
		  r이 작으면 로라의 성능이 낮아지고,높으면 gpu메모리를 많이먹음
		추가파라미터를 기존파라미터에 얼마나 많이 반영할지(알파)
		  즉 기존파라미터와 덧셈할때,가중치를 붙일수있음
		모델의 많은 파라미터중에서 어떤 파라미터를 재구성할지
	  임
	2.코드로 로라 학습 사용하기
	  쓰는건 쉬움
	  걍 허깅페이스의 peft라이브러리로 하면됨(책참조)
	  
	  로라를 쓰면 모델의 추론 메모리 사용량에 0.117%정도만 추가로 더 사용하기때문에 학습하기 쉬움
	  그러면서도 전체 미세조정과 거의 동일한 성능을 달성할수있음

  5.효율적인 학습방법(PEET):QLoRa
    Q로라는 양자화를 더한 로라임
	이건 추가적으로 페이지 옵티마이저로,cpu메모리를 가상메모리처럼 사용해서 oom을 방지하는 기능이 추가됨
	1.4비트 양자화와 2차 양자화
	  양자화를 하려는데,데이터가 순서를 사용하면 정렬해야하고,
	  어떤데이터가 몇번째순위인지를 저장해야하기때문에 연산량이 많고 순위를 저장하기위해 메모리를 사용한다는 단점이 있음
	  
	  만약 기존데이터의 분포를 알고있다면,연산없이 그냥 분포대로 순위를 정할수있음(정규분포를 따른다면 대충 위치아니까)
	  그안에서 선그어서 데이터를 균등하게 분리하는방식
	  
	  보통 모델파라미터는 거의 정규분포에 가깝기때문에,이런 방식을 사용해서 모델의 성능을 거의 유지하면서 빠른 양자화를 할수있음(NF4)
	  이게 4비트양자화이고,여기서 한발 더 나아간게 2차양자화임
	  2차양자화는 NF4과정에서 생기는 32비트 상수도 효율적으로 저장하는걸 목적으로함
	  이건 64개의 모델 파라미터마다 1개의 상수(정규분포)를 저장하는것,이때 다시 256개의 양자화 상수를 하나의 블록으로 묶어 8비트 양자화를 수행할수있음
	  이러면 32비트 256개를 8비트 256개와 양자화상수를 저장하기위한 32비트 1개로 바꿀수있음
	2.페이지 옵티마이저
	  페이지 옵티마이저는,일시적으로 메모리 사용량이 커지는 지점들에서 oom이 터지는걸 방지하기위해,페이징을 거쳐서 cpu메모리를 가상메모리로 사용하는것보다
	
	3.Q로라 활용하기
	  사용은 쉬움,그냥 bitsandbytes를 사용하면 모델을 불러올때 4비트양자화를 간단히 할수있음
	  
	
6.sLLM 학습하기	
  sLLM은 비용효율적인 특정작업이나 도메인에 특화된 작은 llm임
  1.데이터셋
    기존에 구축된 대표성있는 데이터셋이 있으면 그걸 사용하면되지만,없다면 직접 만들어야함
	이때 보통 gpt4를 사용해서 만들면 좋음
	
	만들어진 데이터셋을 검수하면 좋긴하겠지만,시간비용상 힘드니까 규칙기반으로 간단하게 데이터정제만 해두고 쓰면됨

  2.성능평가 파이프라인 준비하기
    평가에서도 가장 쓰기 편한건 gpt4임
	llm모델을 개발할떄 제일 어려운게 성능평가임
	그래서 sql을 만드는 sLLM의 경우엔,생성한 sql과 정답 sql을 비교해서 정답여부를 gpt4로 판단받는형식으로 사용할수있음
	
	이떄 필요한건 평가데이터셋과,프롬프트와,평가에 사용할 코드(api호출)임
	평가데이터셋은 합성데이터셋을 12%정도 나눠서 사용하면됨
	단 이 평가데이터셋의 경우 일일히 사람이 확인해서 정확도를 높이는게 좋음(평가에서 일관성없으면 성능이 매우떨어지기때문)
	
	프롬프트는 llm의 경우 학습에 사용한 프롬프트형식을 추론할떄도 동일하게 사용해야 결과품질이 좋음
	
	평가할땐 뭐 평가데이터셋 수가 작으면 금방하지만,많은경우엔 병렬처리를 해야함
	이럴땐 openai cookbook의 깃허브 저장소의 코드(책참조)를 사용하면 편함
	얘는 비동기로 동작하고,에러뜨면 재시도도 알아서하고,요청제한걸리면 다시 요청보내고 이런걸 알아서해줌

  3.실습:미세조정 수행하기
    일단 가장 먼저해야할건 기반모델 선택임
	젤 편한건 리더보드보고,사용할 크기중에서 가장 차트높은걸 고르는거
	일단 불러와서 한번 평가데이터셋과 프롬프트넣어서 돌려보는게 좋음(기본모델의 정답률 수치확인)
	
	그리고나서 autotrain-advanced라이브러리로 미세조정을 하면됨
	자세한 학습코드는 책참조하고,메모리에러가 발생하면 배치사이즈를 좀 줄여보기
	
	또한 기반모델을 바꿔가면서 똑같이 학습시켜서 성능을 테스트해보는것도 좋음
	
	
7.모델 가볍게 만들기	
  모델의 성능을 약간 희생하면 gpu비용을 크게 낮출수있음
  1.언어모델추론 이해하기  
    언어모델이 텍스트 생성을 마치는건,eos(스퀸스끝)토큰을 생성하거나,최대길이를 넘기면 끝남
	그전까진 계속 루프를 돌리면서,자기가 생성한 모든 텍스트를 쿼리에 담아서 어텐션을 돌려 새로운 토큰을 찾아냄
	
	언어모델은 한번에 하나의 토큰만 생성할수있고,입력텍스트를 기반으로 바로 다음토큰만 예측하는 자기회귀적 특성을 가짐
	하지만 이미 처리했던 텍스트는 동시에 병렬적으로 처리할수있어서,시간이 늘어나진않음
	
	1.kv캐시
	  이렇게 셀프 어텐션 처리과정에서,같은 입력토큰에 대해 중복계산이 발생하는 비효율을 줄이는게 kv캐시임
	  얘는 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법임(그래서 kv임)
	  즉 이전에 했던계산은 계산결과를 가져오고,새로운토큰만 연산하는식
	  
	  대신 캐시를 저장해야하기때문에 메모리가 소모됨
	    2바이트(fp16사용시)*2(k,v)*어텐션레이어수*토큰임베딩차원*최대시퀸스길이*배치크기=kv캐시메모리
	  임
	2.gpu구조와 최적의 배치 크기
	  서빙이 효율적인지 판단할떈,비용,처리량,지연시간이 있음
	  즉 시간당 처리한 요청수(처리량)와,하나의 토큰을 생성하는데 걸리는시간(지연시간),비용임
	  효율적인 서빙은 같은gpu로 처리량을 높이고 지연시간을 낮춰야함
	  
	  추론을 수행할떈 배치크기만큼의 토큰을 한번에 생성함
	  그런데 모델은 고대역폭메모리에 저장하고,연산은 sram에서 수행되는데,연산을 하려면 고대역폭메모리의 파라미터를 sram으로 옮겨야함
	  즉 모델을 옮기는 시간이 있음
	  최적의 배치크기보다 배치크기가 작으면 모델파라미터를 이동시키느라 연산이 멈추는구간이 생기고(메모리바운드),
	  최적의 배치크기보다 배치크기가 크면 연산이 오래걸려서 지연시간이 길어짐(연산바운드)
	  
	  즉 적절한 배치크기를 찾아야함
	  계산식은
	    2*P*배치크기/하드웨어연산속도=P/메모리대역폭
	  임
	3.kv캐시 메모리 줄이기
	  트랜스포머는 어텐션 연산을 한번에 여러헤드에 대해 수행함
	  이때 많은수의 키값벡터를 저장해서 많은 메모리를 사용하고,더많은 데이터를 불러와야해서 속도가 느려짐
	  
	  이걸 해결하려고 모든 쿼리벡터가 하나의 키값 벡터를 공유하는 멀티쿼리어텐션이 등장했지만,성능이 떨어졌음
	  그래서 나온게 n개단위로 묶어서 사용하는 그룹쿼리어텐션임(즉 개별과 전체통합의 중간)
	  이건 그렇게 성능이 낮지도않고 속도도 빠르고 메모리도 덜먹음

  2.양자화로 모델 용량 줄이기
    양자화는 학습 후 양자화와,양자화 학습으로 나누는데 보통 학습후 양자화를 양자화라고 부름
	양자화는
	  비츠 앤 바이츠
	  GPTQ
	  AWQ
	가 주로 사용됨
	
	1.비츠앤바이츠
	  비츠앤바이츠 라이브러리는 쉽게 양자화를 할수있는 라이브러리임
	  이건 8비트로 연산을 수행하면서 성능저하가 거의없이 성능을 유지하는 8비트행렬연산과,4비트 정규분포 양자화방식(이게 4비트양자화랑 같은거)을 지원함
	  4비트는 앞에서본거랑 같고
	  
	  8비트는 기본적인 로직은 거의 비슷한데,크기가 큰 이상치가 포함된 열은 별도로 분리해서 16비트 그대로 계산하는것
	  
	  얘들이 쓰는 양자화는 다른 양자화와 달리 별도의 시간이 걸리지않는게 장점임(모델을 불러오면서 바로 양자화가 가능)
	
	2.GPTQ
	  GPTQ는 양자화 이전의 모델에 입력 x를 넣었을떄와,양자화이후 모델에 입력x를 넣었을때 오차가 가장 작아지게 모델의 양자화를 수행하는 방식임
	  즉 열단위로 한줄한줄 테스트해가면서 양자화를 해보는거임
	  
	  이건 방식상 시간이 좀 걸림(175b기준 a100으로 4시간)
	  그래서 이미 양자화한 모델이 있으면 그거 가져다쓰는게 좋음
	
	3.AWQ
	  양자화란 모델이 가지고있는 정보를 최대한 손실없이 보존하면서 용량을 줄이는것
	  AWQ는 모든 파라미터가 동등하게 중요하진않고,특별히 중요한 파라미터는 그대로 두고 나머지만 양자화를 하자는 방식임
	  
	  이때 중요한 파라미터는,모델파라미터의 값이 크거나(값이 크다면 큰영향을 주니까),입력데이터의 활성화값이 큰 채널을 중요하다고 가정하는것
	  그래서 상위1퍼의 모델파라미터는 그대로 두고,99퍼만 양자화하는식임
	  
	  그러나 여러종류의 데이터타입이 섞이면 연산이 느려지기때문에 이걸 해결해야함	  
	  이건 대충 말하자면,스케일러를 사용해 숫자를 크게만들어서 차이를 만들어내는것(그러면서 데이터타입은 통일하고)
	  1.5와 1.7을 양자화하면(2를곱하고 반올림하면) 둘다 3이되지만,모든 파라미터에 2를 한번더 곱해주면 6과 7이 됨
	  이런식으로 스케일러를 사용해서 정보의 손실을 막는것(이떄 스케일러는 2가 제일 적절함)

  3.지식증류 활용하기
    증류는 더 크고 성능이 높은 선생모델의 출력으로,더 작고 성능이 낮은 학생모델을 만드는 방법임
	학생은 선생의 생성결과를 모방하는방식으로 학습함,즉 선생의 더 큰파라미터로 쌓은 값을 더 작은 모델로 압축해 전달하는느낌
	
	요즘은 선생모델로 아예 새로운 학습 데이터셋을 대규모로 구축하거나,사람이 했던일을 다 gpt에 맞기는등 이런식으로 사용됨
	즉 데이터를 만들거나,데이터를 선별하는걸 시키는거

8.sllm 서빙하기
  1.효율적인 배치전략
    가능하면 한번에 많은 입력데이터로 한꺼번에 처리하는게 당연히 효율적인 처리가 가능함
	그러나 언어모델의 특성상,어디까지 예측할지(앞으로 몇토큰 예측할지)를 알기 어렵기떄문에 배치전략을 세울때 고려할사항이 많음
	
	1.일반배치(정적배치)
	  그냥 보통 배치처리하는느낌으로,한번에 n개를 받아서 동시처리하기만 하는 방식이 일반배치(정적배치)임
	  이때 문제는,중간에 일찍 끝난 입력도 다른애들이 끝나기까지를 기다려야하고,중간에 일찍끝난애가 있으면 그만큼 결과적으로 배치크기가 작아져 효율성이 떨어짐
	2.동적배치
	  이건 일반배치를 조금 변형한건데,일반배치가 n개를 다받을때까지 계속 기다린다면,이건 서비스에 맞게 n초동안 요청을 묶어서 배치크기를 키움
	  즉 약간의 지연시간을 추가하고 gpu효율성을 올린거
	3.연속배치
	  이건 일반배치에서 한번에 들어온 배치데이터의 추론이 끝날떄까지 기다리지않고,생성이 끝나면 그자리에 다시 새 문장을 넣어서 새로운 요청을 처리하는방식
	    115555
		22227777
		3336666
		44444888
	  이런느낌임
	  이건 대신 신경써야할게 좀 있음
	  언어모델추론은 입력프롬프트를 병렬처리하는 사전연산과 토큰을 생성하는 디코딩으로 나눌수있는데,
	  이 두개는 처리방식이 달라서 한번의 토큰생성이 끝날때마다 새 문장을 배치하는게 아닌,처리중인문장수와 대기중인 문장수의 비율을 보고 추가하기도함
	  
  2.효율적인 트랜스포머 연산
    1.플래시 어텐션
	  이건 트랜스포머가 더 긴 시퀸스를 처리하도록 만들어진 방법임
	  트랜스포머는 연산량이 학습과정에선 시퀀스길이의 제곱에 비례하고,추론과정에선 시퀸스길이에 비례하기때문에 긴걸 처리하기 힘듬
	  즉 학습과정에서도 추론처럼 시퀸스에 비례하는 1차함수로 만드는게 목적임
	  
	  트랜스포머연산은 쿼리와 키 벡터를 곱하는데서 많은 메모리를 소모함
	  정확히는 행렬곱을 하는데 드는 시간보다,마스크,소프트맥스,드롭아웃처리에 드는시간이 행렬곱보다 훨씬많음
	  그런데 행렬곱이 어텐션의 핵심이고,연산량도 제일 많음
	  문제는 연산량보다 메모리를 사용하는게 더 오래걸린다는것(메모리를 읽고쓰는게)
	  
	  그래서 플래시어텐션은 블록단위로 어텐션을 수행하고,전체어텐션행렬을 쓰거나 읽지않는식으로 어텐션연산의 속도를 높였음
	  그래서 블록단위라서 hbm과의 io를 최대한 줄이고,sram에서 다 처리할수있게 만든거
	  이러면 n*n행렬을 저장하지않고 어텐션연산이 가능함,대신 역전파때는 순전파를 다시계산해야하지만,연산시간보다 입출력시간이 훨씬 크기떄문에 이게 훨씬빠름
	2.플래시 어텐션2
	  이건 1에서 순전파시와 역전파시의 gpu처리량이 낮았던걸 개선한버전임
	  이때 행렬곱이 아닌 연산을 줄이고,시퀸스길이방향의 병렬화를 추가해서 개선했음
	  
	  행렬곱이 아닌 연산은 행렬곱인 연산보다 16배 비싸서 행렬곱이 아닌 연산을 최대한 줄이고,
	  기존에는 배치크기*어텐션헤드수만큼의 스레드블록으로 병렬처리를 했다면,여기에 시퀸스길이방향 병렬화를 추가한것
	  
	  이런식으로 2배가량의 속도를 향상시켰음
	3.상대적 위치 인코딩
	  셀프어텐션은 입력토큰의 위치에 관계없이 모두 동등하게 처리하기때문에 위치정보를 별도로 추가해야함
	  이떄 대충 수식을 거쳐서 수식에서 정해지는값을 위치임베딩값으로 더해주는식으로 처리됐음
	  
	  그러다가 이 위치인코딩도 학습파라미터로 두고 함께 학습시키기 시작함(학습할수있는 임베딩층)
	  이래도 결국 추론때는 위치임베딩값을 더하는건 같지만
	  이런방식이 절대적 위치 인코딩임
	  이 절대적 위치 인코딩의 단점은,학습데이터보다 더 긴 입력이 들어오면 빠르게 생성품질이 떨어짐
	  
	  그래서 나온게 상대적 위치 인코딩임
	  절대적 위치인코딩은 어텐션값에서 현재 두 토큰간의 거리가 아닌,전체 문장에서 어디에 위치하냐에 따라 다른값이 더해지지만,
	  상대적 위치인코딩은 전체 텍스트가 아닌,현재 토큰의 위치 기준으로 다른 토큰들이 얼만큼 떨어져있냐를 기준으로 모델에 입력해서,
	  전체문장이 아닌 토큰간의 거리를 학습하게함
	  
	  대표적인 상대위치인코딩으로 RoPE와 ALiBi가 있음(보통 ALiBi사용)
	  이런걸 사용하면 입력토큰이 길어져도 성능이 빠르게 나빠지지않음
	  또한 간단한 인코딩방식이라 학습과 추론에 추가시간도 들지않음

  3.효율적인 추론 전략
    1.커널퓨전
	  gpu연산은 커널단위로 이뤄짐
	  하지만 연산을 수행하기위해선 전후에 추가적인 작업을 위한 오버헤드가 발생함
	  그래서 반복적으로 수행하는 연산에 대해선,각 커널단위로 분리하는거보다 하나로 묶어 오버헤드를 줄이는 커널퓨젼을 사용하면 좀 더 효율적이됨
	  
	  플래시어텐션도 이런방식으로 오버헤드를 줄인것
	
	2.페이지 어텐션
	  kv캐시는 키값벡터를 중복연산을 줄여 연산시간은 줄이지만,메모리 사용량이 커져서 배치크기가 줄어듬
	  그래서 페이지어텐션을 사용하면 kv캐시를 효율적으로 관리하면서 배치크기도 크게잡을수있음
	  
	  기존 kv캐시가 앞으로 사용할수도있는 메모리를 미리 잡아두면서 메모리낭비가 심했다면,이걸 개선한것
	  핵심은 kv캐시가 연속적인 물리 메모리를 사용하기위해 미리 메모리를 준비하기때문에 발생함
	  그래서 가상메모리처럼 논리적메모리와 물리적메모리를 연결하는 블록테이블을 관리해서 논리적으로 연속적이게 만든게 페이지어텐션임
	  
	  또한 같은프롬프트의 결과값을 공유하는등의 여러 부가효과가 있지만 크게 중요하진않은듯?(병렬샘플링시 값공유)
	
	3.추측 디코딩
	  이건 쉬운단어는 더 작고 효율적인 모델이 예측하고,어려운단어는 크고 성능좋은모델이 예측하는 방식임
	  이건 작은 드래프트모델과 큰 타깃모델을 사용하고,작은 드래프트모델이 빠르게 추론해서 k개의 토큰을 먼저 생성하고,
	  타깃모델이 이 토큰이 내가만든것과 같을지 계산해서 동일하다면 승인하고 동일하지않다면 거절함
	  
	  즉 일단 드래프트 모델이 2개를 먼저 만들고,이걸 병렬적으로 타겟이 검증하는 방식임
	  이것도 연산속도보단 메모리입출력속도가 빠르다는거에서 나온방식임
	  즉 n개의 토큰을 작은모델로 만들고,그걸 타겟이 검증하면서 특정 토큰의 일치도가 낮을경우 그 이후의 모든 토큰을 버리는식으로 동작함
	  
  4.실습:llm서빙 프레임워크
	서빙은 vLLM이라는 프레임워크를 쓰면 됨
	llm의 추론은 크게 오프라인추론과 온라인추론으로 나눠짐
	
	1.오프라인 서빙
	  이건 정해진 입력데이터에 대해 배치추론을 수행하는걸 말함
	  모델에 입력할 데이터가 이미 정해져있기때문에,배치처리로 처리량을 높일수있음
	  
	  사용법은 실제사용할떄 책보자
	
	2.온라인 서빙
	  이건 요청이 올때 추론을 수행함
	  즉 서버를 띄우고 추론을 그쪽에서 날리는거
	  vLLM쓰면 서버띄우는건 알아서해줌
	
	  
9.llm어플리케이션 개발하기	  
  1.rag
    rag는 검색증강생성으로,답변에 필요한 충분한 정보와 맥락을 제공하고 답변하게 하는 방법을 말함
	이때 보통 정보를 검색해서 선택하기때문에 검색이 붙은거
	
	rag는 검색할 데이터를 벡터db에 저장하고,사용자의요청을 벡터 db에서 검색한후 사용자의 요청과 결합해 프롬프트를 완성하는 과정임
	보통 rag는 라마인덱스나 랭체인으로 구현함
	1.데이터 저장
	  검색된 데이터도 임베딩모델을 거쳐서 저장되어야 정상적으로 모델이 사용할수있음
	  이 임베딩백터는 벡터db에 저장됨
	  
	  이때 검색결과는 임베딩을 거쳐서 db에 저장되게되고,사용자의 쿼리도 임베딩을 거치는데,이건 db에서 조회를 하게됨(코사인유사도등으로 가장가까운거 찾기)
	2.프롬프트에 검색결과 통합
	  앞서 저장한 텍스트를 llm에 전달하기위해서는,사용자의 요청과 관련이 큰 문서를 벡터db에서 찾고,검색결과를 프롬프트와 통합해서 llm에 던져야함
	  즉 사용자는 프롬프트만 던지게되므로,중간에서 오케스트레이션해줄 도구가 필요해지게됨
	  
	  즉 핵심은,데이터를 저장하는 벡터db와,질문으로 저장된 데이터중 가장 유사도가 가까운걸 찾는 행위 이 2개만 있으면 일단 rag를 만들수있는거
	
	3.라마인덱스로 rag구현
	  라마인덱스는 대표적인 llm오케스트레이션 라이브러리임
	  이 실습은 미리 데이터셋을 벡터화시켜서 db에 담아두고,그 db에 프롬프트를 던져 유사문서를 찾아내서 컨텍스트를 만드는 실습임
	  상세는 책보자

  2.llm캐시
    llm을 통해 생성을 수행하는 작업은 시간과 비용이 많이듬
	만약 비슷한 프롬프트가 이미 들어온적이 있다면,해당 프롬프트의 결과를 다시 던져줄수있음
	
	이 캐시도 벡터db를 사용해서 구현할수있음
	1.작동원리
	  캐시는 프롬프트통합과 llm생성 사이에 위치함
	  즉 llm에 넣기 직전(컨텍스트와 프롬프트를 합치고나서),벡터db를 하나 더 만들어서 비슷한게있는지를 확인해보고 없어야지 llm에던지는방식임
	  이때 완벽히 일치할때의 일치캐시와,유사요청을 확인하는 유사검색캐시로 나눌수있음
	  일치캐시는 그냥 비교하면되는데,유사검색캐시는 임베딩으로 바꾸어서 비교해야함
	2.실습
	  파이썬 딕셔너리(일치캐시)와 크로마 라이브러리(유사검색캐시)로 캐시를 구현할수있음

  3.데이터 검증
    llm이 생성한 데이터가 적절한지를 상용llm에서는 확인해야함(시스템 프롬프트요청,정치적 입장을 묻는 질문,개인정보등)
	방식으로는 규칙기반,분류,회귀모델사용,임베딩유사도기반,llm활용등을 사용할수있음
	요즘은 다 llm으로 하거나 임베딩유사도로 하는게 젤 많이쓰이긴하는듯(둘을 같이쓰거나)
	1.실습
	  nemo-guardrails 라이브러리도 구현할수있음

  4.데이터 로깅
    사용자의 입력과 llm의 출력을 기록하는건,llm의 특성상 같은입력이라도 출력이 달라질수있기때문에 반드시 필요함
	이건 뭐 직접만들어도되고,라이브러리써도됨
	라마인덱스쓰면 검색증강생성 과정을 w&b같은 라이브러리에 기록할수도 있음
	
10.임베딩모델로 데이터의미 압축하기	
  1.텍스트 임베딩 이해하기
    여러문장의 텍스트를 임베딩벡터로 변환하는걸 텍스트임베딩 또는 문장임베딩이라고 부름
	임베딩은 데이터의 의미를 압축한 숫자배열을 말함
	
	1.문장임베딩의 장점
	  데이터를 압축해서 숫자로 표현하는건,해당 숫자로 벡터를 표현할수있기때문에 중요함(코사인유사도등으로)
	  예전엔 원핫인코딩과 백오브워즈,tf-idf같은 희소백터계열로 하기도했지만, 특히 워드투벡방식을 요즘은 자주사용함
	2.워드투벡
	  이건 희소벡터가 아닌 밀집 임베딩 방식이고,문장임베딩도 같은 밀집임베딩방식임
	  이건 특정 단어 주변에 어떤단어가 있는지 예측하는 모델을 만들고(가운데 단어를 예측하는 방식과 중간단어로 주변단어를 예측하는 방식)
	  이걸로 단어를 임베딩벡터로 변환하는거임

  2.문장 임베딩 방식
    문제는 워드투벡같은경우도 단어단위로 임베딩으로 바꿔서,문장 전체로 봤을때 특정단어가 다른단어에 영향을 줄수없음(어텐션이 안된다는느낌)
	1.문장 사이 관계를 계산하는 두가지 방법
	  그래서 사용되던게,bert모델로 문장을 2개 주고,두개간의 코사인유사도를 구하는 방식임
	  이때 두층을 따로 계산한뒤 코사인 유사도를 구하는방식(바이인코더)와
	  두 문장을 같이 모델에 넣고,모델이 직접 두문장 사이의 관계를 0에서 1까지 출력하는 교차인코더방식이 있음(어텐션쓰는거임)
	  
	  교차인코더가 연산량은 더 많지만,좀 더 정확한 관계예측이 가능함
	  그러나 교차인코더는 새문장과 유사한문장을 찾고싶을때마다 모든 문장에 대해 연산을 해야함(전체db를 전부 검색해서 연산돌려야함)
	  그래서 확장성이 떨어져서,바이인코더를 보통 사용함
	  
	  바이인코더는 각 문장마다의 독립적인 임베딩을 결과로 반환하기때문에,각 문장별로 한번씩만 하면됨
	  그리고 각 임베딩을 저장해둔후,나중에 필요할떄 코사인유사도연산만 하면되는거임
	2.바이 인코더 모델 구조
	  바이인코더는 모델의 출력을 풀링층을 통해 고정된크기의 문장임베딩으로 만듬
	  모델은 입력토큰마다 출력임베딩을 생성해서 문장의 길이가 달라지면 출력하는 임베딩수가 달라지기때문에 쓰기힘들어서,풀링층을 거쳐서 같은크기로 통일시키는것
	  
	  sentence-transformers라이브러리를 사용하면 바이인코더를 쉽게 쓸수있음
	  자세한 구현은 책 참조하고
	  풀링모드를 설정해야하는데,이건 모델이 출력한 결과임베딩을 고정크기문장임베딩으로 통합할때의 방식임
	    클래스모드:첫토큰의 출력임베딩을 문장임베딩으로 사용
		평균모드:모든 입력토큰의 출력임베딩을 평균한값을 문장임베딩으로 사용
		최대모드:모든입력토큰의 출력임베딩에서 문장길이방향에서 최대값을 찾아 문장임베딩으로 사용
	  이렇게 3개가 있음(cnn에서 풀링층과 비슷한느낌임)
	  보통 평균모드를 디폴트로 사용함
	3.텍스트와 이미지 임베딩생성
	  이미지모델을 가지고 임베딩을 생성할수도있음
	  결국 입력을 임베딩으로 바꾸고,고정크기로 바꾸는거니까(모델만 잘고르면됨)
	
  3.실습:의미검색 구현
    faiss를 사용해서 벡터연산을 해서 시멘틱서치를 할수있음
	자세한건 책참조
	
	의미검색은 키워드가 동일하지않아도 의미가 유사하면 찾을수있다는 장점이 있지만,관련성이 떨어지는 검색결과가 나오기도 한다는 단점이있음
	그래서 bm25(키워드검색)과 하이브리드로 쓰기도함
	
	또한 라마인덱스에서도 임베딩을 해서 쓸수있음(이게 더 편함)

  4.검색방식을 조합해 성능 높이기
    키워드검색은 의미검색과 달리 같은 키워드가 많이 포함될수록 유사도를 높게 평가하는 방식임
	이건 정확성이 높은대신 키워드가 달라지면 검색하지못한다는 단점이있음(시멘틱서치와 반대)
	그래서 이 둘을 섞어쓰기도함
	
	1.bm25
	  이건 tf-idf의 발전형임
	  대신 특정 단어가 자주나와도 일정값이상으로 커지지않고,짧은문서에서 등장하는 단어는 중요도를 높게 판단하는 방식임
	2.상호순위조합이해하기
	  하이브리드 서치는 키워드검색도 하고,시멘틱서치도 해서 둘을 합쳐서 사용하는방식임
	  이떄 둘의 가중치를 같게 만들어서(범위를) 더해야함
	  이게 상호순위조합임

  5.실습:하이브리드검색 구현
    구현은 책보자
	  
11.자신의 데이터에 맞춘 임베딩모델 만들기:RAG 개선하기	
  교차인코더는 느리지만 더 정확하게 유사도를 계산할수있음
  그래서 교차인코더와 바이인코더를 결합해서 rag에서 검색성능을 높이고싶어하는 욕구가 생김
  즉 교차인코더로 갯수를 줄이고,바이인코더로 정확한계산을 하는방식
  1.언어모델을 임베딩모델로 만들기
    언어모델에 풀링층을 붙여서 학습시키면 바로 임베딩모델로 사용할수있음(풀링모드는 보통 평균모드)
	문장임베딩 모델을 학습시킬땐 보통 대조학습을 사용함
	관련이 있거나 유사한 데이터는 더 가까워지게 만들고,관련이 없거나 유사하지않은 데이터는 더 멀어지게 하는 방식
	
	1.대조학습
	  이건 모델의 특성에 따라 어떻게 유사한애들을 붙일건지를 선택할수있음
	  질문과 답변을 유사도높게 학습시킬수도있고,그냥 비슷한문장끼리 유사도높게 학습시킬수도있고,서로 이어지는문장이면 유사도 높게 학습시킬수도있음
	  보통은 서로 유사한 데이터셋을 수집한 데이터셋이나,서로 이어지는 문장을 수집해 임베딩모델의 학습에 사용함
	  
	  언어모델을 추가학습하지않고,바로 풀링층을 붙여서 동작시켜보면 성능이 낮은걸 확인할수있음
	  또한 학습할때의 손실함수는 코사인시밀러리로스(cosineSimilarityLoss)를 사용함
	  자세한 학습법은 책참조

  3.임베딩모델 미세조정하기
    검색증강생성은 검색쿼리와 관련된 문서를 찾아 프롬프트에 컨텍스트로 추가할때 임베딩모델을 활용함
	그래서 임베딩모델을 추가학습해서,실습데이터와 문장사이 유사도를 더 잘 계산할수있게 만들수있음
	자세한 학습코드는 책참조
	
	rag가 잘 동작하려면,질문과 관련된 기사본문이 검색결과의 상위에 나타나야하고,
	그러려면 관련있는질문내용쌍에는 높은 유사도 점수를,관련없는 질문내용쌍에는 낮은 유사도점수를 줘야함
	
	문장임베딩 모델도 다른 모델처럼 학습데이터와 유사한 데이터에서 가장 잘 동작함
	그래서 파인튜닝을 거쳐서 쓰는게 좋음
	1.mnr로스를 활용해 미세조정하기
	  이건 코사인 유사도 로스 대신 쓸수있는 손실함수로,데이터셋에 서로 관련이 있는 문장만 있는경우에 사용하기 좋은 손실함수임
	  이걸 쓰면 하나의 배치데이터 안에서 다른 데이터의 기사본문을 관련이 없는 데이터로 사용해 모델을 학습시킴
	  그래서 서로 관련이 있는 데이터만으로 학습데이터를 구성하면됨

  4.검색품질을 높이는 순위 재정렬
    바이인코더로 후보군을 압축하고,교차인코더로 순위를 재정렬할때,교차인코더로는 보통 문장분류모델을 활용함
	유사도계산은 두 문장을 입력으로 받아서 관련이 있는지없는지를 분류하는 문제로 바라볼수있기때문

  5.바이인코더와 교차인코더로 개선된 rag 구현하기
    구현과 검증이라 생략
	구현할때보자
	대충 기본임베딩모델만 쓰기,파인튜닝한 임베딩모델 쓰기,파인튜닝모델+순위재정렬(바이인코더)인데
	  기본임베딩 88%,0.013초
	  파인튜닝  94.6%,0.014초
	  파인튜닝+순위재정렬 97.3%,1.1초
	이렇게 걸림
	파인튜닝까지는 하는게 나은거같고,바이인코더는 선택일듯

12.벡터 db로 확장하기:rag구현하기	
  1.벡터db란
    벡터db는 벡터임베딩을 키로 사용하는 db를 말함
	모든 정보는 임베딩으로 변환할수있음
	이렇게 임베딩으로 정보가 변환되었다면 이걸 벡터db에 저장하고,벡터간의 거리로 유사데이터를 찾을수있음
	즉 벡터db는 벡터사이의 거리계산에 특화된 db라고 보면됨
	
	기존 머신러닝의 경우 특성을 사람이 일일히 뽑아내줘야했다면,딥러닝을 대충데이터때려박으면됨(특성뽑는것도 학습하니까)
	이런 특징을 표현학습 이라고 함
	이런 특징을 활용해서,가까운걸 가깝게,먼걸 멀게 표현할수있으면 구분할수있어짐
	벡터db는 이런 임베딩벡터의 특징을 이용하기위해 개발됐음
	벡터db를 사용하는 단계는
	  저장:저장할 데이터(문서)를 임베딩모델을 거쳐 벡터로 변환해서 벡터db에 저장
	  검색:검색할 데이터(검색쿼리)를 임베딩모델을 거쳐 벡터로 변환하고 벡터db에서 검색
	  결과반환:벡터db에서 검색쿼리의 임베딩과 거리가 가까운 벡터를 찾아 반환
	순서임
	이러면 검색쿼리와 가장 가까운 벡터를 찾을수있는데,이건 가장 유사한 데이터를 의미함
	보통 유클리드거리,코사인유사도,점곱등을 가장 많이 활용함
	
	벡터db엔 벡터전용 db와 벡터도 저장할수있는 db등이 있음
	벡터전용으로는 파인콘,크로마등이 있고,겸용으로는 포스트그레스큐엘,몽고디비등이 있음
	아예 가볍게하려면 벡터라이브러리도 있고

  2.벡터 db 작동원리
    벡터db는 벡터사이의 거리를 계산해서 유사한 벡터를 찾음
	가장 기본적인 방법은 저장된 모든 벡터를 조사해 가장 유사한 k개의 벡터를 반환하는 knn방식임
	보통 이건 안쓰고 ann을 사용함
	1.knn과 한계
	  이건 직관적이고 모든 데이터를 조사해서 정확하지만,연산량이 데이터에 비례하게 늘어나 속도가 느려짐 
	  
	  벡터검색을 위해선 인덱스를 먼저 만들어야함
	  이때 중요한건 인덱스의 메모리사용량과 색인시간이고,검색에서는 검색시간과 재현율(실제로 유사한게 몇개나 반환됐는지)임
	  
	  knn은 벡터차원이 커지면 검색속도가 느려지기때문에,정확도를 약간 희생하고 속도를 올리는게 ann임
	2.ann검색
	  ann은 근사최근접이웃으로 대용량 데이터셋에서 주어진 쿼리항목과 가장 유사한 항목을 효율적으로 찾는데 사용되는 기술임
	  이건 약간의 정확도를 희생해서 훨씬 더 빠른 검색속도를 제공함
	  
	  이건 임베딩벡터를 빠르게 탐색할수있는 구조로 저장해서,검색시 탐색범위를 좁히는데 집중함
	  여기엔 ivf와 hnsw등이 있음
	  ivf는 검색공간을 제한하려고,데이터셋을 클러스터로 그룹화하는방식이고
	  hnsw는 그래프기반 인덱싱구조임
	  상위에선 연결이 적고,하위에서 연결이 밀집된 다층그래프를 구축해서 그래프를 빠르게 탐색하는방식
	  보통 hnsw를 주로 사용함
	  
	  계산범위를 좁히면 재현율이 떨어지기때문에,파라미터의 의미를 잘 이해해야함
	  여기서 재현율은
	    재현율=예측중 정답/실제정답의 수
	  임
	  
	3.탐색가능한 작은세계(nsw)
	  hnsw는 그래프기반의 ann검색알고리즘임
	  그래프는 노드와 간선으로 이뤄지고,노드는 저장하는데이터를 의미하고,벡터임베딩이 노드가 됨
	  간선은 노드와 노드를 연결하는선으로,간선을 통해 서로 연결된 노드만 탐색이 가능해서,
	  노드끼리 얼마나 많은 간선을 연결할지가 성능과 속도에 영향을 줌
	  
	  탐색가능한 작은세계란,완전히 랜덤한 그래프와 완전히 규칙적인 그래프 사이에,적당하게 랜덤하게 연결된 그래프 상태를 말함
	  완전규칙그래프는 정확한탐색이 되지만,검색시간이 길어지고,완전랜덤은 서로 연결되지않은 노드가 있다면 탐색할수없어짐
	  탐색가능한 작은세계는,완전규칙그래프에서 간선몇개를 랜덤하게 바꾼형태임
	  이건 이슈가 하나 있는데,지역최솟값문제임
	  이걸 해결하기위해 계층구조를 추가했음
	4.계층구조
	  데이터가 크기순으로 정렬되어있다면,링크드리스트를 처음부터 다 훑는거보다,여러단계를 건너뛰는값을 알고있으면 거기로 가면 빠름
	  대충 이런방식으로,다음값에 추가로 랜덤하게 방향성만 맞는 랜덤위치의 값을 가지는느낌으로 구성하는거임
	  그래서 랜덤위치가 목표보다 작으면 랜덤위치로,크면 그냥 다음위치로 이동하는식
	  즉 정상적으로 이동하는거에,지름길을 추가한느낌

  3.실습:HNSW인덱스의 핵심파라미터 이해하기
    hnsw인덱스의 경우 하나의 벡터에 연결하는 최소연결수(m),
	색인과정에서 가장 가까운 n개를 선택하기위해 저장하는 후보의수(ef_construction),
	검색과정에서 가장 가까운 k개를 선택할떄 저장하는 후보의수(ef_search)세가지 파라미터로 메모리 사용량,색인시간,재현율,검색시간이 달라짐
	
	1.최소연결수(m)
	  m은 임베딩벡터에 연결하는 간선의 수임
	  벡터에 연결되는 간선수가 많을수록 그래프가 촘촘해져서 검색의 품질,즉 재현율이 올라가지만,
	  더 많은 연결을 만들어야해서 메모리사용량이 올라가고 색인시간이 길어짐
	  또한 탐색과정에서 더 많은 간선을 고려하기때문에 검색시간도 증가함
	  
	  보통 m은 32를 기본값으로 세팅하고,5~48사이를 사용함
	  m값에 따른 상세수치는 책에있음
	  
	2.색인의 후보의수(ef_construction)
	  인덱스에 새 벡터를 추가할때,검색할때처럼 유사하게,추가한 벡터와 가장 가까운벡터를 탐색함
	  이떄 ef_construction는 n개의 가장 가까운 벡터를 선택할 후보군의 크기로,이게 크면 실제로 가장 가까운벡터를 선택할 확률이 올라감(재현율 업)
	  대신 색인시간이 증가함,메모리나 검색시간은 크게 영향받지않음
	  
	  보통 이건 크게 재현율에 영향을 주진않음,그리고 메모리나 검색시간도 달라지지않아서 320정도 잡고써도됨
	  상세수치는 책에있음
	
	3.검색의 후보의수(ef_search)
	  이건 ef_construction과 비슷하지만,검색에서 사용되는 후보군의 크기임
	  이건 크면 재현율이 올라가지만,검색시간이 길어짐
	  이것도 메모리사용량과 색인시간엔 영향을 주지않지만,검색시간이 길어짐
	  
	  이건 재현율에 좀 영향을 줘서,32정도를 기본값으로 쓰고,앞뒤로 테스트해보는게좋음

  4.실습:파인콘으로 벡터검색 구현하기
    이건 대충 파인콘쓰는법이라서 필요해지면보면될거같음
	글케어렵진않음,결국 crud라서
	그리고 라마인덱스에서 파인콘 연결하는법도 있음

  5.실습:파인콘을 활용해 멀티모달검색 구현하기
	임베딩모델만 있다면 어떤 데이터든 임베딩벡터로 만들수있음
	이미지나 텍스트나 전부 같은 벡터공간에 임베딩할수있다는것(모델만 잘 선택하면)
	그렇게 어렵진않으니 필요해지면보자 

13.llm운영하기
  1.mlops
    mlops는 데브옵스에서 데이터와 모델까지 추가한 개념임
	mlops의 핵심은 데이터수집부터 전처리,학습,평가,배포에 이르는 전체 과정을 자동화하고 관리하는 머신러닝파이프라인임
	특히 중요한건,모델의 재현성을 보장하는게 제일 중요함
	즉 이전에 수행된 ml워크플로를 그대로 반복하면,동일한값을 얻을수있어야함
	
	모델 개발과정의 모든 단계가 문서화되고,버전관리가 되어야 이전모델을 동일하게 재현할수있음
	즉 데이터와 모델,코드의 버전관리가 이뤄지고,모든단계의 입력값과 파라미터를 추적하고 기록해야함
	또한 모델학습을 자동으로 트리거해서,새로운데이터로 지속적으로 모델을 업데이트해야함
	즉 모델의 성능이떨어지면 자동으로 재학습과 배포가 이뤄지게하는것(cicd를 머신러닝에 적용한느낌)
	mlops에서는 데이터관리,실험관리,모델저장소,모델모니터링등을 다루게됨
	
	1.데이터관리
	  데이터준비를 할땐여러 중요한 의사결정이 포함되고,각 의사결정마다 다양한 형태의 데이터셋이 생성됨
	  데이터범위를 어케할지,어떤전처리를 쓸지,특성공학으로 뭘 추가할지 등
	  이렇게 데이터가 달라지면 의미가 달라지게됨
	  
	  모델의 결과를 재현하려면 데이터셋도 버전관리를 해야하고,모델이 어떤 데이터셋을썼는지도 기록해야함,이를위해 dvc같은걸 쓸수있음
	
	2.실험관리
	  머신러닝의경우 어떤 모델을 사용할지부터 결정해야함(서포트백터,딥러닝등)
	  또한 모델마다 하이퍼파라미터도 탐색,선택해야하고
	  이런것들을 다 기록해야하는데,도구로 mlflow나 W&B등이 있음
	
	3.모델 저장소
	  이건 머신러닝모델을 체계적으로 관리하고 버전제어하는데 필수적인 요소임
	  여러 버전으로 모델을 만들다보면 여러 모델이 생성되는데
	  이걸 사용하면 다양한 모델을 하나로 통합해서 관리할수있음
	  또한 모델의 전체수명주기를 추적,관리할수있고,변경이록도 기록해주니까,문제가 생기면 빠르게 되돌아갈수있음
	  보통 mlflow 모델저장소나,세이지메이커 모델저장소등을 사용함
	
	4.모델 모니터링
	  이건 모델이 배포된 후에 지속적으로 의도한대로 작동하는지 모니터링하는거임
	  특히 머신러닝특성상,무조건 답은 뱉기때문에 엉뚱한 답을 뱉는게 아닌지를 확인해야함
	  특히 학습한지 오래됐다면,입력데이터의 변동으로 인해 모델의 성능저하가 발생할수있으니,모니터링을 해야함
	  또한 머신러닝시스템의 기본지표등도 기록해 시스템에 문제가없는지등을 지속적으로 확인해야함
	  
	  보통 프로메테우스,그라파나등이 있고,세이지메이커도 있음

  2.llmops는 뭐가다를까
    mlops와 비슷하지만,몇가지 차이점이 있음
	보통 모델을 만들기보단 api로 사용한다는것과,결과물을 정량평가하기 힘들다는것
	
	1.상업용모델과 오픈소스모델 선택하기
	  가장 큰 차이는 모델의 범위임
	  llmOps는 모델의 파라미터수가 훨씬 많고,하나이상의 문제를 해결하기위해 만들어지는경우가 많음
	  이래서 학습이 힘듬
	  그래서 사용하려는 목적과 문제의 난이도에 따라 상업용모델과 오픈소스모델중 적절한 모델을 선택해야함
	  
	  상업용모델은 성능이 높고 api로 사용이 쉽지만,버전관리를 할수가없어서 잘 작동하던 프롬프트가 갑자기 안될수도있고 그래서 모니터링이 필요함
	  또한 요청마다 돈이 나간다는것도 비용관리가 어려워짐
	  또한 파인튜닝이 안되는경우가 많음
	  오픈소스모델은 직접 파라미터를 변경할수있어서 파인튜닝이 자유롭고,기능에 맞춰 추론속도를 올릴수있지만,
	  직접 추론인프라를 관리해야하고 모델을 파인튜닝하기위해 장비들이 필요함
	  
	  또한 모델을 작게 유지하면서 성능을 올리기위해 지식증류나 양자화등을 사용할수있음
	
	2.모델 최적화 방법의 변화
	  llm을 최적화하는 방법은,사전학습,미세조정,프롬프트 엔지니어링,rag 4가지로 나눌수있음
	  보통 llm은 모델이 커서 사전학습은 거의 하지않고,오픈소스모델은 파인튜닝은 할수있지만 상업용모델은 거의할수없음
	  또한 프롬프트를 구조화해 원하는 결과를얻는 프롬프트엔지니어링과,프롬프트에 컨텍스트를 추가하는 rag정도가 있음
	  
	  llmOps에서도 사전학습은 하지않으니 넘어가고,파인튜닝정도를 기본으로 사용함
	  파인튜닝도 풀파인튜닝은 gpu가 많이들어서,일부파라미터만 학습하는 로라등을 주로 사용함
	  프롬프트를 어떻게 구조화하냐에 따라 같은요청도 다른품질의 결과를 생성함
	  
	  특히 llmOps에선 프롬프트에 따라 모델성능이 달라지기때문에,프롬프트도 하이퍼파라미터처럼 실험의 대상이고,재현율을 위해 기록하게됨
	  또한 rag의 경우도 임베딩모델과 벡터db,저장하고 검색할문서를 추가적으로 관리,운영해야하고
	
  3.llm평가하기
    llm은 모델의 성능을 평가하기가 쉽지않음
	1.정량적 지표
	  f1값같이 정량적으로 평가하는 방법으로는
	    번역평가:bleu라는 사람이 번역한것과 유사도체크,n그램사용
		요약이나 번역:rouge,사람이 요약한것과 n그램 유사도체크
		펄플렉시티:모델이 새로운 단어를 생성할떄의 불확실성을 수치화한것
	  등이 있긴한데,llm특성상 별로 도움이되진않음
	  n그램기준인 애들은 순서만 바뀌어도 헤메고,펄플렉시티는 직관적으로 해석하기가 어려움
	  
	2.벤치마크 데이터셋을 활용한 평가
	  이건 서로 다른 모델을 평가할때 사용되는 방법으로,특정 데이터셋을 두고 모델에 넣어서 리턴값을 평가하는거임
	  객관식문제풀기라고 보면됨
	
	3.사람이 직접 평가
	  이건 좋긴한데,비용이 많이들고 시간이 많이걸림
	4.llm을 통한 평가
	  gpt같은 머리좋은애들로 평가를 시키는것
	  이때는 평가점수뿐아니라 평가이유등도 같이 리턴하게해서 평가자 자체를 개선하거나 할수도있음
	  
	5.rag평가
	  rag도 평가가 어려움
	  rag는 관련된 문서를 찾기위한 검색단계와,검색한 내용을 바탕으로 생성하는 단계로 나눌수있음
	  그래서 
	    답변관련성(생성된답변이 요청과 관련성이 있는지)
	    맥락관련성(컨텍스트가 요청과 얼마나 관련이 있는지)
	    신뢰성(생성된응답이 검색된 컨텍스트에 얼마나 사실적으로 부합하는지)
	  이렇게 3개를 기준으로 평가하는데,ragas같은 평가프레임워크도 있긴함
	  평가를 위해서는 질문,생성된 답변,검색된 맥락데이터,실제정답으로 이뤄진 테스트데이터셋이 필요함
	
14.멀티모달llm	
 1.멀티모달llm이란
   멀티모달llm이란 텍스트뿐 아니라 이미지,오디오,비디오등 다양한 형식의 데이터를 이해하고 생성할수있는 llm을 말함
   현재기준으로는 텍스트와 이미지를 다루는게 메인임
   1.멀티모달llm의 구성요소
     멀티모달llm은 모달리티인코더,입력프로젝터를 통해 이미지형식의 데이터를 텍스트로 변환해서 llm에 입력하고,
	 llm의 텍스트출력을 출력프로젝터,모달리티생성기를 통해 특정데이터형식의 출력을 생성함
	 즉 모달리티인코더-입력프로젝터-llm-출력프로젝터-모달리티생성기 이 구조임
	 
	 이때 입력프로젝터와 출력프로젝터만 학습시켜서 사용하고,나머지는 프리즈상태로 두게됨
	 
	 모달리티인코더는 텍스트이외의 데이터형식을 처리하는 모델임
	 이 인코더를 통과한 입력데이터는 특징벡터로 변환되고,특징벡터는 입력프로젝터를 통해 텍스트로 변환됨
	 즉 모달리티 인코더는 다양한 형식의 입력데이터를 텍스트로 변환하기위한 준비단계라고 할수있음
	 
	 이미지로 한정하면,비전 트랜스포머가 모달리티 인코더로 가장 많이 활용됨
	 이건 이미지를 패치단위로 나눠서,텍스트에서 단어처리하는거처럼 트랜스포머돌리는것
	 대신 이건 사전을 구축하긴어려워서 선형변환을 통해 이미지임베딩벡터로 바꿔서 토큰임베딩처럼 만듬
	 이때 clip모델이 많이 사용됨
	 
	 모달리티인코더가 이미지를 임베딩으로 바꿨다면,입력프로젝터는 임베딩을 llm이 이해할수있는 텍스트로 변환함
	 이렇게 모든 입력데이터를 텍스트로 바꿔서 llm에 먹여서 텍스트를 생성하고 반환하는식임
	 
	 또한 llm이 이미지를 생성해서 반환하려면,이미지를 생성하는게 필요한지 판단하는 단계와 필요하다면 어떤 이미지를 생성할지 정하는게 필요함
	 이 두개는 출력프로젝터를 통해 수행되고,생성이 필요하면 이미지생성모델에 적절한 프롬프트를 생성하는게 출력프로젝터의 역할임
	 이렇게 출력프로젝터를 거쳐서 생성된 이미지생성프롬프트는 모달리티생성기에 전달되고,최종적으로 이미지를 생성함
	 
   2.멀티모달llm 학습과정
     얘를 학습시킬땐 사전학습된 텍스트전용 llm이 멀티모달입력과 출력을 지원하기위한 추가훈련을 거쳐야함
	 이것도 llm과 마찬가지로 사전학습과 지시데이터셋을 통한 파인튜닝으로 해결함
	 이떄 이미지-텍스트쌍같은 대규모 멀티모달데이터세트로 학습하게됨
	 
	 사전학습이 끝나면 멀티모달지시튜닝단계를 진행해서,멀티모달llm을 소규모 멀티모달지시데이터셋으로 파인튜닝함
	 여기서 지시란,모델이 이미지캡션을 생성하거나 입력이미지에 대한 질문응답같은 특정 멀티모달작업을 수행하도록 학습시키는걸 말함
	 사전학습으로 전체적인 멀티모달에 대한 이해도를 만들고,지시튜닝으로 특정 사용사례에 맞춰 성능이 올라가게 학습시키는거
	 
	 여기서 입력프로젝터와 출력프로젝터만 학습한다는걸 잊지말아야함

  2.이미지와 텍스트를 연결하는 모델:CLIP
    1.clip란
	  이건 텍스트와 이미지의 관계를 계산할수있게 텍스트모델과 이미지모델을 함께 학습시킨모델임
	2.clip모델의 학습방법
	  얘의 학습데이터는 서로 관련이 있는 이미지텍스트쌍을 사용함
	  이건 이미지와 이미지에 대한 설명이 대응된 데이터를 말함
	  이걸가지고 대조학습을 통해 모델을 학습시키는데,대조학습은 유사한 데이터쌍은 더 가깝게,유사하지않은 데이터쌍은 더 멀게 학습시키는 방법임
	3.clip모델의 활용과 성능
	  학습을 마친 clip모델은 제로샷추론을 할수있음
	  즉 사전학습데이터외에 특정작업을 위한 데이터로 미세조정하지않아도 추론을 할수있음
	4.clip모델직접 활용하기
	  허깅페이스 라이브러리로 불러다 쓸수있음

  3.텍스트로 이미지를 생성하는모델 dall-e
    1.디퓨전모델 원리
	  기본적으로 디퓨전모델,즉 노이즈에서 노이즈를 제거해 이미지를 만들어내는 방식을 요즘은 주로 사용함
	  즉 인코더디코더모델을 주로 사용함
	  이떄 사람이 뭘 할수있게하기위해,텍스트임베딩을 노이즈랑 섞어서 이미지를 뽑아내게 학습을 시킴
	2.dall-e모델
	  여기서 llm이 텍스트를 만들어준다면,이걸 사용해서 이미지를 생성할수있음(텍스트생성-임베딩변환-프라이어로 이미지임베딩변환-디코더로 생성)
	  여기서 프라이어는 텍스트임베딩을 입력으로 받아 이미지임베딩을 예측하는 디퓨전모델임
  	  즉 노이즈와 텍스트임베딩을 받아서 이미지 임베딩을 만드는 모델임
	  
	  이 프라이어를 통해 생성된 이미지임베딩과,노이즈를 합쳐서 최종결과물을 뽑아내는것
	  이 프라이어는 중간에 넣어주는게 효율이 좋음(좀 더 텍스트를 잘 이해함)

  4.LLaVA
    이건 이미지를 인식하는 CLIP모델과 LLM을 결합해 모델이 이미지를 인식하고 이미지에 대한 텍스트를 생성할수있는 모델임
	1.LLaVA의 학습데이터
	  멀티모달대화모델의 가장 어려운점은,데이터셋이 부족하다는것
	  그래서 보통 gpt등으로 데이터를 생성해서 사용함
	2.LLaVA구조
	  자세한건 책보자
   
15.llm 에이전트	
  에이전트는 알아서 생각하고 행동하는 시스템임
  에이전트는 llm과 rag를 하나의 구성요소로 사용함
  1.에이전트란
    1.에이전트의 구성요소
	  에이전트가 주변환경을 감각으로 인식하고,의사결정을 내려 행동하게 만들기위해서는 크게 3가지가 필요함
	  감각,두뇌,행동에서 각각 하나씩
	2.에이전트의 두뇌
	  두뇌는 요즘은 llm을 기반으로 많이 사용함
	  
	  두뇌는 감각을 통해 현재상황과 사용자의 요청을 인식한걸 바탕으로,요청이 무엇인지,현재 어떤상황인지를 이해하고 목표달성을 위해 어떤행동을 취할지를 결정함
	  그과정에서 지금까지 수행했던 사용자와의 대화나 행동을 저장한 기억을 확인하고,상황을 이해하는데 필요한 지식이 있다면 검색해서 활용함
	  이렇게 보강된 정보를 바탕으로 에이전트는 목표달성을 위한 작업을 세분화하는 플래닝단계를 거쳐서 어떤행동이 필요할지 결정해 행동단계로 넘어감
	  
	  llm은 여기서 필요한 모든 능력을 갖추고있음
	  먼저 사용자의 요청이나 상황정보를 이해할수있고,rag를 사용해 필요한지식을 검색하고 활용할수있음
	  추론능력을 갖추고있고 컨텍스트학습같이 새로운 작업에 적절히 대응하는 일반화능력도 있음
	  또한 유용한지식을 저장할때 필요한 요약도 잘하고,행동에 필요한 도구에 대한 적절한 설명이 주어진다면,적절한 행동을 선택할수도있음
	
	3.에이전트의 감각
	  llm은 기본적으로 텍스트를 받아서 텍스트를 내보냄
	  또한 이미지같은 멀티모달도 받을수있음(위스퍼로 음성을 텍스트로 바꿀수도있고)
	  결국 텍스트로 바꿔서 입력되기만 하면됨
	  동영상은 비디오인코더와 오디오인코더의 출력을 합쳐서 텍스트로 뽑아내는식으로
	
	4.에이전트의 행동
	  행동은 결국 텍스트만 뽑아낼수있으니,llm이 특정 출력을(보통 함수형태로)내면 그걸 인식해서 출력을 실행시키는 도구를 제공해야함
	  이때 llm이 이해할수있게 설명을 제공하면 상황에맞춰 도구를 선택할수있음

  2.에이전트 시스템의 형태
    1.단일에이전트
	  이건 입력받은 프롬프트를 통해 모든 결정을 내림
	  프롬프트엔 목표,제약사항,도구등이 들어가고,이걸 통해 다음에 수행할 도구를 선택함
	  
	  얘는 모든과정을 스스로 처리해서 편리하지만,작업을 수행하는과정에서 길을 잃기 쉬움
	  또한 llm의 성능에 큰 영향을 받아서,성능이 낮으면 빈번하게 실패하고 잘 작동하지않음
	
	2.사용자와 에이전트의 상호작용
	  그래서 문제가 생기거나,현재 결과물에 대해 사람에게 평가를 맡기고,거기에 대해 다시 작업하는식으로 동작하게할수도있음
	
	3.멀티에이전트
	  이건 각 에이전트마다 서로 다른 프로필을 주고 작업을 수행함
	  llm은 큰덩어리의 작업보단 구체적이고 세분화된 작업을 잘 수행하고,역할이 부여되면 더 성능이 높아지니까,에이전트를 여러개 만들어서 작업을 수행하는식임
	  이떈 수평적으로도 할수있고,수직적으로도 만들수있음

  3.에이전트 평가하기
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	  
	
	
	
	
	
	
	
    
  