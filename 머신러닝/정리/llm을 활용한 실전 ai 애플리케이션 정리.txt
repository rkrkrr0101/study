1.llm지도
  스킵
2.트랜스포머
  1.트랜스포머 기본 구조(적당히 아는거제외하고 요약함)
	트랜스포머의 핵심은 어텐션이고,트랜스포머의 주요 구조는 인코더와 디코더

	당연히 텍스트는 임베딩되어야하고,임베딩하는층도 모델에 포함되어있고,이것도 같이 학습됨
	또한 어텐션이니까 문장 전체에서 가중치를 가져오는건 맞지만,그래도 근처에있으면 영향을 줄 확률이 높으니 위치정보를 더해주는 층이 있음

	쿼리는 입력하는 검색어
	키는 쿼리와 관련성을 찾기위해 문서가 가진 특징
	값은 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬해서 문서를 제공할떄 문서

	즉 우리가 검색하면서 원하는건 값임
	어텐션이 동작할때,특정 단어가 쿼리로 주어졌을떄,해당 문장내에서의 관계도(키-값)에서 키는 해당 단어고,값은 임베딩값임
	이 임베딩값을 각각 위치에따라 0.x를 몇을 곱해주냐(관련도)가 어텐션의 핵심임

	그런데 동음이의어나,나는,다녀왔다 이런 단어들은 토큰 자체로만 보면 관련성을 찾기 힘듬
	그래서 트랜스포머에서는 토큰임베딩을 변환하는 가중치Wq와 Wk,Wv를 도입함
	각각 Wq는해당 쿼리단어에 대해 학습하는거고,
	Wk는 키에대해 학습하는거임
	Wv는 값을 출력할때 토큰임베딩을 텍스트로 변환할떄의 가중치임

	또한 이 어텐션을 하나만 쓰는게 아닌,여러 어텐션을 동시에 사용하고,그걸 합산한게 효과가 더 좋음(파라미터가 늘어나는효과)
  2.인코더와 디코더
    인코더는 자연어 이해를 할떄 사용하고,디코더는 자연어 생성을 할때 사용함
	두개를 같이 쓰는건 더 넓은 범위의 작업을 할때 사용함
	
	gpt같은건 디코더만 사용한모델임
	자연어를 이해하는거만 필요하다면 인코더만 사용하는 BERT같은걸 사용할수있음(리뷰이해한후 ox표시같은거)

  3.주요 사전학습 메커니즘
    인과적 언어 모델링은 문장의 시작부터 끝까지 순차예측하는 방식임,보통 gpt같은데서 사용하는 학습방법
	마스크 언어 모델링은 단어 하나를 감추고 해당 단어를 예측하는 방식임,BERT같은 인코더쪽에서 주로 사용(양방향예측이 필요할때)
	
3.허깅페이스 트랜스포머 라이브러리	
  1.허깅페이스 라이브러리
	허깅페이스 트랜스포머 라이브러리는,다양한 트랜스포머 모델을 같은 인터페이스로 사용할수있게 지원하는 오픈소스 라이브러리임
	이건 크게 트랜스포머모델과 토크나이저를 활용할떄 쓰는 transformers라이브러리와,데이터셋을 가져다쓸수있는 datasets라이브러리로 구성됨

	쓰는것도 그냥 프리트레인모델가중치를 불러오고,토크나이저를 불러오고,입력을 토큰화한후 모델에 넣으면됨(책참고)
	거의 모든 모델을 거의 같은 코드로 사용할수있음
	도커쓰는거랑 거의 비슷한느낌임
  2.허깅페이스 허브 탐색
    이건 그냥 허브사이트 들어가서 모델보는법
	모델과 데이터셋 허브가 따로있음

  3.라이브러리 사용법 익히기
    모델은 바디와 헤드로 구성됨
	바디는 은닉층이고,헤드는 최종출력층임
	즉 기본적으로 제공되지않는 헤드를 가져가 끼우려면,추가적으로 파인튜닝을 시켜야함
	또한 각 모델에는 config.json이 주어지는데,이걸 열어보면 어떤모델인지,뭘하는모델인지를 알수있음
	
	토크나이저는 텍스트를 토큰단위로 나누고,각토큰을 대응하는 토큰아이디로 변환함
	이것도 학습하면서 어휘사전을 구축하기때문에,보통 모델과 함께 저장함(그래서 같은 모델과 토크나이저를 쓰려면 같은 모델아이디로 불러오면됨)
	
	데이터셋을 사용하면 데이터셋을 가져올수있음

  4.모델 학습시키기
    허깅페이스에는 모델학습을 쉽게 할수있게 트레이너api를 제공함
	그냥 
	  데이터를 준비하고(학습,평가)
	  토크나이저를 불러오고
	  모델을 불러오고
	  학습파라미터를 넣어주고
	  트레이너를 돌리면 끝임
	즉 예전에 텐서플로로 하던방식에서,레이어를 까는작업을 그냥 불러오는거로 대체하고,학습파라미터랑 데이터셋,토크나이저를 알아서 붙여넣고 돌리면 되는식으로 바꾼것
  
  5.모델 추론하기
	이것도 그냥 파이프라인이라고 쉽게 추론할수있게 해줌
	이건
	  작업종류
	  모델
	  설정
	을 받아서 모델파이프라인을 만들고,거기에 데이터를 넣으면 값을 리턴해줌
	
	
4.말잘듣는 모델 만들기
  1.사전학습과 지도미세조정
    사전학습은 모델의 기본 체급을 만드는 과정임
	인터넷상의 다양한 텍스트데이터로 사전학습을 보통 하는데,보통 다음에 올 단어의 확률을 예측하는식으로 학습함
	
	또한 사용자의 요청에 적절히 응답하려면,요청의 형식을 적절히 해석하고,응답의 형태를 적절히 작성하고,둘이 잘 연결되어야함
	이렇게 만드는걸 지도미세조정이라고 함
	이름에 지도가 들어간거부터 알겠지만,학습데이터에 정답이 포함된 데이터로 학습하는거임
	그래서 이걸통해 llm은 사용자의 요청에 맞춰 응답하도록 학습하는데,이를 정렬이라고 함
	이때 사용되는 데이터셋은 지시데이터셋이라고 부름(지시에 맞춰 응답한 데이터셋)
	
	이런 지시데이터셋에 비해 사전데이터셋은 형식이 매우 다양하고,요청에 응답하는 형식의 데이터는 적고,특히 양질의 데이터는 훨씬 적음
	그래서 이걸 보완하기위해 지시데이터셋을 사용하는것
	
	지시데이터셋은 사용자의 요구사항을 표현한 문장임
	입력에는 답변을 하는데 필요한 데이터가 들어가고,
	출력에는 지시사항과 입력을 바탕으로한 정답응답이 들어감
	
	즉 지시데이터셋이나 사전데이터셋이나 학습과정은 똑같은데(둘다 다음단어를 예측하는 인과적 언어 모델링 사용),
	단순히 학습하는 데이터셋에 차이가 있어서 그런것
	
	지시데이터셋의 크기는 대충 1000개정도면 충분함
	크기보단 지시사항이 다양한 형태로 되어있고,응답데이터의 품질이 높은게 더 중요함
	결국 모델의 지식이나 능력은 사전학습에서 대부분 끝났고,이걸 잘 끌어내는게 지도미세조정이기때문(즉 기초모델을 잘고르면 작은 지시데이터셋써도됨)
	또한 지시데이터셋의 품질이 좋으면 성능이 많이 올라갈수있음
	즉 이미 공개된 데이터셋을 한번 더 필터링하는게 더 나을수있다는것

  2.채점모델
    a와 b중 더 좋은 데이터를 표시한 데이터셋을 선호데이터셋이라고 부름
	각 데이터에 점수를 매긴거랑 비슷한데,이런 데이터는 찾기 힘들어서 단순히 양쪽을 비교만 하는식으로 만들어진 데이터셋
	
	선호데이터셋을 통해 답변의 점수를 매기는 리워드모델을 만들고,llm의 응답을 여러개 만들게해서 리워드모델로 피드백을 먹이는식으로 강화학습을 할수있음
	이게 사람의 피드백을 활용한 강화학습(RLHF)임
	
	단 이런방식은 단순히 점수를 높게받는데만 집중하는 보상해킹이 생기기쉬워서,이게 안생기게 면밀히 만들어야함
	그때 사용하는게 ppo로,현재 응답에서 조금만 수정해서 점수를 높일수있게 하는방식
	
	근데 rlhf는 너무너무 사용하기 어려워서,보통 기피됨
	리워드모델이 성능이 좋지않아서 일관성없는 점수를 뱉으면,학습이 제대로되지않고,
	참고모델,학습모델,리워드모델 3개의 모델이 필요해지고,
	강화학습자체가 하이퍼파라미터에 민감하고 학습이 불안정해서 쓰기가 힘듬
	
  3.강화학습이 꼭 필요할까?
    그래서 사용되는게 기각샘플링과 직접선호최적화임
	
	기각샘플링은 지도미세조정을 마친 llm을 통해 여러 응답을 생성하고,그중 리워드모델이 가장 높은 점수를 준 응답을 모아서 다시 지도미세조정을 하는것
	즉 자기지도학습을 하는거임
	이러면 강화학습을 쓰지않아서 안정적이게됨
	즉 리워드모델과 ppo는 그대로 사용하고,강화학습으로 점수매기는방식이 아닌,
	학습할 llm의 응답을 받아서 그거중 좋은거만 모아서 응답을 다시먹이는식으로 하는것(뱉은거중 좋은거만 다시먹는거임)
	
	직접선호최적화(dpo)는 선호데이터셋을 직접 언어모델에 학습시켜서,리워드모델과 강화학습을 사용하지않는방식임
	즉 llm이 리워드모델의 역할도 같이(은닉층에서) 처리하는것임
	기본적으로 llm은 해당 문제를 처리할 기초체급을 갖추고있고,여기서 현재 처리하려는 문제에 대해 방향을 제시해주는게 지도미세조정임
	즉,해당 방향으로 적당한 학습률을 가지고 선호데이터셋을 먹여버리면,그쪽으로 사고를 유도할수있다 이런방식임
	
	요즘은 dpo를 가장 많이 사용함
	
	
5.gpu 효율적인 학습	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
    
  