1.llm지도
  스킵
2.트랜스포머
  1.트랜스포머 기본 구조(적당히 아는거제외하고 요약함)
	트랜스포머의 핵심은 어텐션이고,트랜스포머의 주요 구조는 인코더와 디코더

	당연히 텍스트는 임베딩되어야하고,임베딩하는층도 모델에 포함되어있고,이것도 같이 학습됨
	또한 어텐션이니까 문장 전체에서 가중치를 가져오는건 맞지만,그래도 근처에있으면 영향을 줄 확률이 높으니 위치정보를 더해주는 층이 있음

	쿼리는 입력하는 검색어
	키는 쿼리와 관련성을 찾기위해 문서가 가진 특징
	값은 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬해서 문서를 제공할떄 문서

	즉 우리가 검색하면서 원하는건 값임
	어텐션이 동작할때,특정 단어가 쿼리로 주어졌을떄,해당 문장내에서의 관계도(키-값)에서 키는 해당 단어고,값은 임베딩값임
	이 임베딩값을 각각 위치에따라 0.x를 몇을 곱해주냐(관련도)가 어텐션의 핵심임

	그런데 동음이의어나,나는,다녀왔다 이런 단어들은 토큰 자체로만 보면 관련성을 찾기 힘듬
	그래서 트랜스포머에서는 토큰임베딩을 변환하는 가중치Wq와 Wk,Wv를 도입함
	각각 Wq는해당 쿼리단어에 대해 학습하는거고,
	Wk는 키에대해 학습하는거임
	Wv는 값을 출력할때 토큰임베딩을 텍스트로 변환할떄의 가중치임

	또한 이 어텐션을 하나만 쓰는게 아닌,여러 어텐션을 동시에 사용하고,그걸 합산한게 효과가 더 좋음(파라미터가 늘어나는효과)
  2.인코더와 디코더
    인코더는 자연어 이해를 할떄 사용하고,디코더는 자연어 생성을 할때 사용함
	두개를 같이 쓰는건 더 넓은 범위의 작업을 할때 사용함
	
	gpt같은건 디코더만 사용한모델임
	자연어를 이해하는거만 필요하다면 인코더만 사용하는 BERT같은걸 사용할수있음(리뷰이해한후 ox표시같은거)

  3.주요 사전학습 메커니즘
    인과적 언어 모델링은 문장의 시작부터 끝까지 순차예측하는 방식임,보통 gpt같은데서 사용하는 학습방법
	마스크 언어 모델링은 단어 하나를 감추고 해당 단어를 예측하는 방식임,BERT같은 인코더쪽에서 주로 사용(양방향예측이 필요할때)
	
3.허깅페이스 트랜스포머 라이브러리	
  1.허깅페이스 라이브러리
	허깅페이스 트랜스포머 라이브러리는,다양한 트랜스포머 모델을 같은 인터페이스로 사용할수있게 지원하는 오픈소스 라이브러리임
	이건 크게 트랜스포머모델과 토크나이저를 활용할떄 쓰는 transformers라이브러리와,데이터셋을 가져다쓸수있는 datasets라이브러리로 구성됨

	쓰는것도 그냥 프리트레인모델가중치를 불러오고,토크나이저를 불러오고,입력을 토큰화한후 모델에 넣으면됨(책참고)
	거의 모든 모델을 거의 같은 코드로 사용할수있음
	도커쓰는거랑 거의 비슷한느낌임
  2.허깅페이스 허브 탐색
    이건 그냥 허브사이트 들어가서 모델보는법
	모델과 데이터셋 허브가 따로있음

  3.라이브러리 사용법 익히기
    모델은 바디와 헤드로 구성됨
	바디는 은닉층이고,헤드는 최종출력층임
	즉 기본적으로 제공되지않는 헤드를 가져가 끼우려면,추가적으로 파인튜닝을 시켜야함
	또한 각 모델에는 config.json이 주어지는데,이걸 열어보면 어떤모델인지,뭘하는모델인지를 알수있음
	
	토크나이저는 텍스트를 토큰단위로 나누고,각토큰을 대응하는 토큰아이디로 변환함
	이것도 학습하면서 어휘사전을 구축하기때문에,보통 모델과 함께 저장함(그래서 같은 모델과 토크나이저를 쓰려면 같은 모델아이디로 불러오면됨)
	
	데이터셋을 사용하면 데이터셋을 가져올수있음

  4.모델 학습시키기
    허깅페이스에는 모델학습을 쉽게 할수있게 트레이너api를 제공함
	그냥 
	  데이터를 준비하고(학습,평가)
	  토크나이저를 불러오고
	  모델을 불러오고
	  학습파라미터를 넣어주고
	  트레이너를 돌리면 끝임
	즉 예전에 텐서플로로 하던방식에서,레이어를 까는작업을 그냥 불러오는거로 대체하고,학습파라미터랑 데이터셋,토크나이저를 알아서 붙여넣고 돌리면 되는식으로 바꾼것
  
  5.모델 추론하기
	이것도 그냥 파이프라인이라고 쉽게 추론할수있게 해줌
	이건
	  작업종류
	  모델
	  설정
	을 받아서 모델파이프라인을 만들고,거기에 데이터를 넣으면 값을 리턴해줌
	
	
4.말잘듣는 모델 만들기
  1.사전학습과 지도미세조정
    사전학습은 모델의 기본 체급을 만드는 과정임
	인터넷상의 다양한 텍스트데이터로 사전학습을 보통 하는데,보통 다음에 올 단어의 확률을 예측하는식으로 학습함
	
	또한 사용자의 요청에 적절히 응답하려면,요청의 형식을 적절히 해석하고,응답의 형태를 적절히 작성하고,둘이 잘 연결되어야함
	이렇게 만드는걸 지도미세조정이라고 함
	이름에 지도가 들어간거부터 알겠지만,학습데이터에 정답이 포함된 데이터로 학습하는거임
	그래서 이걸통해 llm은 사용자의 요청에 맞춰 응답하도록 학습하는데,이를 정렬이라고 함
	이때 사용되는 데이터셋은 지시데이터셋이라고 부름(지시에 맞춰 응답한 데이터셋)
	
	이런 지시데이터셋에 비해 사전데이터셋은 형식이 매우 다양하고,요청에 응답하는 형식의 데이터는 적고,특히 양질의 데이터는 훨씬 적음
	그래서 이걸 보완하기위해 지시데이터셋을 사용하는것
	
	지시데이터셋은 사용자의 요구사항을 표현한 문장임
	입력에는 답변을 하는데 필요한 데이터가 들어가고,
	출력에는 지시사항과 입력을 바탕으로한 정답응답이 들어감
	
	즉 지시데이터셋이나 사전데이터셋이나 학습과정은 똑같은데(둘다 다음단어를 예측하는 인과적 언어 모델링 사용),
	단순히 학습하는 데이터셋에 차이가 있어서 그런것
	
	지시데이터셋의 크기는 대충 1000개정도면 충분함
	크기보단 지시사항이 다양한 형태로 되어있고,응답데이터의 품질이 높은게 더 중요함
	결국 모델의 지식이나 능력은 사전학습에서 대부분 끝났고,이걸 잘 끌어내는게 지도미세조정이기때문(즉 기초모델을 잘고르면 작은 지시데이터셋써도됨)
	또한 지시데이터셋의 품질이 좋으면 성능이 많이 올라갈수있음
	즉 이미 공개된 데이터셋을 한번 더 필터링하는게 더 나을수있다는것

  2.채점모델
    a와 b중 더 좋은 데이터를 표시한 데이터셋을 선호데이터셋이라고 부름
	각 데이터에 점수를 매긴거랑 비슷한데,이런 데이터는 찾기 힘들어서 단순히 양쪽을 비교만 하는식으로 만들어진 데이터셋
	
	선호데이터셋을 통해 답변의 점수를 매기는 리워드모델을 만들고,llm의 응답을 여러개 만들게해서 리워드모델로 피드백을 먹이는식으로 강화학습을 할수있음
	이게 사람의 피드백을 활용한 강화학습(RLHF)임
	
	단 이런방식은 단순히 점수를 높게받는데만 집중하는 보상해킹이 생기기쉬워서,이게 안생기게 면밀히 만들어야함
	그때 사용하는게 ppo로,현재 응답에서 조금만 수정해서 점수를 높일수있게 하는방식
	
	근데 rlhf는 너무너무 사용하기 어려워서,보통 기피됨
	리워드모델이 성능이 좋지않아서 일관성없는 점수를 뱉으면,학습이 제대로되지않고,
	참고모델,학습모델,리워드모델 3개의 모델이 필요해지고,
	강화학습자체가 하이퍼파라미터에 민감하고 학습이 불안정해서 쓰기가 힘듬
	
  3.강화학습이 꼭 필요할까?
    그래서 사용되는게 기각샘플링과 직접선호최적화임
	
	기각샘플링은 지도미세조정을 마친 llm을 통해 여러 응답을 생성하고,그중 리워드모델이 가장 높은 점수를 준 응답을 모아서 다시 지도미세조정을 하는것
	즉 자기지도학습을 하는거임
	이러면 강화학습을 쓰지않아서 안정적이게됨
	즉 리워드모델과 ppo는 그대로 사용하고,강화학습으로 점수매기는방식이 아닌,
	학습할 llm의 응답을 받아서 그거중 좋은거만 모아서 응답을 다시먹이는식으로 하는것(뱉은거중 좋은거만 다시먹는거임)
	
	직접선호최적화(dpo)는 선호데이터셋을 직접 언어모델에 학습시켜서,리워드모델과 강화학습을 사용하지않는방식임
	즉 llm이 리워드모델의 역할도 같이(은닉층에서) 처리하는것임
	기본적으로 llm은 해당 문제를 처리할 기초체급을 갖추고있고,여기서 현재 처리하려는 문제에 대해 방향을 제시해주는게 지도미세조정임
	즉,해당 방향으로 적당한 학습률을 가지고 선호데이터셋을 먹여버리면,그쪽으로 사고를 유도할수있다 이런방식임
	
	요즘은 dpo를 가장 많이 사용함
	
	
5.gpu 효율적인 학습	
  1.gpu에 올라가는 데이터
    1.딥러닝 모델 데이터타입
      딥러닝을 돌릴때 가장 자주 만나는 에러는 oom임
	  기본적으로 gpu메모리에는 모델 자체가 올라가고,모델은 행렬곱을 위한 파라미터의 집합임
	  
	  일반적으론 한 파라미터는 16비트로 수를 표현함(부호1비트 지수8비트 가수7비트,bf16)
	  딥러닝모델은 학습이나 추론이나 gpu메모리에 전부 다 올라가기때문에,모델의 용량이 얼마인가가 매우 중요함
	  딥러닝모델의 용량은 파라미터수에 파라미터당 비트를 곱하면됨
	  즉 14b모델이 16비트(2바이트)로 저장된다면
	    14b=14*2=28GB
	  가 필요한것
	2.양자화
	  이렇게 14b만 해도 엄청나게 메모리를 잡아먹기때문에,파라미터수는 줄이기 싫은데 메모리사용량은 줄이고 싶어서 나온게 양자화임
	  이건 더 적은 비트로 모델을 표현하는것
	  양자화를 하면 모델의 용량이 줄어서 메모리 사용량이 줄어들지만,대신 정보가 소실되기때문에 모델의 성능이 저하됨
	  즉 양자화의 핵심은 더 적은 비트를 사용하면서 원본데이터의 정보를 최대한 소실없이 유지하는게 핵심과제임
	  
	  이때 가장 중요한건,변환하려는 데이터 형식의 수를 최대한 낭비하지않고 사용하는거임
	  즉 사용되지않는 비트공간을 최대한 없애고,주어진 범위 내에서 수들을 최대한 퍼트리는방법을 찾는게 핵심
	  
	  그래서 사용되는게,데이터들을 k개의 데이터를 묶은 블록으로 만들고,이 블록단위로 절대최댓값을 구해서 변환을 하는거임
	  이러면 이상치들을 최대한 억제하면서 데이터의 형식을 낭비하지않고 양자화를 할수있어짐
	3.gpu 메모리 분해
	  gpu 메모리엔
	    모델 파라미터
		그레이디언트
		옵티마이저 상태
		순전파 상태
	  이렇게 4가지가 올라가고,
	  딥러닝 학습은 간단히 말하면 순전파를 수행하고,그때 계산한 손실로부터 역전파를 수행하고,옵티마이저로 모델을 업데이트함
	  이떄 역전파를 수행하려면 순전파를 했을때의 값이 필요함(순전파상태값),또한 그레이디언트는 역전파 결과 생성됨
	  
	  이떄 최소로 필요한 메모리양은,2바이트x파라미터수(B)를 N으로 잡았을때,
	    모델파라미터=N
		그레이디언트=N
		옵티마이저 상태=2N(상태가 2개라서)
	  즉 4N이 기본적으로 필요하고,순전파상태를 저장하기 위한 메모리가 추가로 필요함
	  즉 모델을 사용하는것보다,모델을 학습시키려면 최소 4배가 필요하다는거,추가적으로 1배치사이즈의 데이터도 메모리에 올려야하고
	  
	  허깅페이스의 모델메모리 계산기를 사용해서 대략적인 메모리양을 알수있음

  2.단일 gpu 효율적으로 활용하기
    1.그레이디언트 누적
	  학습과정에서 배치크기를 크게 가져가면,더 빠르게 수렴하고 성능이 올라가지만,배치크기가 커지면 순전파상태저장에 필요한 메모리가 증가함
	  그래서 사용되는게 그레이디언트 누적임
	  이건 제한된 메모리 안에서 배치크기를 키우는것과 같은 효과를 얻는방법임
	  
	  이건 스탭을 n번 반복하고,n번의 스탭이 지나가고 나서 모든걸 합친다음 n으로 나눠서 역전파를 수행하고,모델을 업데이트함
	  이러면 배치사이즈가 n배 커진 효과를 받을수있음
	  대신 시간은 길어지겠지만
	2.그레이디언트 체크포인팅
	  역전파 계산을 하려면 순전파의 결과를 저장하고있어야함
	  가장 기본적인건 모두 저장하는거지만,이건 비효율적이고
	  가장 메모리를 많이 절약하는건 역전파에 꼭 필요한거만 저장하고,
	  나머지는 다시 계산하는방식인데,이건 한번의 역전파를 위해 순전파를 반복적으로 계산해야한다는 단점이 있음
	  
	  이 두가지를 절충하는 방법이 그레이디언트 체크포인팅임
	  중간중간에 체크포인트를 두고,거기서부터만 다시 계산하는식으로 동작시키는거임
	  이러면 메모리 사용량도 좀 줄고,그렇게 많이 계산할필요도 없음(즉 위에 두개를 반씩 합친느낌)
	  이건 쓰려면 걍 메서드에서 키기만 하면됨

  3.분산학습과 ZeRO
    1.분산학습
	  분산학습은 GPU를 여러개 활용해서 모델을 학습시키는걸 말함
	  이거의 목적은 크게 2가지로,모델 학습속도를 올리거나,1개의 gpu로 학습이 어려운 모델을 다루는것
	  
	  학습속도를 올리는건,각각의 gpu에 각각 모델을 올리고,학습데이터를 병렬로 처리해서 학습속도를 올리는것(데이터 병렬화)
	  여러 gpu에 모델을 나눠서 올리는건,모델병렬화라고 부르는데,이건 모델의 층별로 나누는 파이프라인병렬화와,한층의 모델도 나누는 텐서병렬화가 있음
	  즉 한모델을 세로로 자르냐 가로세로로 둘다 자르냐 차이임
	  
	  파이프라인병렬화는 진짜 직관적으로 이해가 되지만,텐서 병렬화는 행렬을 분리해도 같은 결과를 얻을수있게 행렬곱셈을 정렬함(자동으로 해줌)
	  트랜스포머에서는,셀프어텐션에서 쿼리,키,값 가중치행렬을 열방향으로 나눠서 행렬곱을 수행하는식
	2.데이터 병렬화에서 중복 저장 줄이기(ZeRO)
	  데이터 병렬화를 할때도 같은 모델을 여러 gpu에 올리는건 메모리낭비임
	  ZeRO는,하나의 모델을 하나의 gpu에 올리지않고,
	  모델병렬화처럼 모델을 나눠 여러 gpu에 올리고 각 gpu에서는 자신의 모델 부분만 연산을 수행하면 된다는 컨셉임
	  즉 4개로 나눠서 1의 속도로 처리하지말고,모델병렬화를 하고 4배의 속도로 처리하는게 속도도 비슷한데 메모리효율적이라는 논리

  4.효율적인 학습방법(PEET):LoRA
    기반모델의 크기가 커지면,하나의 gpu로 모든 파라미터를 학습하는 전체미세조정은 하기 어려움
	그래서 개인이 모델의 일부 파라미터만 학습시키는게 PEET임
	그중에서 가장 주목받는게 로라임
	
	로라는 모델파라미터를 재구성해 더 적은 파라미터를 학습해서 gpu메모리사용량을 줄임
	파라미터 재구성이란 행렬을 더 작은 2개의 행렬의 곱으로 표현해,전체 파라미터를 수정하는게 아닌 더 작은 2개의 행렬을 수정하는걸 의미함
	
	이건 원래 모델의 가중치는 그대로 두고,그 모델의 가로 세로 가중치(로라가중치) 한줄만 새로 만들어서,
	원래모델의 가중치와 로라가중치의 가로세로를 곱한걸 더하는방식임
	이러면 로라가중치에 대해서만 학습하면 되기떄문에 gpu사용량이 줄어듬
	이떄 어떤 파라미터위치를 선택할지는 정해야하고,보통 선형연산의 가중치를 선택함(트랜스포머에선 qkv와 피드포워드층)
	전체 선형층에 로라를 적용한게 일반적으론 성능이 가장 좋음

    1.로라 설정 살펴보기
	  로라를 사용할떄 결정해야할 사항은
	    모델파라미터에 더할 행렬 A,B를 만들떄 차원 r을 몇으로 할지
		  r이 작으면 로라의 성능이 낮아지고,높으면 gpu메모리를 많이먹음
		추가파라미터를 기존파라미터에 얼마나 많이 반영할지(알파)
		  즉 기존파라미터와 덧셈할때,가중치를 붙일수있음
		모델의 많은 파라미터중에서 어떤 파라미터를 재구성할지
	  임
	2.코드로 로라 학습 사용하기
	  쓰는건 쉬움
	  걍 허깅페이스의 peft라이브러리로 하면됨(책참조)
	  
	  로라를 쓰면 모델의 추론 메모리 사용량에 0.117%정도만 추가로 더 사용하기때문에 학습하기 쉬움
	  그러면서도 전체 미세조정과 거의 동일한 성능을 달성할수있음

  5.효율적인 학습방법(PEET):QLoRa
    Q로라는 양자화를 더한 로라임
	이건 추가적으로 페이지 옵티마이저로,cpu메모리를 가상메모리처럼 사용해서 oom을 방지하는 기능이 추가됨
	1.4비트 양자화와 2차 양자화
	  양자화를 하려는데,데이터가 순서를 사용하면 정렬해야하고,
	  어떤데이터가 몇번째순위인지를 저장해야하기때문에 연산량이 많고 순위를 저장하기위해 메모리를 사용한다는 단점이 있음
	  
	  만약 기존데이터의 분포를 알고있다면,연산없이 그냥 분포대로 순위를 정할수있음(정규분포를 따른다면 대충 위치아니까)
	  그안에서 선그어서 데이터를 균등하게 분리하는방식
	  
	  보통 모델파라미터는 거의 정규분포에 가깝기때문에,이런 방식을 사용해서 모델의 성능을 거의 유지하면서 빠른 양자화를 할수있음(NF4)
	  이게 4비트양자화이고,여기서 한발 더 나아간게 2차양자화임
	  2차양자화는 NF4과정에서 생기는 32비트 상수도 효율적으로 저장하는걸 목적으로함
	  이건 64개의 모델 파라미터마다 1개의 상수(정규분포)를 저장하는것,이때 다시 256개의 양자화 상수를 하나의 블록으로 묶어 8비트 양자화를 수행할수있음
	  이러면 32비트 256개를 8비트 256개와 양자화상수를 저장하기위한 32비트 1개로 바꿀수있음
	2.페이지 옵티마이저
	  페이지 옵티마이저는,일시적으로 메모리 사용량이 커지는 지점들에서 oom이 터지는걸 방지하기위해,페이징을 거쳐서 cpu메모리를 가상메모리로 사용하는것보다
	
	3.Q로라 활용하기
	  사용은 쉬움,그냥 bitsandbytes를 사용하면 모델을 불러올때 4비트양자화를 간단히 할수있음
	  
	
6.sLLM 학습하기	
  sLLM은 비용효율적인 특정작업이나 도메인에 특화된 작은 llm임
  1.데이터셋
    기존에 구축된 대표성있는 데이터셋이 있으면 그걸 사용하면되지만,없다면 직접 만들어야함
	이때 보통 gpt4를 사용해서 만들면 좋음
	
	만들어진 데이터셋을 검수하면 좋긴하겠지만,시간비용상 힘드니까 규칙기반으로 간단하게 데이터정제만 해두고 쓰면됨

  2.성능평가 파이프라인 준비하기
    평가에서도 가장 쓰기 편한건 gpt4임
	llm모델을 개발할떄 제일 어려운게 성능평가임
	그래서 sql을 만드는 sLLM의 경우엔,생성한 sql과 정답 sql을 비교해서 정답여부를 gpt4로 판단받는형식으로 사용할수있음
	
	이떄 필요한건 평가데이터셋과,프롬프트와,평가에 사용할 코드(api호출)임
	평가데이터셋은 합성데이터셋을 12%정도 나눠서 사용하면됨
	단 이 평가데이터셋의 경우 일일히 사람이 확인해서 정확도를 높이는게 좋음(평가에서 일관성없으면 성능이 매우떨어지기때문)
	
	프롬프트는 llm의 경우 학습에 사용한 프롬프트형식을 추론할떄도 동일하게 사용해야 결과품질이 좋음
	
	평가할땐 뭐 평가데이터셋 수가 작으면 금방하지만,많은경우엔 병렬처리를 해야함
	이럴땐 openai cookbook의 깃허브 저장소의 코드(책참조)를 사용하면 편함
	얘는 비동기로 동작하고,에러뜨면 재시도도 알아서하고,요청제한걸리면 다시 요청보내고 이런걸 알아서해줌

  3.실습:미세조정 수행하기
    일단 가장 먼저해야할건 기반모델 선택임
	젤 편한건 리더보드보고,사용할 크기중에서 가장 차트높은걸 고르는거
	일단 불러와서 한번 평가데이터셋과 프롬프트넣어서 돌려보는게 좋음(기본모델의 정답률 수치확인)
	
	그리고나서 autotrain-advanced라이브러리로 미세조정을 하면됨
	자세한 학습코드는 책참조하고,메모리에러가 발생하면 배치사이즈를 좀 줄여보기
	
	또한 기반모델을 바꿔가면서 똑같이 학습시켜서 성능을 테스트해보는것도 좋음
	
	
7.모델 가볍게 만들기	
  모델의 성능을 약간 희생하면 gpu비용을 크게 낮출수있음
  1.언어모델추론 이해하기  
    언어모델이 텍스트 생성을 마치는건,eos(스퀸스끝)토큰을 생성하거나,최대길이를 넘기면 끝남
	그전까진 계속 루프를 돌리면서,자기가 생성한 모든 텍스트를 쿼리에 담아서 어텐션을 돌려 새로운 토큰을 찾아냄
	
	언어모델은 한번에 하나의 토큰만 생성할수있고,입력텍스트를 기반으로 바로 다음토큰만 예측하는 자기회귀적 특성을 가짐
	하지만 이미 처리했던 텍스트는 동시에 병렬적으로 처리할수있어서,시간이 늘어나진않음
	
	1.kv캐시
	  이렇게 셀프 어텐션 처리과정에서,같은 입력토큰에 대해 중복계산이 발생하는 비효율을 줄이는게 kv캐시임
	  얘는 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법임(그래서 kv임)
	  즉 이전에 했던계산은 계산결과를 가져오고,새로운토큰만 연산하는식
	  
	  대신 캐시를 저장해야하기때문에 메모리가 소모됨
	    2바이트(fp16사용시)*2(k,v)*어텐션레이어수*토큰임베딩차원*최대시퀸스길이*배치크기=kv캐시메모리
	  임
	2.gpu구조와 최적의 배치 크기
	  서빙이 효율적인지 판단할떈,비용,처리량,지연시간이 있음
	  즉 시간당 처리한 요청수(처리량)와,하나의 토큰을 생성하는데 걸리는시간(지연시간),비용임
	  효율적인 서빙은 같은gpu로 처리량을 높이고 지연시간을 낮춰야함
	  
	  추론을 수행할떈 배치크기만큼의 토큰을 한번에 생성함
	  그런데 모델은 고대역폭메모리에 저장하고,연산은 sram에서 수행되는데,연산을 하려면 고대역폭메모리의 파라미터를 sram으로 옮겨야함
	  즉 모델을 옮기는 시간이 있음
	  최적의 배치크기보다 배치크기가 작으면 모델파라미터를 이동시키느라 연산이 멈추는구간이 생기고(메모리바운드),
	  최적의 배치크기보다 배치크기가 크면 연산이 오래걸려서 지연시간이 길어짐(연산바운드)
	  
	  즉 적절한 배치크기를 찾아야함
	  계산식은
	    2*P*배치크기/하드웨어연산속도=P/메모리대역폭
	  임
	3.kv캐시 메모리 줄이기
	  트랜스포머는 어텐션 연산을 한번에 여러헤드에 대해 수행함
	  이때 많은수의 키값벡터를 저장해서 많은 메모리를 사용하고,더많은 데이터를 불러와야해서 속도가 느려짐
	  
	  이걸 해결하려고 모든 쿼리벡터가 하나의 키값 벡터를 공유하는 멀티쿼리어텐션이 등장했지만,성능이 떨어졌음
	  그래서 나온게 n개단위로 묶어서 사용하는 그룹쿼리어텐션임(즉 개별과 전체통합의 중간)
	  이건 그렇게 성능이 낮지도않고 속도도 빠르고 메모리도 덜먹음

  2.양자화로 모델 용량 줄이기
    양자화는 학습 후 양자화와,양자화 학습으로 나누는데 보통 학습후 양자화를 양자화라고 부름
	양자화는
	  비츠 앤 바이츠
	  GPTQ
	  AWQ
	가 주로 사용됨
	
	1.비츠앤바이츠
	  비츠앤바이츠 라이브러리는 쉽게 양자화를 할수있는 라이브러리임
	  이건 8비트로 연산을 수행하면서 성능저하가 거의없이 성능을 유지하는 8비트행렬연산과,4비트 정규분포 양자화방식(이게 4비트양자화랑 같은거)을 지원함
	  4비트는 앞에서본거랑 같고
	  
	  8비트는 기본적인 로직은 거의 비슷한데,크기가 큰 이상치가 포함된 열은 별도로 분리해서 16비트 그대로 계산하는것
	  
	  얘들이 쓰는 양자화는 다른 양자화와 달리 별도의 시간이 걸리지않는게 장점임(모델을 불러오면서 바로 양자화가 가능)
	
	2.GPTQ
	  GPTQ는 양자화 이전의 모델에 입력 x를 넣었을떄와,양자화이후 모델에 입력x를 넣었을때 오차가 가장 작아지게 모델의 양자화를 수행하는 방식임
	  즉 열단위로 한줄한줄 테스트해가면서 양자화를 해보는거임
	  
	  이건 방식상 시간이 좀 걸림(175b기준 a100으로 4시간)
	  그래서 이미 양자화한 모델이 있으면 그거 가져다쓰는게 좋음
	
	3.AWQ
	  양자화란 모델이 가지고있는 정보를 최대한 손실없이 보존하면서 용량을 줄이는것
	  AWQ는 모든 파라미터가 동등하게 중요하진않고,특별히 중요한 파라미터는 그대로 두고 나머지만 양자화를 하자는 방식임
	  
	  이때 중요한 파라미터는,모델파라미터의 값이 크거나(값이 크다면 큰영향을 주니까),입력데이터의 활성화값이 큰 채널을 중요하다고 가정하는것
	  그래서 상위1퍼의 모델파라미터는 그대로 두고,99퍼만 양자화하는식임
	  
	  그러나 여러종류의 데이터타입이 섞이면 연산이 느려지기때문에 이걸 해결해야함	  
	  이건 대충 말하자면,스케일러를 사용해 숫자를 크게만들어서 차이를 만들어내는것(그러면서 데이터타입은 통일하고)
	  1.5와 1.7을 양자화하면(2를곱하고 반올림하면) 둘다 3이되지만,모든 파라미터에 2를 한번더 곱해주면 6과 7이 됨
	  이런식으로 스케일러를 사용해서 정보의 손실을 막는것(이떄 스케일러는 2가 제일 적절함)

  3.지식증류 활용하기
    증류는 더 크고 성능이 높은 선생모델의 출력으로,더 작고 성능이 낮은 학생모델을 만드는 방법임
	학생은 선생의 생성결과를 모방하는방식으로 학습함,즉 선생의 더 큰파라미터로 쌓은 값을 더 작은 모델로 압축해 전달하는느낌
	
	요즘은 선생모델로 아예 새로운 학습 데이터셋을 대규모로 구축하거나,사람이 했던일을 다 gpt에 맞기는등 이런식으로 사용됨
	즉 데이터를 만들거나,데이터를 선별하는걸 시키는거

8.sllm 서빙하기
  1.효율적인 배치전략
    가능하면 한번에 많은 입력데이터로 한꺼번에 처리하는게 당연히 효율적인 처리가 가능함
	그러나 언어모델의 특성상,어디까지 예측할지(앞으로 몇토큰 예측할지)를 알기 어렵기떄문에 배치전략을 세울때 고려할사항이 많음
	
	1.일반배치(정적배치)
	  그냥 보통 배치처리하는느낌으로,한번에 n개를 받아서 동시처리하기만 하는 방식이 일반배치(정적배치)임
	  이때 문제는,중간에 일찍 끝난 입력도 다른애들이 끝나기까지를 기다려야하고,중간에 일찍끝난애가 있으면 그만큼 결과적으로 배치크기가 작아져 효율성이 떨어짐
	2.동적배치
	  이건 일반배치를 조금 변형한건데,일반배치가 n개를 다받을때까지 계속 기다린다면,이건 서비스에 맞게 n초동안 요청을 묶어서 배치크기를 키움
	  즉 약간의 지연시간을 추가하고 gpu효율성을 올린거
	3.연속배치
	  이건 일반배치에서 한번에 들어온 배치데이터의 추론이 끝날떄까지 기다리지않고,생성이 끝나면 그자리에 다시 새 문장을 넣어서 새로운 요청을 처리하는방식
	    115555
		22227777
		3336666
		44444888
	  이런느낌임
	  이건 대신 신경써야할게 좀 있음
	  언어모델추론은 입력프롬프트를 병렬처리하는 사전연산과 토큰을 생성하는 디코딩으로 나눌수있는데,
	  이 두개는 처리방식이 달라서 한번의 토큰생성이 끝날때마다 새 문장을 배치하는게 아닌,처리중인문장수와 대기중인 문장수의 비율을 보고 추가하기도함
	  
  2.효율적인 트랜스포머 연산
    1.플래시 어텐션
	  이건 트랜스포머가 더 긴 시퀸스를 처리하도록 만들어진 방법임
	  트랜스포머는 연산량이 학습과정에선 시퀀스길이의 제곱에 비례하고,추론과정에선 시퀸스길이에 비례하기때문에 긴걸 처리하기 힘듬
	  즉 학습과정에서도 추론처럼 시퀸스에 비례하는 1차함수로 만드는게 목적임
	  
	  트랜스포머연산은 쿼리와 키 벡터를 곱하는데서 많은 메모리를 소모함
	  정확히는 행렬곱을 하는데 드는 시간보다,마스크,소프트맥스,드롭아웃처리에 드는시간이 행렬곱보다 훨씬많음
	  그런데 행렬곱이 어텐션의 핵심이고,연산량도 제일 많음
	  문제는 연산량보다 메모리를 사용하는게 더 오래걸린다는것(메모리를 읽고쓰는게)
	  
	  그래서 플래시어텐션은 블록단위로 어텐션을 수행하고,전체어텐션행렬을 쓰거나 읽지않는식으로 어텐션연산의 속도를 높였음
	  그래서 블록단위라서 hbm과의 io를 최대한 줄이고,sram에서 다 처리할수있게 만든거
	  이러면 n*n행렬을 저장하지않고 어텐션연산이 가능함,대신 역전파때는 순전파를 다시계산해야하지만,연산시간보다 입출력시간이 훨씬 크기떄문에 이게 훨씬빠름
	2.플래시 어텐션2
	  이건 1에서 순전파시와 역전파시의 gpu처리량이 낮았던걸 개선한버전임
	  이때 행렬곱이 아닌 연산을 줄이고,시퀸스길이방향의 병렬화를 추가해서 개선했음
	  
	  행렬곱이 아닌 연산은 행렬곱인 연산보다 16배 비싸서 행렬곱이 아닌 연산을 최대한 줄이고,
	  기존에는 배치크기*어텐션헤드수만큼의 스레드블록으로 병렬처리를 했다면,여기에 시퀸스길이방향 병렬화를 추가한것
	  
	  이런식으로 2배가량의 속도를 향상시켰음
	3.상대적 위치 인코딩
	  셀프어텐션은 입력토큰의 위치에 관계없이 모두 동등하게 처리하기때문에 위치정보를 별도로 추가해야함
	  이떄 대충 수식을 거쳐서 수식에서 정해지는값을 위치임베딩값으로 더해주는식으로 처리됐음
	  
	  그러다가 이 위치인코딩도 학습파라미터로 두고 함께 학습시키기 시작함(학습할수있는 임베딩층)
	  이래도 결국 추론때는 위치임베딩값을 더하는건 같지만
	  이런방식이 절대적 위치 인코딩임
	  이 절대적 위치 인코딩의 단점은,학습데이터보다 더 긴 입력이 들어오면 빠르게 생성품질이 떨어짐
	  
	  그래서 나온게 상대적 위치 인코딩임
	  절대적 위치인코딩은 어텐션값에서 현재 두 토큰간의 거리가 아닌,전체 문장에서 어디에 위치하냐에 따라 다른값이 더해지지만,
	  상대적 위치인코딩은 전체 텍스트가 아닌,현재 토큰의 위치 기준으로 다른 토큰들이 얼만큼 떨어져있냐를 기준으로 모델에 입력해서,
	  전체문장이 아닌 토큰간의 거리를 학습하게함
	  
	  대표적인 상대위치인코딩으로 RoPE와 ALiBi가 있음(보통 ALiBi사용)
	  이런걸 사용하면 입력토큰이 길어져도 성능이 빠르게 나빠지지않음
	  또한 간단한 인코딩방식이라 학습과 추론에 추가시간도 들지않음

  3.효율적인 추론 전략
    1.커널퓨전
	  gpu연산은 커널단위로 이뤄짐
	  하지만 연산을 수행하기위해선 전후에 추가적인 작업을 위한 오버헤드가 발생함
	  그래서 반복적으로 수행하는 연산에 대해선,각 커널단위로 분리하는거보다 하나로 묶어 오버헤드를 줄이는 커널퓨젼을 사용하면 좀 더 효율적이됨
	  
	  플래시어텐션도 이런방식으로 오버헤드를 줄인것
	
	2.페이지 어텐션
	  kv캐시는 키값벡터를 중복연산을 줄여 연산시간은 줄이지만,메모리 사용량이 커져서 배치크기가 줄어듬
	  그래서 페이지어텐션을 사용하면 kv캐시를 효율적으로 관리하면서 배치크기도 크게잡을수있음
	  
	  기존 kv캐시가 앞으로 사용할수도있는 메모리를 미리 잡아두면서 메모리낭비가 심했다면,이걸 개선한것
	  핵심은 kv캐시가 연속적인 물리 메모리를 사용하기위해 미리 메모리를 준비하기때문에 발생함
	  그래서 가상메모리처럼 논리적메모리와 물리적메모리를 연결하는 블록테이블을 관리해서 논리적으로 연속적이게 만든게 페이지어텐션임
	  
	  또한 같은프롬프트의 결과값을 공유하는등의 여러 부가효과가 있지만 크게 중요하진않은듯?(병렬샘플링시 값공유)
	
	3.추측 디코딩
	  이건 쉬운단어는 더 작고 효율적인 모델이 예측하고,어려운단어는 크고 성능좋은모델이 예측하는 방식임
	  이건 작은 드래프트모델과 큰 타깃모델을 사용하고,작은 드래프트모델이 빠르게 추론해서 k개의 토큰을 먼저 생성하고,
	  타깃모델이 이 토큰이 내가만든것과 같을지 계산해서 동일하다면 승인하고 동일하지않다면 거절함
	  
	  즉 일단 드래프트 모델이 2개를 먼저 만들고,이걸 병렬적으로 타겟이 검증하는 방식임
	  이것도 연산속도보단 메모리입출력속도가 빠르다는거에서 나온방식임
	  즉 n개의 토큰을 작은모델로 만들고,그걸 타겟이 검증하면서 특정 토큰의 일치도가 낮을경우 그 이후의 모든 토큰을 버리는식으로 동작함
	  
  4.실습:llm서빙 프레임워크
	서빙은 vLLM이라는 프레임워크를 쓰면 됨
	llm의 추론은 크게 오프라인추론과 온라인추론으로 나눠짐
	
	1.오프라인 서빙
	  이건 정해진 입력데이터에 대해 배치추론을 수행하는걸 말함
	  모델에 입력할 데이터가 이미 정해져있기때문에,배치처리로 처리량을 높일수있음
	  
	  사용법은 실제사용할떄 책보자
	
	2.온라인 서빙
	  이건 요청이 올때 추론을 수행함
	  즉 서버를 띄우고 추론을 그쪽에서 날리는거
	  vLLM쓰면 서버띄우는건 알아서해줌
	
	  
9.llm어플리케이션 개발하기	  
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
    
  