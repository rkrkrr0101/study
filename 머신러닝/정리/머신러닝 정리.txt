1.기초

머신러닝은 작업의 성능이 경험으로 인해 변하면 머신러닝 프로그램임

시스템이 학습할때 사용하는 샘플은 훈련세트
각 훈련데이터는 훈련샘플(샘플)

스팸메일분류기에서는
작업=스팸메일구분
경험=훈련데이터
성능=정확도(이건 자기가 원하는거로 직접 정해야하는데 분류에선 정확도 보통씀)

머신러닝을 통해 우리가 몰랐던 연관관계나 추세를 발견하는것은 데이터 마이닝

레이블의 종류에 따른 분류

	지도학습:훈련데이터에 레이블(정답인지 아닌지 표시)가 있는데이터로 학습하는방법

		대표적으로 분류가 전형적인 지도학습임(스팸메일분류같은)

		또 다른 예는 회귀가 있음
		회귀는 예측변수를 통해 타깃수치를 예측하는것
		예를들어 차 브랜드,주행거리등을 파라미터로 받아서 중고차값을 뱉는식
		
	비지도학습:비지도학습은 훈련데이터에 레이블이 없는상태에서 학습하는것
			대표적으로 군집,시각화등이 있음
			시각화는 레이블이 없는 데이터를 넣으면 눈으로 볼수있는 표현을 만들어줌
			예로 강아지 고양이 트럭등을 군집지어져있는걸 시각화시킨다던지 할수있음
			
	준지도학습:준지도학습은 일부만 레이블이 있는 데이터고 나머지는 레이블이 안된 데이터인상태에서 학습하는것
			예를들어서 사진여러개에서 사진들에 사람a,사람b가 사진 1,2에 있는지는 비지도학습으로 할수있지만,
			그사람이 누군지는 지도학습으로 해야하니까 
			사진에 사람abc태그하는건 비지도학습으로 하고 그 사람 이름같은거 적는건 지도학습으로 하는식
			
	강화학습:강화학습은 환경을 관찰해서 행동을 실행하고,거기따른 보상또는 벌점을 받아서 큰 보상을 따르게 행동방식을 수정하면서 반복하는것
		  게임ai나 보행로봇등에 주로 쓰임

실시간인지 아닌지에 따른 분류

	배치학습:시스템을 오프라인에서 학습 다 시킨다음에 그걸 적용하는식,온라인상에서 입력이들어와도 학습하지않고 출력만 함
		  배치학습이 새로운데이터를 학습하려면 전체데이터를 써서 처음부터 다시훈련해야함
		
	온라인학습:시스템을 하나씩,또는 일정 묶음단위로(미니배치) 주입해서 시스템을 계속 훈련시킴
		   이건 빠른 변화에 적응해야하는 시스템에 적합함
		   
		   온라인학습은 데이터를 학습하고나면 그 데이터는 필요없으니까(전으로 돌릴필요가없으면)삭제해도 됨
		   
		   그리고 메인메모리에 들어가지 않는 크기의 데이터셋을 학습할때도 이걸 사용할수있음
		   미니배치를 계속 넣는식으로 돌리면됨(외부메모리학습,배치학습인데 데이터클때 온라인학습방식으로 돌리는거)
		   
		   온라인학습에서 중요한 파라미터는 변화하는 데이터에 얼마나 빨리 적응할것인지를 나타내는 학습률
		   학습률을 높게하면 새로운 데이터에 빨리 적응하지만,이전 데이터를 금방 잊어버림
		   학습률을 낮게하면 새로운 데이터에 적응하는데는 오래걸리지만,새로운 데이터의 잡음같은거에 덜민감해짐
		   
		   온라인학습의 가장 큰 단점은 악의적데이터가 주입될때 시스템성능이 나빠질수 있다는것
		   그래서 시스템을 모니터링하거나 입력데이터를 모니터링해서 이런걸 걸러내야함

어떻게 일반화 되는가에 따른 분류
	사례 기반 학습:훈련샘플을 기억해서 유사도등을 측정해서 비교하는식으로 일반화 하는 방식
			  특성1과 특성2가 있는 그래프가 있을때,주변에 있는 값 중 가장 가깝거나 많은거로 분류하는식
	
	모델 기반 학습:샘플들의 모델을 만들어서 예측에 사용하는 방식
			  식을 만들어서  거기에 특성을 넣고 가공해서(절편을주거나,민감도를 조절하거나 해서) 대충 근처값이 나오게 하는식임
			  
			  모델이 얼마나 좋은지는 측정지표를 정해야 하는데
			  모델이 얼마나 좋은지 측정하는 효용함수나,모델이 얼마나 나쁜지 측정하는 비용함수가 있음
			  
			  선형회귀에서는 선형모델의 예측과 훈련데이터 사이의 거리를 재는 비용함수를 사용해서 
			  파라미터를 조절해 이 거리를 최소화하는게 목표임 이걸 모델을 훈련시킨다고 함
			  
즉, 머신러닝은
	데이터를 분석해서
	모델을 선택하고
	훈련데이터로 모델을 훈련시킴(비용함수가 제일 작아지는값을 찾음)
	마지막으로 새로운 데이터를 넣어서 예측을하고 잘되기를 기대함
	
머신러닝에서 문제가 되는 가장 큰 두가지는
	나쁜데이터:
		충분하지 않은량의 데이터
		
		대표성이 없는 훈련 데이터
			우리가 일반화 하기를 원하는 새로운 사례를 잘 대표하는 데이터를 넣어야함
			근데 샘플이 작으면 샘플링 잡음(우연에의한 대표성없는 데이터)
			샘플이 커도 추출방법이 잘못되면 샘플링 편향(무슨 이유등으로 한쪽에 몰린 데이터들만 추출된다거나)
			등으로 문제가 생길수있음
		
		낮은 품질의 데이터
			데이터가 에러,이상치,잡음등으로 가득하면 이상한패턴을 찾거나 패턴을 찾기 어려워하기때문에 
			데이터 정제에 시간을 투자해야함(일부샘플이 이상치인게 확실하면 지우거나,특성이 빠져있으면 샘플을 무시하거나 중간값채우거나)
			
		관련 없는 특성
			특성이 관련없는특성만 가득하거나 그러면 막 이상한값이 나올수도있음
			그래서 훈련에 사용할 좋은 특성들을 찾고,특성들을 합쳐서 더 유용한 특성을 만들어야함(특성추출)
			
	나쁜 알고리즘:
		데이터 과대적합
			훈련 데이터에 너무 잘 맞지만, 일반성이 떨어진다는 것
			세트에 잡음이 많거나 데이터셋이 너무 작을떄(샘플링잡음)잡음이 섞인 패턴을 감지하게되면(잡음의 양에 비해 모델이 너무복잡하면),
			당연히 새로운 샘플에 일반화가 안됨
			그래서 특성을 잘 골라야하고 훈련데이터의 잡음을 줄이고,훈련데이터 양을 늘려야함
			
			모델을 단순하게하고,과대적합의 위험을 감소시키기위해 모델에 제약을 가하는걸 규제 라고 함
			만약 a+bxn=c 라는 식에서 n이 특성이면,건드릴수있는건 a와 b인데 이 두개를 바꿀수 있는 자유도를 모델에 주면,(자유도2)
			모델은 직선의 절편과 기울기를 조절할수 있는데,우리가 여기서 a=0으로 고정시켜버리면 할수있는 자유도가 줄어드니까(자유도1)
			모델이 간단해지고 막 데이터에 끼워맞춘 식을 만들기 어려워지게됨
			딱 0 이렇게 안해도 수정은 할수있지만 값의 최소,최대값을 정해두면 자유도 1과 2사이 어딘가에 위치한 모델이 됨
			
			여기서 저렇게 딱 0이렇게 모델에서 변하지않고 상수로 주는걸 하이퍼파라미터라고 함
			이건 학습알고리즘에 영향을 받지않고,훈련전에 정해져있고,훈련하는동안 상수로 남아있음
			머신러닝에서 하이퍼파라미터 튜닝은 매우 중요한 과정임
			
		데이터 과소적합
			과대적합의 반대,모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할때 일어남
			이걸 해결하려면 모델파라미터를 늘리거나,더 좋은 특성을 제공하거나(특성추출등을 해서),모델의 제약을 줄여야함
		
	테스트와 검증
		
		모델이 얼마나 잘 일반화 될지 아는 방법은 가장 확실한건 실제서비스에 넣고 모니터링하는거지만,리스크가 크니까
		훈련데이터를 훈련세트와 테스트 세트로 나눠서
		훈련세트만 가지고 훈련을 한 다음에 테스트 세트를 일반화샘플처럼 사용해서 모델을 테스트함
		새로운 샘플에 대한 오류 비율을 일반화 오차라고 하는데,테스트세트로 평가해서 이 오차에 대한 추정값을 얻을수 있음
		훈련 오차가 낮지만(훈련세트에서 모델의 오차가 적지만)일반화 오차가 크다면 이건 과대적합됐다는것
		
		보통 데이터의 80퍼센트를 훈련에 쓰고,20퍼센트를 테스트용으로 떼어두는데,데이터크기에 따라 테스트데이터를 줄여도됨
		
		하지만 일반화오차를 테스트세트에서 여러번 측정하면,모델과 하이퍼파라미터가 테스트세트에 최적화된 모델을(과대적합된)만들기떄문에
		모델이 새로운 데이터에 잘 작동하지 않을수있음
		그래서 보통 홀드아웃 검증이라는 훈련세트의 일부를 떼어내서 여러 후보모델을 평가하고 가장 좋은 하나를 선택함
		이 홀드아웃세트를 검증세트라고 함
		
		구체적으로는 검증세트가 빠진 훈련세트로 다양한 하이퍼파라미터값을 가진 여러 모델을 훈련해서 프로토타입1을 만들고,
		검증세트에서 가장 높은 성능을 내는 모델을 선택해서 전체훈련세트에서 다시 훈련해 최종모델(프로토타입2)를 만들고,
		이걸 테스트세트에서 평가해서 일반화오차를 추정함
		
		이건 보통 잘 작동하는데,검증세트가 너무작으면 모델이 정확하게 평가가 안되고,검증세트가 너무 크면 훈련세트가 너무 작아지기떄문에
		최종모델은 전체훈련세트에서할건데 너무 작은데서 하면 적절하지 않음
		그래서 이걸 해결하는 방법은, 작은 검증 세트를 여러개 만들어서 사용해 반복적인 교차검증을 수행하는것
		모든 검증세트를 모든 모델에 돌린다음 값을 평균내서 고르면 훨씬 정확한 성능을 측정할수있지만,단점으로 훈련시간이 많이늘어남
		
		데이터 불일치
			만약 사진찍어서 뭔지 맞추는걸 하고싶은데 웹에서 긁어오면 엄청나게 정확한 데이터들만 있기때문에
			실제 데이터를 완벽하게 대표하지 못할수 있음
			이럴때 진짜 찍은 사진을 반반나눠서 검증세트와 테스트세트에 넣고(중복되거나 비슷한사진이 들어가면안됨)돌려야하는데
			보통 이러면 성능이 매우 나쁘게 나옴
			
			근데 이게 데이터불일치때문인지,과대적합인지 알기 어려운데 이걸 구분하는 방법은
			웹에서 긁은 데이터를 떼어내서(훈련데이터를 떼서)훈련개발세트를 만들고(훈련데이터에서 분리함)
			훈련세트에서 돌린다음 훈련개발세트에서 평가했는데 잘 작동하면 과대적합이 아니고 데이터불일치니까 데이터 전처리를 해야하고
			만약 성능이 나쁘면 과대적합이니까 규제하거나 데이터를 늘리거나 데이터정제를 해야함
			
			그래서 머신러닝은 학습용 데이터셋에서 종속변수와 독립변수의 관계를 분석해서 이거로 모델을 만드는거기떄문에
			철저하게 학습용 데이터셋에 종속됨
			
2.머신러닝 처음부터끝까지(회귀)
		
머신러닝의 순서는
	1.큰그림보기
	2.데이터 구하기
	3.데이터에서 뭘쓸지 알기위해 탐색하고 시각화하기
	4.데이터 전처리
	5.모델 선택,훈련
	6.모델 파라미터조정
	7.솔루션 제시
	8.런칭,유지보수
순서
1.큰그림보기
	
	무슨 데이터를 써서 무슨 값을 뽑을건지 알고,
	이걸 만드는 이유(이걸 최종값으로 쓸건지 다음모델에 인풋값으로 쓸건지 등) 
	이유를 알고 얼마나 정확하게 넣어야하는지 시간투자 얼마나해야하는지 등을 판단,
	현재 솔루션이 있는지,있다면 해결방법에 관한 정보랑 참고성능으로 사용할수있음
	데이터셋이 어떻게 구성된건지 
	등을 토대로 사용할 모델과 방식을 찾아야함
	
	다음으로 성능 측정 지표를 선택해야함
	회귀에서 주로 사용하는건 평균 제곱근 오차(RMSE)를 사용,만약 이상치로 보이는 구역이 많으면 평균 절대 오차(MAE)를 사용할수있음
	
	마지막으로 지금까지 짜둔 플랜을 적어서 확인해봐야됨
	만약 다음거에 값으로 들어갈줄알았는데 분류로 들어가면 아예 문제가 달라지니(회귀->분류) 확인해야함
	
2.데이터 가져오기
	
	주피터,넘파이,판다스,맷플롤립,사이킷런등을 설치하고(그냥 아나콘다 주피터노트북쓰는게 젤편할듯)
	
	1.데이터 가져오기 
		데이터 가져오는 함수 만들어두는게 편함
		
		import os
		import tarfile
		import urllib
		
		다운로드루트='url'
		저장위치=os.path.join("폴더","폴더2")
		다운로드파일=다운로드루트+"받을파일"
		
		def 데이터저장함수 (다운로드파일,저장위치):
			os.makedirs(저장위치,exist_ok=True) 저장위치로 디렉토리만들고
			저장파일=os.path.join(저장위치,"파일명") 파일저장할거 만들어두고
			urllib.request.urlretrieve(다운로드파일,저장파일) 파일 받아서 저장파일에 넣고(.tgz파일)
			파일저장=tarfile.open(저장파일)
			파일저장.extractall(path=저장위치) 파일생성(csv파일)
			파일저장.close() 닫기
		
	
	2. 데이터 읽기 
		데이터 읽기함수는
		
		import pandas as pd
		
		def 데이터읽기함수(저장위치):
			파일=os.path.join(저장위치,파일명)
			return pd.read_csv(파일)
		
		하면 데이터프레임 객체를 리턴함
		
		데이터를 받았으면 데이터를 대충 보고(값이랑 자료형같은거,오브젝트면 csv니까 문자열일거고,그게 랜덤문자인지 범주인지 확인)
		
		a=데이터읽기함수(저장위치)
		a.head() 5개만출력
		a.info() 자료형과 널이아닌값등 확인가능
		a.describe()숫자형 특성의 요약정보(max,min,평균,표준편차,25%,50%,75% 등)
		
		%matplotlib inline  주피터노트북에서 바로 출력하게 함 주피터노트북의 매직커맨드 
		import matplotlib.pyplot as plt
		a.hist(bins=50,figsize=(20,15)) 히스토그램 출력
		plt.show()
		
		히스토그램을 보고 데이터의 특이사항을 찾아야함
		데이터가 어떤식으로 스케일링 되어있는지,상한 하한이 있는지,상한하한이 있다면 어떻게 처리할것인지,한계값밖은 어떻게처리할것인지
		특성들 스케일 어떻게 맞출것인지,왼쪽이나 오른쪽이 두껍거나 그러면 종모양으로 어떻게 바꿀것인지 등
	
	
	
	
	3.테스트세트 만들기 
		대충 보고 테스트세트를 떼어놓고 나선 테스트세트는 절대 들여다보면 안됨
		막 테스트세트의 패턴보고 끼워맞추기 할수있게되기때문
		
		테스트세트 만드는 가장 쉬운방법은 랜덤으로 퍼센트만큼 꺼내는것
		
		import numpy from np
		
		def 랜덤테스트분할(데이터,테스트비율):
			데이터셔플=np.random.permutation(len(데이터))
			테스트사이즈=int(len(데이터)*테스트비율)
			테스트세트=데이터셔플[:테스트사이즈]
			트레인세트=데이터셔플[테스트사이즈:]
			리턴 데이터.iloc[트레인세트],데이터.iloc[테스트세트]
		
		트레인,테스트=랜덤테스트분할(데이터,0.2)
			
		이렇게 만들면됨
		
		단 이렇게하면 여러번 만들면(매번시드가 달라서)전체를 다 보게 되니까 
		샘플의 키값의 해시값으로 나누는게 젤 정확함(데이터가 늘어날수도있으니)
		
		from zlib import crc32
		
		def 해시체크(id,테스트비율):
			return crc32(np.int64(id))& 0xffffffff<test_ratio *2**32
		def 해시데이터분할(데이터,테스트비율,id컬럼이름):
			id=데이터[id컬럼이름]
			테스트세트=id.apply(lambda id_:해시체크(id_,테스트비율)
			return 데이터.loc[~테스트세트],데이터.loc[테스트세트]  트레이세트 테스트세트 순

		데이터프레임.loc[]는  a.loc[로우,컬럼]순으로 문자열로 검색 iloc는 인덱스로 검색
		
		만약 프라이머리키가 없어서 만들어야할때 행의 인덱스를 사용하면 새 데이터는 반드시 데이터의 끝에 추가해야하고,
		어떤데이터도 삭제되면 안됨
		이게 불가능하면 여러개를 조합해서 만들어야하는데 두개조합하면 유일성이 보장되어야함
		
		그냥 제일 편하게 분할하는건 사이킷런꺼 쓰는건데
		
		from sklearn.model_selection import train_test_split
		
		트레인세트,테스트세트=train_test_split(데이터,test_size=0.2,random_state=42) 하면 42시드로 20%만큼 분할해서나옴 
	
	
		3-2.계층샘플링
			근데 이렇게 완전히 무작위로 하면 데이터셋이 많이 크면 괜찮은데,안그러면 데이터 편향이 생길 확률이 큼
			만약 여자비율이 61퍼센트고 남자가 39퍼센트면, 여자61명 남자 39명을 뽑아야 하는데 랜덤돌리면 오차가 생겨서 여자 55명 남자 45명
			이렇게 뽑힐수가 있음(숫자가 작을때)
			
			그래서 계층에서 비율을 맞춰서 랜덤으로 뽑아야 하는데 이걸 계층적 샘플링이라고 함
			
			계층을 만드는 방법은
			
			데이터["새로만드는컬럼"]=pd.cut(데이터["목표컬럼"],
									bins[0,1.5,3.0,np.inf], 계층 범위
									labels=[1,2,3,4]) 계층별이름
			
			데이터['새로만드는컬럼'].hist() 데이터 보기
			
			이렇게 만들어서 계층샘플링을 하면 됨
			
			
			계층샘플링은 
			from sklearn.model_selection import StratifiedShuffleSplit
			
			split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42) 트레인세트1개 테스트사이즈0.2 시드42
			for 트레인인덱스,테스트인덱스 in split.split(데이터,데이터["새로만드는컬럼"]):
				트레인세트=데이터.loc[트레인인덱스]
				테스트세트=데이터.loc[테스트인덱스]
			
			이렇게하면 새로만드는컬럼에 있는 비율대로 랜덤하게 잘라서 트레인테스트 나눠담음
			
			그리고 나서 새로만드는컬럼을 삭제해주면됨
			
			for 세트 in (트레인세트,테스트세트):
				세트.drop("새로만드는컬럼",axis=1,inplace=True) axix는 행인지열인지 선택 1이면 열을따라동작(세로줄제거)
														  inplace는 자기자신에 바로 적용할건지 선택(세트=세트drop()안해도된다는거)
			
			
3.데이터 시각화
	1.지리적 데이터 시각화 
		일단 훈련세트떼뒀는데 건드리지 않게 복사본 만들어서 사용(훈련세트는 더이상 직접 작업하지않음 이상치제거같은거 빼곤)
		
		데이터=트레인세트.copy()
				  
		위도 경도같은게 있으면
		
		데이터.plot(kind="scatter"(종류=산점도), x="위도", y="경도",alpha=0.1(하나당 밝기=0.1)
				  s=데이터["구역인구"]/100(원의반지름=),label="구역인구"(라벨),figsize=(10,7)(차트크기),
				  c="가격"(색="가격),cmap=plt.get_cmat("jet")(색상종류뭐쓸건지),colorbar=True(옆에 바 띄울건지)
		)
		plt.legend() 범례띄우기
		이렇게 산점도로 나타내서 볼수있음
	   
	2.상관계수
		데이터셋이 별로 안크면 모든 특성간의 표준 상관계수를 corr()로 구할수있음
		
		corr값=데이터.corr()
		
		corr값["원하는컬럼"].sort_values(ascending=False)
		
		하면 원하는컬럼의 증가감소에 따른 상관관계가 나오는데,선형적인거만 알수있지 비선형적인건 알수없음(x2=y같은 2차함수이상)
		그리고 상관관계는 기울기와는 상관없이 진짜 얼마나 관계있는지가 나옴
		
		이걸 그래프로 보는 방법은
		
		from pandas.plotting import scatter_matrix
		
		원하는컬럼=["1번컬럼","2번컬럼","3번컬럼","4번컬럼"]
		scatter_matrix(데이터[원하는컬럼],figsize=(12,8))
		
		하면 그래프가 나오는데 여기서 이상치나 상관관계같은거 체크하면됨,이상치가 있으면 없애주는게 좋음
	
	
	3.특성 조합
		마지막으로 데이터를 섞어서 좀 더 높은 상관계수를 뽑아낼수 있을수도있음
		특정 지역의 방 갯수는 사람수를 모르면 주택 가격에 영향을 미치는게 낮지만,
		특성 조합으로 가구당 방 갯수를 구할수있으면 상관관계가 높아짐
		
		대충 데이터["가구당방갯수"]=데이터["총 방수"]/데이터[가구수]
		이런식으로 특성을 조합해서 추가해서 상관관계를 확인하면 좋은특성이 나올수있음
		
4.데이터 전처리
	데이터 전처리는 수동으로 하드코딩해서 하면 안되고 어지간하면 함수만들어서 자동화 해야함
	이유는
		어떤 데이터셋이 들어와도(새로운 데이터셋이) 데이터변환이 쉬움
		다른 프로젝트에 사용할수있는 변환 라이브러리를 쌓게됨
		실제 시스템에서 알고리즘에 새 데이터를 주입하기 전에 변환시킬떄 이함수를 사용할수 있음
		여러 데이터 변환을 쉽게 시도할수있고 어떤 조합이 가장 좋은지 확인하기 편리함
	
	이거도 맨처음에 훈련세트를 복사하고
	레이블값을 떼어내서 따로 넣어두고 데이터에서 레이블값을 제거함
	
	1.데이터 정제
		데이터에 누락된값(null)이 있으면 이상하게 동작하니까 이걸 처리해줘야하는데 방법은 3개임
			1.해당 로우를 제거              
			2.해당 컬럼을 제거              
			3.어떤 값으로 채우기(0,평균값 등)   
		
			데이터.dropna(subset=['원하는컬럼'])       1번
			데이터.drop('원하는컬럼',axis =1)          2번
			데이터['원하는컬럼'].fillna(0,inplace=True)3번
			
			만약 3번으로 채웠으면 저 채운값을저장해둬야 나중에 테스트세트나 실제 운영할때 새로운데이터에 있는 누락된값을 채울수있음
			
			사이킷런을 쓰면 채우기 쉬운데 중간값같은건 수치형에서만 계산 가능하기때문에 텍스트같은건 제외해서 뽑아둔다음 계산하고 합쳐야함
			
			from sklearn.impute import SimpleImputer
			
			imputer=SimpleImputer(strategy="median") 중간값으로 세팅
			데이터숫자=데이터.drop(문자열데이터컬럼들,axis=1)
			imputer.fit(데이터숫자)
			
			이러면 imputer안에 누락된값들이 다 중간값으로 채워져있음
			이걸 보고싶으면
			imputer.statistics_ 하면 각 특성의 평균값을 볼수있음
			
			이걸 넘파이 배열로 받을수도 있는데 
			
			X=imputer.transform(데이터숫자) 이러면 배열로 받아지고 다시 데이터 프레임으로 바꿀려면
			
			데이터=pd.DataFrame(X,columns=데이터숫자.columns,index=데이터숫자.index)
			이러면 데이터프레임으로 되돌릴수있음

			
			당장은 한두개특성만 비어있는칸이 있다고해도 실제 동작할때는 어떤값이 누락될지 알수없으니까 모든값에 imputer돌리는게 바람직
			그리고 물론 테스트세트에서는 훈련세트에서의 평균값을 그대로 넣어야함 테스트세트의 평균값넣으면안됨
			
	tmi)사이킷런은
		추정기:데이터셋으로 모델 파라미터를 추정하는객체를 추정기라고 부름,위에서 평균값을 알아내는 imputer은 추정기임
			추정은 fit로 이뤄지고 하나의 매개변수로 하나의 데이터셋만 전달하고,추정과정에서 필요한 다른 매개변수들은 전부 
			하이퍼파라미터로 간주되고 인스턴스변수로 저장됨
		변환기:데이터셋을 변환하는 추정기를 변환기라고 함,imputer은 변환기임
			 데이터셋을 변환할때는 데이터셋을 매개변수로 받은 transform()메서드가 수행하고 변환된 데이터셋을 반환함
		예측기:어떤 추정기들은 주어진 데이터셋에 대해 예측을 만들수가 있는데 전장에 나온 선형모델이 예측기임
			예측기의 predict()는 새로운 데이터셋을 받아 이에 맞는 예측값을 반환함
			그리고 테스트세트를 사용해 예측의 품질을 측정하는 score()메서드를 가지고있음
		보통 그래서 사이킷런으로 만들때는 pipeline를 써서 여러 변환기를 연결한다음 마지막에 추정기를 배치하는식으로 쉽게만들수있고,
		사이킷런은 대부분의 매개변수에 기본값이 있어서 어지간하면 동작함
	
	2.텍스트와 범주형특성
		범주형 특성들은 그냥 숫자로 변환시켜서 사용하는게 좋음 
		대부분 머신러닝 알고리즘은 숫자를 다루기때문
		
		숫자로 바꾸려면
		
		from sklearn.preprocessing import OrdinalEncoder
		ord=OrdinalEncoder()
		데이터2=ord.fit_transform(데이터)
		
		이러면 모든 범주형 특성들이 같은거끼리 1,2,3,4,5등으로 표시되고
		범주형특성들의 카테코리 목록은 ord.categories_를 하면 리스트가 반환됨
		
		근데 이렇게하면 알고리즘이 붙어있는거끼리 가깝다고 생각하기때문에,
		실제로 그렇지않다면 카테고리별 이진특성을 만들어 해결함(특성을 전부 만들어 두고 하나가 1이면 나머지가 다 0이되는식)
		이걸 원-핫 인코딩이라고 부름
		
		하는방법은
		
		from sklearn.preprocessing import OneHotEncoder
		
		OHenco=OneHotEncoder()
		데이터원핫=OHenco.fit_transform(데이터)
		
		이러면 원핫식으로 변환된게 나옴
	3.나만의 변환기
		변환기를 만들려면 사이킷런은 덕타이핑을 지원하므로
			fit() self를 반환
			transform()
			fit_transform()
		메서드를 구현한 클래스를 만들면 됨 
		마지막 fit_transform는 TransformerMixin을 상속하면 자동으로 생성되고
		BaseEstimator을 상속하고 생성자에 *args나 **kargs를 사용하지 않으면 하이퍼파라미터 튜닝에 필요한
			get_params()
			set_params()
		를 추가로 얻을수있음 
		
		예시로
		from sklearn.base import BaseEstimator,TransformerMixin

		rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6

		class CombinedAttributesAdder(BaseEstimator,TransformerMixin):
			def __init__(self,add_bedrooms_per_room=True): #생성자
				self.add_bedrooms_per_room=add_bedrooms_per_room
			def fit(self,X,y=None):
				return self
			def transform(self,X):
				rooms_per_household=X[:,rooms_ix]/X[:,households_ix]
				population_per_household=X[:,population_ix]/X[:,households_ix]
				if self.add_bedrooms_per_room:
					bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]
					return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]
				else:
					return np.c_[X,rooms_per_household,population_per_household]
				
				
		
		
		위에서 특성 조합하는거 자동화한 함수
		init에서 베드퍼룸특성을 만들어주고 
		fit은 자기자신을 보내주는거만 하고
		트랜스폼에서 특성을 만들어서 리턴해줌
		그리고 get_params과 set_params은 파이프라인과 그리드탐색에 꼭 필요한 메소드라서 무조건 BaseEstimator를 상속해야함
		
	4.특성 스케일링
		몇가지 알고리즘을 빼면 머신러닝은 입력 숫자들의 스케일이 다르면 잘 작동하지 않음
		그래서 숫자들의 스케일링을 맞춰줘야하는데
		대표적으로
			min-max스케일링(정규화)
			표준화
		가 있음
		
		정규화는 가장 간단한데 모든값이 0과 1 범위 사이에 들어가도록 값을 이동하고 스케일을 조정하면됨
		즉,데이터에서 최소값을 빼고,최댓값-최소값의 차이로 나누면 이렇게 됨
		사이킷런에서는 MinMaxScaler변환기를 제공함
		
		표준화는 평균을 빼고 표준편차로 나눠서 결과분포의 분산이 1이되게 만듬
		표준화는 범위의 상한 하한이없어 어떤 알고리즘에선 문제가 될수있지만 이상치에 영향을 덜 받음
		사이킷런에선 StandardScaler변환기를 제공함
		
	5.파이프라인
		이렇게 막 해야할게 많고 정확한순서로 실행해야하니까 PipeLine으로 순서대로 실행할수있음
		근데 일반 파이프라인은 숫자특성만 처리할수있음
		사용법은
		
		from sklearn.pipeline import PipeLine
		from sklearn.preprocessing import StandardScaler
		
		숫자파이프라인=PipeLine([
			('빈칸채우기',SimpleImputer(strategy='median')),
			('특성추가',CombinedAttributesAdder()),
			('표준화변환기',StandardScaler()),
		
		]
		)
		데이터숫자트랜스폼=숫자파이프라인.fit_transform(데이터숫자)
		
		파이프라인은 이렇게 이름과 추정기 쌍을 입력으로 받아서(마지막단계는 변환기 추정기 둘다쓸수있지만 그전엔 전부 변환기여야함),
		전단계의 출력을 다음단계의 입력으로 넣어주고(마지막단계 전엔 fit_transform 마지막단계는 fit)
		
		그리고 범주형과 숫자형을 묶어주고싶으면

		from sklearn.compose import ColumnTransformer
		
		숫자애트리뷰트=list(데이터숫자)
		범주애트리뷰트=["범주애트리뷰트1",...]
		
		풀파이프라인=ColumnTransformer([
			('숫자',숫자파이프라인,숫자애트리뷰트),
			('범주',OneHotEncoder(),범주애트리뷰트)
			
		])
		총데이터=풀파이프라인.fit_transform(데이터)
		
		이런식으로 묶어서 받을수있음
		
		여기서 OneHotEncoder는 희소행렬을 반환하는데 숫자파이프라인은 밀집행렬을 반환함
		이렇게 두개가 섞여있을땐 최종행렬의 밀집정도를 추정해서(0이아닌 원소의비율) 밀집도가 일정수치보다 낮으면 희소행렬을 반환함
		높으면 밀집행렬을 반환
		
5.모델선택과 훈련
	1.훈련세트에서 훈련과 평가
		앞에서 전처리랑 다해놨으면 그냥
		from sklearn.linear_model import LinearRegression
		
		선형모델=LinearRegression()
		선형모델.fit(총데이터,데이터레이블)
		
		하면 끝임
		
		이제 훈련세트에 있는 샘플 몇개를 넣어보려면
		
		a데이터=데이터.iloc[:5]
		a라벨=데이터레이블.iloc[:5]
		a전처리데이터=풀파이프라인.transform(a데이터)
		선형모델.predict(a전처리데이터)
		
		하고나서 a라벨과 선형모델값을 비교해보면 됨
		
		값을 아까 봤던 RMSE로 측정해보려면
		
		from sklearn.metrics import mean_squared_error
		
		데이터예측값=선형모델.predict(총데이터)   데이터 넣고 값만 받아보기 훈련x 내부에 영향끼치지않음
		선형mse=mean_squared_error(데이터레이블,데이터예측값)
		선형rmse=np.sqrt(선형mse)
		
		하면 값이 나옴
		
		여기서 오차가 크면 과소적합된거니 더 강력한모델을 쓰거나 더 좋은특성을 넣거나 규제를 감소시켜야됨
		우리가 지금한거는 규제가 없으니 더 강력한모델이나 더 좋은특성을 넣어야함
		
		만약 더 강력한 모델을 썼는데 과대적합나면 교차검증을 사용할수있음
	2.교차검증
		사이킷런에는 훈련세트를 n개로 나눠서 n번 훈련하는데 그중 n-1개로 훈련하고 n개로 평가하는식으로 할수있는 기능이 있음
		사용법은
		
		from sklearn.model_selection import cross_val_score
		
		스코어=cross_val_score(사용모델,총데이터,데이터레이블,scoring="neg_mean_squared_error",cv=10)
		모델스코어=np.sqrt(-스코어)
		
		사이킷런의 교차검증은 클수록 좋은 효용함수기때문에 mse의 반대값인 neg_mean_squared_error를 사용하고,
		계산하기전에 -붙여서 부호바꿔줘야함
		
		결과보는법은
		print(모델스코어) 점수배열 리턴되고
		print(모델스코어.mean()) 평균 리턴
		print(모델스코어.std()) 표준편차 리턴

		
		근데 이방식은 엄청 오래걸려서 비용이 비싸니까 막 아무때나 쓸수있는건 아님
		
	tmi)여러 다른 모델들을 모아서 하나의 모델을 만드는걸 앙상블 학습이라고 함
	
6.모델 세부 튜닝
	1.그리드탐색
		가장 무식한방법은 괜찮은 값이 나올때까지 수동으로 하이퍼파라미터값 다넣어보는건데 누가 이렇게해
		
		보통 저렇게 하고싶으면 GridSearchCV를 사용함
		
		사용법은
		
		from sklearn.model_selection import GridSearchCV
		
		파라미터배열=[
			{'n_estimators':[3,10,30],'max_features':[2,4,6,8]},
			{'bootstrap':[False],n_estimators:[3,10],'max_features':[2,3,4]}
		]
		
		사용모델=모델()
		
		그리드서치=GridSearchCV(사용모델,파라미터배열,cv=5,scoring='neg_mean_squared_error',
							return_train_score=True)
		그리드서치.fit(총데이터,데이터레이블)
		
		이러면 파라미터 배열 위에있는 3x4개를 평가하고 두번째에서 부트스트랩을 false로 한다음 2x3개를 평가함
		총 18개를 5번씩 90번 훈련함
		
		여기서 만약 max_features:8 n_estimators:30 이렇게 최대값이 나왔으면 올라갈수록 점수가 향상될 가능성이 있으니까
		값을 올려서 다시돌려주는게좋음
		
		만약 최적의 추정기에 직접 접근하고싶으면
			그리드서치.best_estimator_
		하면 나옴
		
		평가점수는 
			그리드서치.cv_results_
		하면 나옴
		
	2.랜덤탐색
		랜덤으로 탐색하고 싶으면 RandomizedSearchCV를 사용하면 됨 GridSearchCV와 거의 똑같음
		특히 규제처럼 설정값이 연속형이면 랜덤탐색이 권장됨
		
		장점은 1000회반복하도록하면 파라미터마다 각기 다른 1000개의 값을 탐색함
		반복횟수를 조절하는거만으로 얼마나 오래돌릴건지를 제어하기 편함
	
	3.앙상블
		최상의 모델을 연결해보는것 최상의 단일모델보다 모델의 그룹이 더 나은 성능을 발휘할때가 많음
	
	4.최상의 모델 분석
	최상의 모델을 분석하면 좋은 특성등 좋은 통찰을 얻는 경우가 많음
	예를들어 
		그리드서치.best_estimator_.feature_importances_
	하면 특성의 상대적인 중요도가 나오는데 이걸 바탕으로 덜 중요한 특성을 제외할수있음
	
	5.테스트세트 사용하기
		이거도똑같이 예측변수와 레이블을 따고 풀파이프라인에 넣고(여기서 학습하면안되니까 fit_transform이 아니라 transform해야함)
		테스트세트에서 모델 평가하면됨
		
			파이널모델=그리드서치.best_estimator_
			
			테스트데이터=테스트세트.drop('레이블',axis)
			테스트레이블=테스트세트['레이블'].copy()
			
			테스트전처리데이터=풀파이프라인.transform(테스트데이터)
			
			파이널예측=파이널모델.predict(테스트전처리데이터)
			
			파이널mse=mean_squared_error(테스트레이블,파이널예측)
			파이널rmse=nq.sqrt(파이널mse)
		
		만약 이런 추정이 얼마나 정확한지 알고싶으면 scipy.stats.t.interval()을 써서 일반화오차 95%신뢰구간을 계산할수있음
		
			from scipy import stats
			신뢰구간=0.95
			제곱오차=(파이널예측-테스트레이블)**2
			np.sqrt(stats.t.interval(신뢰구간,len(제곱오차)-1,loc=제곱오차.mean(),scale=stats.sem(제곱오차))
			
		보통 하이퍼파라미터 튜닝을 많이하면 교차검증보다 성능이 조금낮은게 보통임
		그래도 테스트세트 점수올릴라고 하이퍼파라미터 튜닝하면 안됨 그러면 새로운데이터 일반화가 안됨
7.런칭 유지보수
	전체 전처리 파이프라인과 예측파이프라인이 포함된 사이킷런 모델을 joblib를 사용해서 저장하고,
	이거를 상용환경에서 로드하고 predict를 사용해서 예측을 만들면 됨
	웹을쓰든 뭘쓰든 저거만 연결시켜주고 rest api같은거든 뭐든 연결만 시켜주고 인풋아웃풋만 있으면 됨
	
	배포를 했으면 시스템의 성능을 체크하고 성능이 떨어졋을때 알림을 줄수있는 모니터링 코드를 만들어야함
	
	



3.분류
1.mnist
	mnist는 분류에서 제일 테스트하기 좋은 데이터셋
	받으려면
		from sklearn.datasets import fetch_openml
		mnist=fetch_openml('mnist_784',version=1)
	하면 받아짐
	보통 데이터셋의 구조는
	
		데이터셋을 설명하는 DESCR
		샘플이 있는 data
		레이블이 있는 target
	
	그리고 데이터갯수와 특성수를 확인하고싶으면
	mnist.shape
	하면 (데이터갯수,특성수)가 나옴
	
	이게 픽셀식이면 그려볼수도 있는데 그릴려면
	
		import matplotlib as mpl
		import matplotlib as plt
		
		x=mnist['data']
		y=mnist['target']
		
		샘플=x[0]
		샘플이미지=샘플.reshape(가로픽셀수,세로픽셀수)
		
		plt.imshow(샘플이미지,cmap='binary')
		plt.axos("off")
		plt.show()
	이러면 픽셀이 그려짐
	
	만약 레이블이 문자로 들어가 있으면,알고리즘은 숫자를 기대하기떄문에 정수로 바꿔줘야함
		y=y.astype(np.uint8)
	
	그리고 이제 테스트세트랑 트레인세트를 나눠야하는데,mnist는 이미 나눠뒀으니까 
	앞에 6만개 트레인으로 쓰고 뒤에 1만개 테스트세트로쓰면됨
		xtrain,xtest,ytrain,ytest=x[:60000],x[60000:],y[:60000],y[60000:]
	훈련세트는 이미 섞여있으니까 모든 교차검증폴드가 비슷하게 나옴
	그리고 어떤알고리즘들은 샘플의 순서에 민감해서 비슷한애들이 연이어 나오면 성능이 나빠짐,
	단 날씨나 주식같은 연속성있는애들은 당연히 섞으면안됨
	
2.간단한분류테스트(이진분류)	
	스팸처럼 true false 분류하는걸 이진분류기라고함
	
	확률적 경사 하강법(SGD)는 매우 큰 데이터셋을 효율적으로 처리하는 장점을 가지고있음(그래서 온라인학습에 잘맞음)
	사용법은
		from sklearn.linear_model import SGDClassifer
		
		sgd모델=SGDClassifer(random_state=시드값)
		sgd모델.fit(트레인세트,트레인레이블)
		
	그리고 사용할땐 predict쓰면됨
		sgd모델.predict([이미지])
	
3.성능측정
	1.교차검증
		분류를 cross_val_score로 교차검증하면 정확도가 막 95%이렇게 나오는데
		분류에서는 막 전부 트루주고 그래도 90퍼센트이상 나오는경우가 있기때문에,분류에서는 정확도를 성능측정지표로 잘 사용하지 않음
		특히 불균형한 데이터셋을 쓸때 특히 더 그럼
	2.오차 행렬
		그래서 교차검증말고 사용하는 방법이 오차행렬인데,오차행렬은 클래스a의 샘플이(5가)클래스b로(3으로) 분류된 횟수를 센 행렬임
		만약 5가 3으로 잘못분류된 횟수를 알고싶으면 5행3열을 보면 됨
		
		오차행렬 사용법은 
		
			from sklearn.model_selection import cross_val_predict
			
			예측값=cross_val_predict(모델,트레인세트,트레인레이블,cv=반복횟수) 
			
			from sklearn.metrics import confusion_matrix
			
			confusion_matrix(레이블,예측값)
		cross_val_predict는 cross_val_score처럼 k겹 교차 검증을 하지만 
		평가점수를 반환하지않고 각 테스트폴드에서 얻은 예측을 반환함,즉 3이 5로 예측된 수,5가 5로 예측된 수등을 묶어서 리턴함
		만약 tf만 있는걸 오차행렬로 만들면
			[진짜음성,가짜양성]
			[가짜음성,진짜양성]
		이렇게됨
		만 숫자분류기처럼 10개씩 있는건 왼쪽위부터 대각선이 제대로 들어간값이고 나머지가 오류값
		완벽한 분류기는 대각선을 제외한 모든게 0임
	3.정밀도와	재현율
		좀 더 간략한 지표는 
			양성예측의 정확도,즉 정밀도 ->정밀도= 진짜양성/(진짜양성+가짜양성)
			정확하게 감지한 양성샘플의 비율,즉 재현율->재현율=진짜양성/(진짜양성+가짜음성)
		을 사용함
		
		사이킷런에서 사용하려면
			from sklearn.metrics import precision_score,recall_score
			
			precision_score(레이블,예측값)  #정밀도
			recall_score(레이블,예측값)#재현율
			
		이거 두개를 묶은걸 f1점수라고 하는데 이건 정밀도와 재현율의 조화평균임
		
		사용법은
		
			from sklearn.metrics import f1_score
			f1_score(레이블,예측값)
		
		평균적으로 높은값을 뽑아내야하면 f1점수를 사용하면 되는데,상황에따라 정밀도가 더 중요할수도 있고 재현율이 더 중요할수도있음
		만약 애들유튜브 분류기라고하면 다른 이상한거 막 다 잡는다고 쳐도 최대한 정밀도를 높게 가져가야하고,
		도둑이미지 분석같은걸 하면 아무때나 막 알람 울려도 최대한 재현율 높게 가져가야하는거처럼 상황마다 다름
		
		둘다 높으면 좋겠지만,정밀도를 올리면 재현율이 줄고 재현율을 올리면 정밀도가 줄어듬
	
	4.정밀도재현율 트레이드오프
		
		보통 머신러닝에서 임계값을 잡아두고(이거 이상이면 양성)그걸 움직이면서 정밀도 재현율을 조절함
		사이킷런에선 임계값을 직접 정할수는 없지만 예측점수를 받아볼수는 있음
			임계값점수=sgd모델.decision_function([샘플])
			임계값점수
			
			결과::: array([2412.2434])
			
		저렇게 받아서 후처리하는식으로 조절할수는있음
			임계값세팅=8000
			임계값후처리=(임계값점수>임계값세팅)
			
			결과:::array([False])
		이런식으로 받아볼수있음
		
		우리가 적절한 임계값을 정하려면,cross_val_predict로 모든 샘플의 점수를 구해야하는데,
		이번엔 예측결과가 아니라 결정점수를 받아야함
		
			결정점수=cross_val_predict(sgd모델,트레인세트,트레인레이블,cv=3,method='decision_function')
			#method='decision_function'하면 결정점수 리턴함
		
		이 점수로 precision_recall_curve()함수를 사용하면 모든 임계값에 대한 정밀도 재현율이 나옴
			from sklearn.metrics import precision_recall_curve
			
			정밀도,재현율,임계값=precision_recall_curve(레이블,결정점수)
			
		이제 임계값으로 정밀도 재현율 그래프를 그릴수있음
			def 정밀도재현율그래프(정밀도,재현율,임계값):
				plt.plot(임계값,정밀도[:-1],'b--')
				plt.plot(임계값,재현율[:-1],'g-')
				
			정밀도재현율그래프(정밀도,재현율,임계값)
			plt.show()
			
			
		tmi)(또 다른 방법은 재현율에 대한 정밀도 곡선을 그리는거임,
			그러면 average_precision_score()를 써서 곡선의 아래면적을 계산할수있어서 
			다른 두 모델을 비교하는데 도움이됨)
			
		보면 그래프가 급격하게 꺾이는점이 있는데 그거 직전을 트레이드오프로 선택하는게 좋음
		
		물론 프로젝트성격따라 달라지는데 만약 정밀도 90% 달성하는게 목표면 
			임계값90퍼=임계값[np.argmax(정밀도>=0.90)] 
		하면 최대값의 첫번째 인덱스를(가장 재현율이 높은)반환함
		
		트레인세트에 대한 예측을 만들려면 분류기의 predict를 쓰는게 아니라 
			y트레인90퍼=(예측점수>=임계값90퍼)
		하면됨
		
		근데 막 정밀도 높이면 재현율이 엄청낮아지니까 잘생각해야함
		
	5.ROC곡선
		ROC곡선은 가짜양성에 대한 진짜양성의 그래프임
		이건 1에서 진짜음성을 뺀값과 같음(1-진짜음성을 특이도라고 부름)
		
		즉,roc곡선은 민감도에 대한 1-특이도 그래프임
		
		
		roc곡선을 그리려면 먼저 roc_curve()를 사용해서 임계값에서 재현율(tpr),특이도(tnr)을 계산해야함
		
			from sklearn.metrics import roc_curve
			
			재현율,특이도,임계값=roc_curve(레이블,예측점수)
		
		그리고나서 맷플롯립으로 tpr에대한 fpr곡선을 그리면됨
		
			def roc커브그래프(재현율,특이도,label=none):
				plt.plot(재현율,특이도,linewidth=2,label=label)
				plt.plot([0,1],[0,1],'k--')#대각선,완전랜덤분류기임
		
			roc커브그래프(fpr,tpr)
			plt.show()
		
		여기서도 트레이드오프가 있는데,재현율이 높을수록 분류기가 만드는 거짓양성(fpr)이 늘어남
		좋은 분류기는 랜덤분류기(대각선)에서 최대한 멀리 떨어져있어야함
		
		곡선 아래 면적(AUC)을 측정하면 분류기를 비교할수있음,완벽한 분류기는 1이고 완전한 랜덤분류기는 0.5임
		사이킷런에서 계산하려면
			from sklearn.metrics import roc_auc_score
			
			roc_auc_score(레이블,예측점수)
		하면됨
		
		정밀도재현율과 roc곡선중 어떤때 뭘 선택하냐면
		양성클래스가 드물거나,거짓음성보다 거짓양성이 중요하면 정밀도재현율을 쓰고,그렇지않으면 ROC를 사용함
		
		RandomForestClassifer에서 predict_proba()메서드는 샘플이 행 클래스가 열이고,
		샘플이 주어진 클래스에 속할 확률을 담은 배열을 리턴함(어떤 이미지가 5일 확률 70% 이런식)

4.다중분류		
	다중분류는 이중분류랑 다르게 둘 이상의 클래스를(숫자를 구별한다던지)구별할수있음
	알고리즘마다 여러개를 처리할수 있는 알고리즘(sgd,랜덤포레스트,나이브베이즈)들이 있고,
	이진분류만 가능한 알고리즘(로지스틱회귀,서포트벡터머신)이 있음
	하지만 이진분류기 여러개를 써서 다중클래스를 분류하는 기법도 많음
	
	만약 특성숫자 하나만 구분하는 숫자 이진분류기가 10개면 0부터 10까지 이미지분류를 할수있음,
	이미지 분류할때 가장 점수높은걸 선택하면됨,이런방식을 OvR(혹은 OvA)라고 함
	
	또 다른 방법은 0과 1구별,0과 2구별,1과 2구별 이런식으로 조합마다 이진분류기를 훈련시키는것,이걸 OvO라고 함
	클래스가 N개면 Nx(N-1)/2개가 필요함
	mnist에서 이미지 하나를 분류하려면 45개((10x9)/2)를 통과시켜서 가장 많이 양성으로 분류된 클래스를 선택함
	OvO의 가장 큰 장점은 각 분류기의 훈련에 전체 훈련세트중에서 구별할 두 클래스에 해당하는 샘플만 필요하다는것
	
	일부 알고리즘은 훈련세트크기에 민감해서 큰 훈련세트에서 몇개의 분류기를 훈련시키는거보다,작은훈련세트에서 많은 분류기를 훈련시키는게
	더 빨라서  OvO를 선호함,하지만 대부분은 OvR을 선호함
	
	분류기에서 점수=svm모델.decision_function([샘플])
	하면 분류기 클래스마다 점수를 보여줌,이 점수중 가장 높은점수를 리턴

	사이킷런에서 OvO나 OvR을 사용하도록 강제하려면 OneVsOneClassifier이나 OneVsRestClassifier을 사용하면됨
	sveone=OneVsOneClassifier(SVC())이런식으로 안에다가 알고리즘넣는식

	그리고 2장에서도 그랬지만,입력의 스케일을 조정하면(StandardScaler를써서) 정확도가 올라감
	
5.에러분석
	모델의 성능을 향상시키는 한가지 방법은 에러의 종류를 분석하는거임
	첫번째로,오차행렬을 살펴볼수있음
	아까처럼 cross_val_predict로 예측을 만들고,confusion_matrix로 오차행렬을 만들수있음
	
	보기편하게하려면
		plt.matshow(오차행렬,cmap=plt.cm.gray)
		plt.show()
	하면 오차행렬이 색으로 나옴 밝으면 많이들어갔다는거

	에러부분에 초점을 맞추려면 오차행렬의 각 값을 대응되는 클래스의 이미지갯수로(에러갯수가아님)나눠서 에러비율을 비교(갯수로하면 이미지많은
	애들이 나쁘게보임)
	
		행의합=오차행렬.max(axis=1,keepdims=True)
		나눈값=오차행렬/행의합

	여기서 주대각선만 0으로 채워서 그래프그리면됨
		np.fill_diagonal(나눈값,0)
		plt.matshow(나눈값,cmap=plt.cm.gray)
		plt.show()

	열이 밝으면 그거로 잘못 분류된 이미지가 많다는소리,오차행렬은 반드시 대칭인것은 아님
	
	오차행렬을 분석하면 성능향상방안에 대한 통찰을 얻을수있음,만약 8로 잘못분류되는게 많다면 8처럼보이지만 8이 아닌 훈련데이터를 구해서
	분류기를 학습시킬수도 있고,8분류에 도움이 될만한 특성을 추가할수도있음

6.다중레이블분류
	분류기가 샘플마다 여러개의 클래스를 출력해야할수도있음(얼굴인식 분류기면 사진마다 사람수만큼 [0,1,1,0...]이런식으로)
	이렇게 여러개의 이진태그를 분류하는 시스템을 다중레이블분류라고 함
	
	다중레이블분류를 평가하는 방법은 많지만 가장 편한건 f1점수를 모든 레이블에 대해 평균낸값임
	
	근데 다중레이블 분류에서 단순히 그냥평균내면 더 많이나온애에 대한 점수에 높은 가중치가 붙으니까,
	타깃레이블에 속한 샘플수(지지도)를 가중치로 주면(average='weighted'넣으면됨)됨
	
7.다중출력분류
	
	다중출력분류는 다중출력 다중클래스분류임,즉 다중레이블분류에서 한 레이블이 값을 두개이상 가질수 있도록 일반화한것
	예를들어 이미지에서 잡음을 제거하는 시스템이 있음,잡음이 많은 이미지를 입력으로 받고,깨끗한이미지를 픽셀의 강도를 담은 배열로 출력할때
	분류기의 출력이 다중레이블(픽셀당 한 레이블이 모인 배열)이므로 다중출력분류임




4.모델훈련

1.선형회귀
	보통 선형회귀모델은 (x_0은 1)y=a_0x_0 + a_1x_1+...+a_nx_n  이런식으로 y=n+mx식을 띄게됨
	즉,입력특성의 가중치(특성과 가중치의 곱)의 합과 편향(절편)상수를 더해서 만들어짐
	
	모델을 훈련시킨다는건 훈련세트에 가장 잘 맞도록 모델 파라미터(가중치와 편향)를 설정하는것
	선형회귀에서 가장 널리사용되는 성능측정지표는 RMSE,그렇지만 MSE가 같은결과를 내면서 간단해서 MSE씀
		MSE=모든샘플x( (가중치x특성)-레이블 )/전체샘플갯수
		
	즉 모든샘플의 각각모든 특성에 특성에 따른 가중치를 씌워서 레이블이랑 비교한후,총합해서 가장 값이 작아지는 가중치를 찾으면됨
	
	1.정규방정식
		저 비용함수를 최소화하는 값을 찾는 수학공식이 있긴한데,그걸 정규방정식이라고 함
		근데 잘안씀 쓰기도힘들고 코스트도 비싸서
		
		대충 O(n^2.8)쯤 되는거같음 역행렬쳐서 뭐시기뭐시기하는데 넘어갈래  쓰지도않는거같고
	
2.경사하강법(GD)
	경사하강법은 여러문제에서 최적의해법을 찾을수있는 일반적인 최적화 알고리즘임
	경사하강법은 비용함수를 최소화 하기위해 반복해서 파라미터를 조정해가는것
		
	즉,현재 파라미터에 대해 비용함수의 현재 기울기를 계산하고,
	기울기가 감소하는 방향으로 진행하다 기울기가 0이되면 최소값에 도달한것
	
	구체적으로는 임의의 값으로 시작해서(무작위 초기화),
	한번에 조금씩(학습률에따라)비용함수가 감소하는 방향으로 진행해서,
	알고리즘이 최소값에 수렴할때까지 점진적으로 향상시킴
	
	경사하강법에서 중요한 파라미터는 스텝(한번에 얼마나 파라미터를 바꿀지)의 크기로,학습률 하이퍼파라미터로 결정됨
	만약 학습률이 너무 작으면 알고리즘이 수렴하기위해 반복을 많이해야해서 시간이 오래걸림
	하지만 학습률이 너무 크면 그래프의 반대편으로 건너뛰어서 이전보다 더 높은곳으로 올라갈수도 있음,이러면 발산돼서 답이 안나오게됨 
	
	모든 비용함수가 2차함수그래프는 아니라서 막 기울기가 0이되는게 2군데이상 있거나,중간에 평평한곳이 있거나 그럴수 있는데,
	그러면 최소값으로 수렴하기 매우 어려워짐
	만약 지역최소값이 있고 그쪽으로 접근하면 지역최소값에 걸려서 전역최소값을 찾을수없어지고,
	평지에 걸리면 평지를 지나는데 시간이 오래걸리고 일찍 멈춰서 전역최소값에 도달할수없어짐
	
	그래도 선형회귀에서의 mse함수는 볼록함수(2차함수그래프)라서 지역최소값이 없고,하나의 전역최소값만 있고,연속된함수에,기울기가 변하지않음
	즉,학습률이 너무 높지않고 충분한 시간이 주어졌을때 경사하강법이 전역최소값에 가깝게 접근할수 있다는걸 보장함
	
	그리고 경사하강법은 특성들의 스케일을 맞춰줘야함
	특성들의 스케일이 다르면 시간이 많이 오래걸림(StandardScaler사용)

	1.배치경사하강법
		배치경사하강법은 매 스탭마다 훈련 데이터 전체를 사용해서 계산
		즉, 큰 훈련세트에서는 엄청 느려짐,하지만 특성수에는 그렇게 민감하지 않아서 특성이 수십만개면 경사하강법쓰는게좋음
		
		경사하강법을 구현하려면 각 모델파라미터에 대해 비용함수의 기울기를 계산해야함
		즉, 모델파라미터가 조금 바뀔때 비용함수가 얼마나 바뀌는지를 계산해야함 이걸 편도함수 라고 함
		이걸 모든 파라미터에 대해서 계산함
		
		경사하강법은 기울기가 0이되는 지점을 찾는거기때문에, 현재기울기가 양수면 왼쪽으로 가야하고,음수면 오른쪽으로 가야하니
				다음스탭=모델파라미터-(학습률x기울기벡터xMSE )
				그냥 일반적으로 가볍게보면
				x다음위치=x현재위치-(학습률x기울기값)
				기울기값은 최소값에 가까워질수록 줄어드니 처음엔 크게움직이고,나중엔 조금움직이는거에도 적합함
		해야함
		기울기의 반대로 가야하니 현재모델파라미터에서 빼준것
		
		경사하강법에서 반복횟수는 그리드식으로 찾을수 있지만 너무오래걸리면,
		반복횟수를 엄청크게 잡고 벡터값이 허용수치보다 작아지면 최소값에 도달했다치고 알고리즘을 종료시키면됨
	
	2.확률경사하강법
		배치경사하강법의 가장 큰 문제는 매스탭마다 전체훈련데이터를 사용해야해서 엄청느리다는것임
		그래서 확률경사하강법이라는것도 있는데,이건 매 스탭마다 하나의샘플을 무작위로 선택하고,
		그 하나의 샘플에 대한 기울기를 계산함
		이건 매 반복마다 다뤄야할 데이터가 매우 적기때문에 훨씬빠르고,
		매반복에서 하나의샘플만 있으면되니까 훈련세트가 아무리커도 상관없음
		
		하지만 확률적이기떄문에 배치보다 훨씬 불안정하고,비용함수가 최소값에 다다를때까지 위아래로 요동치면서 평균적으로 감소함
		시간이 지나면 최소값에 매우 근접하겠지만,최소값에 안착하지는 못함
		
		그리고 지역최소값이랑 전역최소값이 있으면 랜덤으로 배치하기때문에 
		확률적 경사하강법이 배치경사하강법보다 전역최소값찾을 가능성이 높음

		즉,전역최소값에 정확히 일치시킬수는 없지만 지역최소값을 피할확률은 높음
		
		이걸 해결하려면 학습률을 점진적으로 감소시키는방법을 생각해볼수있음
		
		시작할떄는 학습률을 크게하고(수렴을 빠르게하고 지역최소값에 빠지지않게함),점차 줄여서 전역최소값에 도달하게 함
		
		이렇게 매 반복에서  학습률을 결정하는 함수를 학습 스케줄 이라고 함
		학습률이 너무빨리 줄어들면 지역최소값에 갇히거나 최소값가기전에 멈출수있고,
		학습률이 너무 천천히 줄어들면 최소값 주변을 맴돌거나 훈련이 너무빨리끝나서 지역최소값에 머무를수있음
		
		확률경사하강법은 일반적으로 한반복에서 훈련세트샘플수만큼 되풀이되고,각 반복을 에포크 라고 함
		
		샘플을 무작위로 선택하기때문에 어떤샘플은 한 에포크에서 여러번 선택될수있고,어떤샘플은 전혀 선택되지못할수도있음
		
		사이킷런에서 확률경사하강법을 쓰려면 SGDRegressor을 쓰면됨
		
	3.미니배치경사하강법
		미니배치경사하강법은 전체훈련세트나 하나의 샘플이 아닌,미니배치라고 부르는 임의의 작은 샘플 세트에 대해 기울기를 계산함
		미니배치의 장점은 gpu를 사용해서 얻는 성능 향상임
		
		미니배치를 어느정도 크게하면,sgd(확률경사하강법)보다 덜 불규칙적으로 움직이고 sgd보다 최소값에 더 가까이 도달함
		하지만 지역최소값에서 빠져나오기는 더 힘들수도있음
	4.알고리즘비교
		선형회귀에서 알고리즘비교
		
		알고리즘명   샘플이클때   외부메모리학습   특성수가많을때   하이퍼파라미터수   스케일조정필요   사이킷런
		정규방정식    빠름         no        느림           0           no         없음 
		SVD       빠름         no        느림           0           no       LinearRegression
		배치경사     느림         no        빠름           2           yes      SGDRegressor
		확률적경사    빠름        yes        빠름           >=2         yes      SGDRegressor
		미니배치경사   빠름        yes        빠름           >=2         yes      SGDRegressor
		
		이알고리즘들은 훈련결과에 차이가 없고,매우 비슷한모델을 만들고 같은방식으로 예측을 함
		
3.다항회귀
	가진 데이터가 직선보다 복잡한 형태라도,즉 비선형데이터라도 학습하는데 선형모델을 사용할수 있음
	제일 간단한 방법은 각 특성의 거듭제곱을 새 특성으로 추가하고,이 특성을 포함한 데이터셋에 선형모델을 훈련시키는것
	이런걸 다항회귀라고 함(즉 제곱을 넣어서 제곱으로 선을 그리니 직선이 아닌 n차함수그래프를 만들수있음)
	
	거듭제곱을 추가하는법은
		from sklearn.preprocessing import PolynomialFeatures
		
		poly=PolynomialFeatures(degree=2(차수),include_bias=False(편향을 위한 변수 제거(상수제거)))
		xpoly=poly.fit_transform(x)
	이러면 원래특성과,특성의 제곱이 포함됨
	
	이걸 그냥 LinearRegression으로 돌리면
		linreg=LinearRegression()
		linreg.fit(xpoly,y)
	이러면 곡선형태의 그래프가 나오는데(직선이 아닌)
	이렇게 할수있는 이유는,
	PolynomialFeatures가 특성 a,b가 있고 차수가 3일때 
	a^2,b^2,a^3,b^3만이 아닌 ab,a^2b,ab^2등 교차항을 추가하기떄문
	그래서 서로간의 관계에 따른 추세도 그래프에 반영시킬수 있음
	
	보통 그래프가 2차함수그래프면 차수도 2에맞춰주는게좋고,3차함수면 3에맞춰주는게 좋음
	
	그리고 PolynomialFeatures를 쓸때 특성이 많으면 엄청나게 늘어나니까 조심해야함
4.학습곡선
	물론 몇차함수로 생성된 그래프인지 알긴 어렵고,고차 다항모델을 쓰면 훈련데이터엔 잘 맞지만,과대적합돼버림
	그래서 좀 높은 차수를 쓴다음에 규제하거나 이런식으로 하는듯
	
	모델의 성능을 추정할때 교차검증을 쓸수도있지만(훈련엔 좋지만 교차검증점수가 나쁘면 과대적합,둘다나쁘면 과소적합)
	학습곡선을 쓸수도 있음
	학습곡선은 훈련세트와 검증세트의 모델 성능을 훈련세트크기나 훈련반복의 함수로 나타냄,
	즉 rmse같은 평가함수를 y 세트반복수를 x로 잡고 그래프를 그림
	
	학습곡선에서 과소적합된 그래프는 훈련데이터의 rmse가 금방금방 상승해서 
	평평해질떄까지 시간이 얼마 안걸리고,평균오차가 크게 나아지지않음
	검증세트에서도  초창기에 엄청 크다가 금방 내려와서 훈련데이터와 비슷하게 평행해서 나감
	
	역시 과소적합되면 금방금방 붙긴하는데 그냥 에러치 자체가 높은게 문제임
	
	학습곡선에서 과대적합된 그래프는 훈련데이터의 rmse가 처음엔 아예 0이다가(차수까지는) 서서히 올라가고,
	검증세트에서는 초창기는 엄청나게 큰데,훈련데이터가 쌓일수록 내려와서, 훈련데이터가 많이 쌓였을땐 과소적합보다 오차가 훨씬 낮고
	데이터가 많아질수록 훈련세트와 검증세트의 선이 붙음
	
	
	tmi)오차는 총 3가지가 있는데
		편향(절편과다름):잘못된 가정으로 인한 오차
			데이터가 2차인데 선형으로 가정한다던지 할경우
			편향이 큰 모델은 과소적합되기 쉬움
		분산:훈련데이터의 작은 변동에 모델이 과도하게 민감해서 나타남
			자유도가 높은(차수가높은)모델이 분산이 높기쉬워서 데이터에 과대적합되는 경향이 있음
		줄일수없는오차:데이터 자체에 있는 잡음,
				  고치는법은 데이터에서 잡음을 제거하는거밖에없음


5.규제가 있는 선형모델
	과대적합을 줄이는 좋은방법은 모델을 규제하는것,
	다항회귀모델을 규제하는 간단한방법은 다항식의 차수를 감소시키는것
	대부분 규제가 있는 모델은 스케일에 민감하기때문에 스케일을 맞춰줘야함
	
	선형회귀모델에서는 보통 모델의 가중치를 제한해서 규제를 가함
	
	1.릿지 회귀
		릿지회귀는 규제가 추가된 선형회귀의 한 버전임
		규제항 a가 비용함수에 추가되는데(a*모델파라미터),
		이는 학습 알고리즘을 데이터에 맞추는것뿐만 아니라 모델의 가중치가 가능한 작게 유지되도록 노력함
		
		식은 가중치벡터의L2노름(벡터 두개의 직선거리)의 제곱을 2로 나누면 되는데 뭐 신경쓰지말고 대충이해하자
		
		규제항은 훈련하는 동안에만 비용함수에 추가되고,훈련이 끝나면 모델의 성능을 규제가 없는 지표로 평가함(보통 훈련할때쓰는 
		비용함수와 테스트에서 사용되는 성능지표는 다름,훈련에 사용되는 비용함수는 최적화를위해 미분가능해야하고,
		테스트에 사용되는 성능지표는 최종목표에 최대한 가까워야 하기 때문)
		
		a는 모델을 얼마나 많이 규제할지 조절함,a가 0이면 선형회귀랑 같고,
		a가 엄청크면 모든 가중치가 0이되고 데이터의 평균을 지나는 수평선이 됨
		즉,a가 커지면 오차에서 분산이 줄고 편향이 커짐(과소적합이늘고 과대적합이 줄어듬)
		
		그리고 편향(절편)은 규제되지않음 그러니까 y=절편+a모델파라미터 꼴이 됨
		
		확률적경사하강법에서 릿지회귀를 쓰는법은
			sgdreg=SGDRegressor(penalty='l2'(엘투))
			하고 그대로 fit해서 진행하면됨
	
	2.라쏘회귀
		선형회귀의 다른 규제버전임
		얘도 릿지회귀처럼 비용함수에 규제항을 더하지만 L1노름(각 파라미터의 차이의 절대값의 합)을 사용함 
		ex(p=(3,1,-3) q=(5,0,7)이면
		3-5 1-0 -3-7 = 2+1+10=13)
		
		라쏘회귀의 중요한 특징은 덜 중요한 특징의 가중치를 제거하려고 한다는점(가중치가 0이됨)
		즉,라쏘회귀는 자동으로 특성선택을 하고 희소모델을 만듬(즉 0이아닌 특성의 가중치가 작음)
		
		라쏘회귀(L1)는 파라미터1이 2 파라미터 2가 0.5일때,
		[1.9 0.4],[1.8 0.3] 이렇게 선형적으로 줄다가 
		[1.5 0.0]이되면 파라미터1을 0으로 보내는식으로 작동한다면(같은수치만큼 줄어듬)
		
		릿지회귀(L2)는 파라미터1이 2 파라미터 2가 0.5일때, 30%라고하면
		[1.643 0.35123], [1.2231 0.221]... [0.002 0.001] 
		이렇게 두 벡터의 직선으로 가지만 0에는 도달할수없고,
		처음엔 줄어드는값이 크지만 점점 줄어드는값이 작아짐,즉 경사하강법이 자동으로 느려지고 수렴에 도움이 됨
		그리고 a가 커질수록 원점에 가까워짐
		
		(라쏘를 쓸때 최적점 근처에서 진동하는걸 막으려면 훈련하는동안 점진적으로 학습률을 감소시키면 스텝이 작아지니까 결국 수렴하게됨)
		
		라쏘의 비용함수는 파라미터가 0일때 미분가능하지 않지만,
		서브그레이언트 벡터(대충 근처값 긁어서 중간값)를 사용하면 경사하강법을 적용할수있음
		
	3.엘라스틱넷
		엘라스틱넷은 릿지와 라쏘를 절충한 모델임
		단순히 릿지와 라쏘의 규제항을 더해서 사용하고,혼합비율 r을 조절해서 사용함
		r=0이면 릿지회귀고 r=1이면 라쏘회귀와 같음
		
		보통 규제중에서 뭘고르냐면 
		보통 규제가 약간이라도 있는게 좋아서 평범한 선형회귀는 쓰지않고,릿지가 기본이지만 사용할 특성이 몇개밖에 없다고 생각되면,
		랏쏘나 엘라스틱넷이 나음(불필요한특성의 가중치를 0으로 만들어주기때문)
		특성수가 훈련샘플수보다 많거나 특성 몇개가 강하게 연관되어있으면 라쏘가 문제를 일으키니까 엘라스틱넷을 선호함
		
		엘라스틱넷을 사용하는법은
			from sklearn.linear_model import ElasticNet
			
			eleast=ElasticNet(alpha=0.1(알파값),L1_ratio=0.5(혼합비율))
			eleast.fit(데이터,레이블)
			
	4.조기종료
		경사하강법같은 반복적인 학습 알고리즘을 규제하는 다른방식은 검증에러가 최솟값에 도달하면 바로 훈련을 중지시키는것
		이걸 조기종료라고 부름
		
		간단히 매번 RMSE 최저값이랑 세팅 저장한다음에 현재값이 그거보다 커지면 되돌리기한다음에 그거리턴하고 종료하면됨
		
6.로지스틱 회귀
	로지스틱 회귀는 분류에서 사용할수 있는 회귀임
	작동방식은 샘플이 특정 클래스에 속할 확률을 추정함(이메일이 스팸일 확률)
	보통 스팸처럼 0,1만 있는 분류기를 이진분류기라고함
	
	1.확률추정
		로지스틱회귀는 선형회귀와 같이 입력특성의 가중치합을 계산하고 편향을 더함,대신 바로 결과를 출력하지않고
		결과값의 로지스틱(0과 1사이값을 출력하는 시그모이드함수)을 출력함(1/1+exp(-t)) 
		exp는 대충 0~1만들기위한 자연로그가지고한거 t는 logodd(양성클래스확률과 음성클래스확률사이의 로그비율) 몰라도됨 
		
		그러면 y가 1부터0까지고 파라미터에 따라 1에서 0사이 어딘가에 있는 함수가 나오는데,
		임계값을 넘기면(기본임계값은 0.5) 양성,넘기지못하면 음성 이런식으로 예측할수있음
	2.훈련과 비용함수		
		즉 우리는 로지스틱회귀에서 양성샘플은 높은확률을 주고 음성샘플은 낮은확률을 주는 파라미터벡터를 찾는것
	
		로지스틱회귀의 비용함수는 로그손실이라고 부르는데,이건 따로 최소값 계산하는 해가 없는데,볼록함수니까 경사하강법을 쓰면
		전역최소값을 찾는걸 보장함
		
	3.결정 경계
		로지스틱회귀는 특성을 기반으로 선을 긋고(절편+파라미터1*특성1+파라미터2*특성2...)=0을 만족하는 x의 집합) 
		그선을 넘으면 15퍼센트로 스팸이다 50퍼센트로 스팸이다 이런식으로 함
		
		다른 선형모델처럼 로지스틱회귀도 L1 L2패널티를 써서 규제할수있음,사이킷런은 L2가 기본값임
		(사이킷런의 LogisticRegression의 규제강도는 alpha가 아니아 그 역수인 C임 C가 높으면 모델의 규제가 줄어듬)
	4.소프트맥스 회귀
		로지스틱회귀는 여러개의 이진분류기를 연결하지않고 직접 다중클래스를 지원하도록 일반화할수있음, 이를 소프트맥스회귀라고함
		
		개념은 샘플이 주어지면 소프트맥스모델이 각 클래스에 대한 점수를 계산하고,그 점수를 소프트맥스함수에 넣어서 각 클래스의 확률을
		추정함
		
		(각 클래스는 자신만의 파라미터벡터가 있는데 이벡터들은 파라미터행렬에 행으로 저장됨)
		
		소프트맥스 함수는 각 점수에 지수함수를 적용한후 정규화(모든지수함수 결과의 합으로 나눔)함
		보통 저 점수를 로그오즈(로짓)라고  부름
		
		즉 점수뽑아서 함수에넣어서 모든클래스에 대한 확률로 변형한뒤에 가장 확률이 높은 클래스를 선택함(다중클래스지 다중출력이 아님)
		
		그러니까 타깃클래스에 대해서는 높은확률,다른클래스에 대해서 낮은확률을 추정하도록 만드는게 훈련의 목적임
		여기엔 크로스엔트로피 비용함수를 잘 쓰는데,
		크로스엔트로피는 추정된 클래스의 확률이 타깃클래스에 얼마나 잘 맞는지 측정하는 용도로 잘 사용됨
		
		사이킷런의 LogisticRegression은 클래스가 둘 이상일떄 기본적으로 OvA를 쓰는데 
		multi_class='multinomial'을 넣어주면 소프트맥스 회귀를 쓸수있음
		
		소프트맥스 회귀를 쓰려면 solver매개변수에 'lbfgs'같은 소프트맥스회귀를 지원하는 알고리즘을 지정해야함
		그리고 기본적으로 C를 사용해서 조절할수있는 L2규제가 적용됨
		
		주의점은 추정경계에 가까울수록 확률이 별차이없는걸 예측할수도 있음(3개일떄 34퍼센트인걸 리턴할수있음)
	

	
5.서포트벡터머신	
1.선형svm분류
	
	서포트벡터머신(svm)은 클래스들 사이에 가장 폭이 넓은 도로를 찾는 일
	각각 클래스중 가장 가까운걸 택해서 그 두개를 기준으로 선을 긋고 그거로 나눔(라지마진분류)
	그래서 가장 가까운애한테만 영향을 받지,막 3만개가 더들어온다해도 경계선이 갱신되는게 아니면 전혀 영향을 주지않음
	즉,경계에 있는 애들한테만 영향을 받고 이런애들을 서포트 벡터라고 함
	
	svm은 특성의 스케일에 매우 민감해서,스케일조정을 해줘야함
	
	만약 모든 샘플이 바로 선으로 나눌수있게 분류 되어있으면 이걸 하드 마진 분류라고 함
	하드마진 분류의 문제는 데이터가 선형적으로 구분할수있어야하고,이상치에 매우민감함
	
	만약 상대클래스쪽에 이상치로 한개가 섞여있으면 하드마진분류가 불가능하고(선을 그을수가없음)
	그정도는 아니더라도 상대클래스에 엄청붙어있으면 도로폭이 좁아지고 그래서 일반화가 잘 되지않음
	
	그래서 도로의폭과 마진오류의 트레이드오프를 해서 한두개는 상대클래스로 넘어가도 무시하고 막 도로건너편에 있어도 무시하는걸
	소프트 마진 분류 라고 함
	
	사이킷런에서는 하이퍼파라미터 C를 조절하면 마진오류허용치를 조절할수있음
	
	사이킷런에서 쓰려면 
		LinerSVC(C=1(마진오류허용치),loss="hinge"(힌지손실))
	하면됨
	
	svm은 로지스틱회귀와 다르게 클래스에 대한 확률을 제공하지않음
	
2.비선형 svm 분류
	비선형이면 일반적으론 분류를 할수없지만,다항회귀처럼 다항특성을 추가하면(x^2라던지) 선형적으로 구분되는 데이터셋이 만들어질수있음
	
	사이킷런에선 PolynomialFeatures에 차수넣고 StandardScaler LinerSVC에 c,loss넣은거 
	파이프라인으로 연결해서 넣으면됨
	
	1.다항식 커널
		다항특성을 추가하는건 쉽고 모든알고리즘에서 다 잘 동작하지만,낮은차수의 다항식은 안나올확률이 높고,높은차수는 엄청느려짐
		
		근데 svm은 커널트릭이라는 사기스킬로(데이터를 고차원으로 매핑해서 차이를 찾음)
		특성을 추가하지않고 다항식을 많이 넣은것과 같은 결과를 얻을수있음
		
		사용법은
			SVC(kernel='poly'(무슨커널쓸건지),degree=3(차수),coef0=1(높은차수 낮은차수 얼마나 영향받을지),C=5)
		모델이 과소적합이면 차수를 늘려야하고,과대적합이면 차수를 줄여야함
	2.유사도 특성
		또 다른 방법은 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도함수로 계산한 특성을 추가하는것
		이 함수는 0(랜드마크와 멀리떨어진경우)부터 1(랜드마크와 같은위치)까지 변화하며 종모양으로 나타남
	
		계산법은 랜드마크와 얼마나 떨어져있는지를 변수로잡고 유사도함수에 집어넣으면 랜드마크와 샘플이 얼마나 떨어져있는지 값이 나오는데
		이걸 토대로 선그어서 구분하는방식
		
		랜드마크값은 제일쉬운방법은 데이터셋에 있는 모든샘플위치에 랜드마크 설정하는것
		이러면 차원이 매우커지고,그래서 변환된 훈련세트가 선형적으로 구분될 가능성이 높음
		단점은 n개의 특성을가진 m개의 샘플이 m개의 특성을 가진 m개의 샘플로 변환된다는것(원본샘플제외했을때)
		즉 훈련세트가 크면 동일한크기의 아주 많은 특성이 만들어짐
		
	3.가우시안 RBF커널
	유사도도 연산비용이 많이드는데 그냥 커널트릭으로 사기칠수있음
	SVC(kernel='rbf',gamma=5(증가하면 종이 좁아져서 샘플의 영향범위가 줄어듬,즉 규제 과소면 증가 과대면 감소),C=0.001) 
	
	커널 선택은 linear을 제일 먼저 해봐야하고,훈련세트가 그렇게 크지않으면 가우시안RBF하면됨
	
	4.계산복잡도
		커널트릭을 안쓸거면 LinerSVC를 씀(훈련샘플과 특성수에 수행시간이 선형적으로 증가 O(m*n)))
		정밀도를 높이면 알고리즘 수행시간이 길어짐(하이퍼파라미터 tol)보통은 기본값두면 잘 작동함
		
		SVC는 커널트릭을 쓸때 쓰는데,시간복잡도가 O(m^2*n)이라서 훈련샘플수가 커지면 엄청나게 느려짐,
		하지만 특성은, 특히 희소특성(0이아닌특성이 몇개없는경우)에는 잘 확장됨
	
3.svm회귀
	svm은 회귀로도 쓸수있는데 회귀는 목표를 반대로 하면 됨
	즉 도로 안에 가능한 많은 샘플이 들어가게 학습시키면됨
	이거도 마진안에서는 훈련샘플이 추가되어도 예측엔 영향을 주지않음
	
	사용법은 
		svmreg=LinearSVR(epsilon=1.5(도로의폭),tol=0.001(허용오차))
	SVR은 SVC의 회귀버전 LinearSVR은 LinearSVC의 회귀버전
	
4.SVM이론
	svm은 초평면(n+1차원의 평면)인 결정함수(초평면과 데이터셋평면이 교차하는부분(결정함수의 값이 0인지점이 결정경계))
		
	커널트릭은 벡터곱을 하면 차원이 하나 더생기는것((a,b)*(a,b)=a^2 ab b^2 )을 이용한것
		


6.결정 트리		
	결정트리는 조건별로(a>10) 분기하는식으로 추론하는 모델임(스무고개같은방식)
	결정트리는 분류 회귀 다중출력 작업도 가능한 알고리즘임
	1.시각화
		 export_graphviz를 쓰면 결정트리 시각화를 할수있음
		 
		 from sklearn.tree import export_graphviz
		 
		 export_graphviz(트리모델,out_file=이미지위치,
						 feature_names=특성명(x[0]대신에 특성이름넣어줌),class_name=이게무슨클래스인지 결과값)
			
	2.예측하기
		결정트리는 루트에서 (a특성>2)같은거로 거르고,밑에서 (a특성>1)로 거르고,그밑에서 (b특성<3)으로 거르는식으로 스무고개하듯이
		분류하는 모델임(아키네이터 생각하면됨)
		
		시각화했을때 노드의 sample 속성은 훈련모델에서 얼마나 많은 훈련샘플이 지나갔는지를 센거고,
		노드의 value속성은 각 클래스마다 얼마나 많은 샘플이 있는지 적혀있고,
		gini속성은 불순도(얼마나 정확하게 들어갔는지)임
		불순도 식은 별로알필요없고 불순도가 높을수록 안좋음(0이 순수한노드임)
		
	tmi)사이킷런은 이진트리만 만드는 CART알고리즘을 쓰는데(T,F만있는) ID3같은거는 둘 이상의 자식노드를 가진 결정트리를 만들수있음

	3.클래스확률추정
		결정트리는 한 샘플이 특정클래스 k에 속할 확률을 추정할수도 있음
		방법은 그냥 샘플 노드따라 내려보내고 리프노드에서의 훈련샘플비율([0,10,10]이면 0%,50%,50%)을 출력하면됨
		
	4.CART알고리즘
		CART알고리즘은 훈련세트를 하나의특성 k의 임계값 t로 두개의 서브셋으로 나눔(ex 길이<2)
		나누는 조건은 ((왼쪽불순도*왼쪽샘플수)/샘플수)+((오른쪽불순도*오른쪽샘플수)/샘플수)가 가장 작은값으로 나눔
		성공했으면 같은방식으로(탐욕적으로)서브셋의 서브셋을 나누고 계속 나누다가 
		최대깊이가 되거나(max_depth=) 불순도를 줄이는분할을 찾을수없으면 멈춤
		
	5.계산복잡도
		결정트리는 예측을 하는건 그냥 길따라 내려가기만 하면 되니까(한노드는 한특성만 확인하니까)O(log₂(m))으로 엄청작음
		
		하지만 훈련하는건 각 노드의 모든 샘플의 모든 특성을 비교하면  O(n*mlog(m))으로 (mlog(m)은 퀵소트값)많이 큼
		만약 훈련세트가 작으면 presort=True 로 주면 미리 정렬치고 들어가기때문에 훈련속도를 올릴수있음
		그래도 훈련세트가 크면 많이 느림
		
	6.지니불순도,엔트로피불순도
		지니불순도랑 엔트로피 불순도가 있는데, 
		둘다 별차이는 없는데 
		지니불순도는 조금 계산이 빠른대신 가장 빈도높은 클래스를 한쪽으로 고립시키는 경향이 있는데,
		엔트로피는 조금 더 균형잡힌 트리를 만듬
		기본값은 지니불순도
	
	7.규제 매개변수
		결정트리는 훈련 데이터의 제약사항이 거의 없음
		
		그래서 따로 제한을 두지않으면(규제하지않으면) 훈련데이터에 딱맞추려고해서 과대적합하기 쉬움
		
		결정트리는 모델 파라미터가 훈련되기전엔 파라미터수가 결정되지 않고,이런모델을 비파라미터 모델이라고 함
		반대로 선형모델같은 모델은 미리 정의된 모델 파라미터 수를 가지니까 파라미터 모델이라고 함
		
		비파라미터 모델은 모델 구조가 데이터에 맞춰져서 고정되지 않고 자유롭지만,과대적합의 확률이 높고
		파라미터 모델은 미리 파라미터수가 정해져있기때문에 자유도가 제한되고 과대적합될 위험은 적지만 과소적합될 위험은 커짐
		
		
		결정트리에서 규제할때 제일 많이 쓰는건 max_depth=(최대깊이)임
		다른건 
			min_samples_split(분할되기위해 노드가 가져야하는 최소샘플수)
			min_samples_leaf(리프노드가 가지고있어야할 최소샘플수)
			min_weight_fraction_leaf(min_samples_leaf와 같지만 가중치가 부여된 전체 샘플 수에서의 비율)
			max_leaf_nodes(리프 노드의 최대 수)
			max_features(각 노드에서 분할에 사용할 특성의 최대 수)
		등이 있음
		보통 min으로 시작하는걸 증가시키거나,max로 시작하는걸 감소시키면 모델에 규제가 커짐
		
		
		다른 방법으로는 제한없이 결정트리를 훈련시킨다음에,불필요한 노드를 가지치기하는 알고리즘도 있음
		x^2검정같은 통계적 검정을 사용해서 순도 향상이 우연히 향상된건지 추정하고,
		이 확률이 임계값보다 높으면(기본값 5%)그 노드는 불필요한것으로 간주되고 그 자식노드는 삭제됨
		이걸 불필요한 노드가 모두 없어질때까지 계속함
		
	8.회귀
		결정트리는 회귀문제에서도 쓸수 있는데
		이거도 똑같이 TF측정한다음에 그안의 샘플의 평균값을 가지고 그래프를 그리는식임
		
		회귀에선 불순도를 줄이는대신,MSE를 최소화하도록 분할하는거 빼고는 똑같이 동작함
		
		분류에서처럼 회귀에서도 규제가 없으면 과대적합되기 쉬움
		
	9.불안정성
		결정트리에서 문제는,계단모양의 결정경계를 만들기 때문에(T,F로 분류하고 그 폭이크니까)
		그래서 훈련세트의 회전에 민감함(딱 중간기준으로 절반나누는것도 45도 회전하면 대각선으로 선그을수없으니(한특성에 대한 TF라서) 
							  중간에 뭐가 더생김)
		
		이런거처럼 결정트리는 훈련 데이터에 있는 작은 변화에도 매우 민감하다는게 문제임
		
		이런걸 해결하려고 랜덤포레스트(결정트리의 앙상블)등을 사용해서 극복할수있음



7.앙상블학습과 랜덤포레스트
	앙상블학습이란 예측기를 엄청 모아서 걔들 답을 평균내거나 가장 많은답으로 분류하는 방식임
	그중 결정트리의 앙상블을 랜덤포레스트라고 함
	
	1.투표기반 분류기
		가장 쉽게 앙상블로 분류하는 방법은 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는것
		이걸 직접투표 분류기라고 함
		
		보통 이렇게 다수결분류기가 앙상블에 포함된 개별분류기중 가장 뛰어난것보다 정확도가 높은경우가 많음
		
		앙상블은 모든 분류기가 완벽하게 독립적이고,오차에 상관관계가 없을수록 성능이 올라감
		그래서 각기 다른 알고리즘으로 학습시키면,다른종류의 오차를 만들 가능성이 높아져서 정확도가 올라갈수있음
		
		만약 모든 분류기가 클래스의 확률을 예측할수있으면,모든 예측을 평균내서 가장 확률높은 클래스를 예측할수있음
		이걸 간접투표라고 함
		이방식은 확률이 높은 투표에 비중을 두고,0.7 0.3일떄 0.3이 무시되지않기때문에 직접투표보다 성능이 높음
		간접투표를 사용하려면 voting='soft'하고,내부에 predict_proba메서드가 있는 분류기만 다 사용하면됨
		(SVC는 probability=True하면 predict_proba가 생김)
		
	2.배깅과 페이스팅
		다양한 분류기를 만드는 방법에는 각기 다른 훈련 알고리즘을 만드는방법도 있지만 
		같은 알고리즘을 써도 훈련 세트의 서브셋을 무작위로 구분해서 분류기를 각기 다르게 학습시킬수도 있음
		
		여기서 훈련세트에서 중복을 허용하여 샘플링하는걸 배깅이라고 하고
		중복을 허용하지 않고 샘플링하는방식을 페이스팅 이라고함
		
		즉,배깅과 페이스팅은 같은 훈련샘플을 여러개의 예측기에(각기 다른 알고리즘의) 사용할수 있지만,
		배깅은 한 예측기에서 같은 훈련샘플을 여러번 쓸수있고,페이스팅은 한예측기에서 반복사용은 불가능함
		
		모든 예측기가 훈련을 마치면 모든 예측기의 값을 모아서 새로운 샘플에 대한 예측을 만듬
		만약 분류였으면 최빈값(가장 많은 예측결과)이고 회귀면 평균을 계산함
		
		개별예측기는 원본훈련세트보다 훨씬 크게 편향되어있지만,수집함수를 통과하면 편향과 분산이 모두 감소함
		
		일반적으로 앙상블의 결과는 원본데이터셋으로 하나의 예측기를 훈련시킬떄와 비교해서 편향은 비슷하지만 분산은 줄어듬
		
		보통 앙상블은 모두 동시에 다른 cpu코어나 서버,gpu에서 병렬로 학습시킬수있고,예측도 병렬로 수행할수 있음
		
		사이킷런에서 배깅과 페이스팅 쓰려면
		
			from sklearn.ensemble import BaggingClassifier
			
			bagclf=BaggingClassifier(
				사용알고리즘,n_estimators=예측기갯수,
				max_samples=샘플갯수,bootstrap=True(배깅페이스팅선택),n_jobs=-1(사용할cpu코어수 -1이면 전체)
			)

		부트스트래핑(배깅)은 각 예측기가 학습하는 서브셋에 다양성을 증가시켜서 배깅이 페이스팅보다 편향이 조금 높지만,
		다양성을 추가하면 예측기의 상관관계를 줄여서 분산을 감소시킴
		
		보통 배깅이 더 나은경우가 많아서 기본값이긴한데 여유가있으면 둘다 평가해서 더 나은쪽을 선택하는게 좋음
		
		2.oob평가
			배깅을 쓰면 어떤 샘플은 한 예측기를 위해 여러번 샘플링되고,어떤건 전혀 선택되지 않을수 있음
			그래서 선택되지 않은 샘플을 oob샘플이라고 부르는데,예측기마다 남겨진 oob샘플은 모두 다름
			
			예측기가 훈련되는동안엔 oob 샘플을 사용하지 않으니까 별도의 검증세트를 사용하지 않고 oob샘플을 사용해서 평가할수있음
			앙상블의 평가는 각 예측기의 oob평가를 평균해서 얻음
			(자기예측기에서 남은거로 자기예측기에만 평가함 각 예측기마다 남은게 다르기때문)
			
	3.랜덤패치와 랜덤서브스페이스
		BaggingClassifier는 특성샘플링도 지원함
		특성샘플링은 max_features,bootstrap_features 두 매개변수로 조절됨
		작동방식은 max_samples,bootstrap과 같지만,이건 샘플이 아니고 특성에 관한 샘플링임
		즉 각 예측기는 무작위로 선택한 입력특성의 일부분으로 훈련됨
		
		그러니까 샘플대신 특성으로 예측기 여러개 만들어서 훈련시키는거임(샘플특성 둘다돌릴수도있고)
		
		이 방법은 특히 이미지같은 매우 고차원의 데이터셋을 다룰때 유용함
		
		샘플과 특성을 모두 샘플링하는걸 랜덤패치라고 하고,
		샘플은 하나쓰고 특성을 샘플링하는걸 랜덤 서브스페이스 라고함
		(bootstrap=False, max_samples=1.0, bootstrap_features=True/False max_features=0.1~1.0)
		
		특성샘플링은 더 다양한 예측기를 만들며 편향을 늘리고 분산을 낮춤
		
	4.랜덤 포레스트
		랜덤포레스트는 배깅이나 페이스팅을 적용한 결정트리의 앙상블임
		전형적으로 max_samples을 훈련세트의 크기로 지정함
		BaggingClassifier에 결정트리를 넣어만들수도있지만,
		이미 RandomForestClassifer가 최적화 다 됐기때문에 이거쓰면됨(회귀는 RandomForestRegressor)
		
		이거도 똑같이 쓰면됨
			RandomForestClassifer(n_estimators=갯수,max_leaf_nodes=최대리프수,n_jobs=-1(코어사용수))
		특정 예외를 제외하고 결정트리의 매개변수와 BaggingClassifier의 매개변수를 모두 가지고있음
		
		
		랜덤포레스트는 트리의 노드를 분할할때 최선의 특성을 찾지않고,
		무작위로 선택한 특성후보에서 최적의 특성을 찾는식으로 무작위성을 더 주입함
		이건 트리를 다양하게만들고 편향을 좀 더 손해보는대신에,분산을 낮춰서 좀 더 나은모델이 나옴
		
		1.엑스트라트리
			랜덤포레스트에서 트리를 만들때 노드는 무작위 특성을 선택하고 거기서 최적을 찾는데,더욱 무작위로 만들기위해서 
			최적의 임계값을 찾는대신 후보특성을 사용해 무작위로 분할한다음 그중에서 최상의 분할을 선택하는걸 엑스트라트리라고 함
			여기서도 또 편향을 올리고 분산을 낮춤
			
			엑스트라트리랑 랜덤포레스트랑 뭐가 더 나을지 알긴어렵고,교차검증하는게 유일한방법임
			
		2.특성중요도
			랜덤포레스트의 다른 장점은 특성의 중요도 측정이 쉽다는것
			사이킷런은 어떤 특성을 사용한 노드가(랜덤포레스트 전체트리에 걸쳐서)평균적으로 불순도를 얼마나 감소시키는지 확인해서
			특성의 중요도를 측정함
			즉,가중치평균이고 각 노드의 가중치는 연관된 훈련샘플수와 같음
			
			사이킷런은 훈련이 끝나면 이 점수를 계산하고 합이 1이되도록 정규화시킴
			이걸 보려면 랜덤포레스트.feature_importances_를 프린트해보면 특성중요도가 나옴
			
			랜덤포레스트는 특성선택해야할때 어떤특성이 중요한지 빠르게 알수있어서 매우편리함
			
	5.부스팅
		부스팅은 약한 학습기 여러개를 연결해서 강한 학습기를 만드는 앙상블임
		
		부스팅방법에는 여러개가 있는데 대표적인게 에이다부스트랑 그레이디언트부스팅임
		
		1.에이다부스트
			에이다부스트는 이전모델이 과소적합했던 샘플의 가중치를 더 높여서 학습하기 어려운 샘플에 점점 맞추는식으로 함
			
			즉 알고리즘이 기반이되는(베이직한) 첫 분류기를 돌린다음에,잘못분류된 샘플의 가중치를 상대적으로 높이고,
			두번쨰 분류기는 업데이트된 가중치로 훈련하고 예측을만들고,잘못분류된 샘플의 가중치를 높이고 이런식으로 반복함
			가중치는 학습률로 조절할수있음
			
			이런방식은 경사하강법과 비슷하게 작동함
			
			모든 예측기가 훈련을 마치면 이 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만듬
			하지만 가중치가 적용된 훈련세트의 전반적인 정확도에 따라 예측기마다 다른 가중치가 적용됨
			
			그렇게해서 새로운샘플을 넣을떈 1번예측기부터 n번예측기까지 전부 돌린다음에 
			각 예측기마다 값을 구해서 예측기가중치(나중거를 올린다던가)를 곱한다음 평균구해서 답을뽑음
			
			사용법은
				adaclf=AdaBoostClassifier(
					사용알고리즘,n_estimators=갯수,
					algorithm='SAMME.R'(에이다부스트의 알고리즘선택),learning_rate=학습률
				)
			
			만약 에이다부스트가 과대적합되면 추정기수를 줄이거나 규제를 올리면됨
			그리고 에이다부스트를 훈련할떄는 전단계가 다음단계에 영향을 미치는식이기때문에 보통 앙상블처럼 병렬처리가 안됨
			
			
		2.그레이디언트 부스팅
			그레이디언트 부스팅도 이전까지 오차를 보정하도록 예측기를 추가하지만,얘는 샘플의 가중치를 수정하지 않고
			이전 예측기가 만든 잔여오차에 새로운 예측기를 학습시켜서 전체를 예측하려고 함
			
			즉 10개중에 3개 예측못했으면 3개가지고 훈련하고 또 거기서 잔여남은거로 예측하는식으로 돌림
			사용법은 
			
				gbrt=GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)
			이런식으로 씀

			학습률(learning_rate)는 각 트리의 기여 정도를 조절함
			이걸 낮게 설정하면 훈련세트 학습을 위해 더 많은 트리가 필요하지만 예측의 성능은 좋아짐 
			이건 축소라고 부르는 규제방법임
			
			최적의 트리를 찾기위해선 조기종료를 쓸수있음
			
			간단하게 만들려면 staged_predict()를 쓰면됨
			
				gbrt=GradientBoostingRegressor(...)
				gbrt.fit(x,y)
				
				errors=[mean_squared_error(yval(테스트세트),ypred) 
				for ypred in gbrt.staged_predict(xval)]
				bstnestimator=np.argmin(errors)+1
				
				gbrtbest=GradientBoostingRegressor(...n_estimators=bstnestimator)

			이런식으로 에러가 올라가는지점 찾아주니까 그지점갯수를 넣으면됨
			
			아니면 예전에 조기종료햇던거처럼 직접구현해도되고
			
			GradientBoostingRegressor은 각 트리가 훈련할때 사용할 훈련샘플의 비율을 지정할수있는 subsample를 지원함
			subsample=0.25하면 각 트리는 무작위로 선택된 훈련샘플의 25%훈련샘플로 학습함
			역시 편향을올리고 분산을 낮춤,그리고 훈련속도가 올라감 이런기법을 확률적 그레이디언트 부스팅이라고 함
			
			최적화된 그레이디언트부스팅으로 XGBoost가 유명함
			


	6.스태킹
		스태킹은 앙상블에 속한 모든 예측기를 취합하는 대신,취합하는 모델을 훈련시키는 방법임
		
		먼저 훈련세트를 두개로 나누고 첫번째 서브셋을 첫번째 레이어의 예측을 훈련시키는데 사용한뒤,
		
		첫번쨰 레이어의 예측기로 두번째 세트의 예측을 만듬
		예측기들이 이 샘플들을 본적이없기떄문에 완전히 새로운 예측이 나옴

		즉 처음에 들어간 데이터들은(x,y) 첫레이어예측기 학습에만 쓰이고,
		각 예측값과 레이블로 새로운값을 만들어서 새로운 훈련세트를 만들어서(xnew,y)이거로 블렌더가 훈련함
		즉 첫레이어의 예측을 가지고 타깃값을 예측하도록 학습함(결과적으로 2번학습함)
		
		근데 뭐 잘안쓰는거같음 사이킷런에선 지원도 안하고
		
		
8.차원축소		
	만약 특성(차원)의 수가 엄청나게 많아지면(주로 샘플수보다 특성수가 커지면) 훈련이 느려지고,좋은 솔루션을 찾기 어렵게됨(차원의저주)
	그래도 특성수를 크게 줄여서 불가능한 문제를 가능한 문제로 변경할수 있는 경우가 많음
	
	mnist 같은경우 바깥에있는 픽셀은 거의 항상 흰색이니까 없어도되는정보 이런거는 완전히 제거해도 많은정보가 손실되지않음
	그리고 인접한 두 픽셀은 종종 많이 연관되어있으니까 두 픽셀을 하나로 합쳐도(두개를평균내서) 많은정보를 잃지않음
	(보통 차원축소를 하면 정보가 유실되니까 훈련속도는 올라가지만 성능이 조금 나빠질수있고,파이프라인이 복잡해지고 유지관리가 어려워짐
	그래서 훈련이 너무 느리지않으면 원본으로 돌리는게 나음)
	
	그리고 차원축소를 하면 데이터 시각화에도 매우 유용함
	보통 3차원까지만 쉽게볼수있으니까 2나 3차원으로 줄이면 고차원 훈련세트를 하나의 그래프로 그릴수있고,군집같은 시각적패턴을 감지할수있음
	
	1.차원의 저주
		고차원으로 갈수록 샘플간의 거리가 멀어져서 훈련세트의 과대적합위험이 커지는 현상
		이론적으로는 훈련세트의 크기를 키우면되지만,현실적으로 불가능함(100차원에서 특성거리를0.1이내로 만들려면 우주원자수보다 많이필요)
	
	2.차원축소방법
	
		1.투영
			보통은 훈련샘플들이 모든 차원에 걸쳐 균일하게 퍼져있지않고,모든 훈련샘플들이 고차원 공간안에 저차원 부분공간에 놓여있음
			(3차원에서 2차원 평면상에 전부 놓여있다던지)
			그러면 여기다 대고 차원을 하나 빼버려서(프레스기로 눌러서)3차원을 2차원으로 만들던가 할수있음
			(즉,부분공간에 수직으로(샘플과 평면사이의 가장 짧은직선을 따라서)투영)
			(그러면서 특성도 서로 합쳐지든가 해서 밀도높은특성으로 바뀔가능성도 있게됨)
			
			그런데 차원축소할때 투영이 언제나 최선은 아님
			롤케이크처럼 돌돌말려있는 형태의 데이터셋을 투영해버리면 그대로 다 섞여버리는데 
			우리가 원하는건 군집별로 분리된 데이터기때문에 롤을 펼쳐서 깔아야함
		
		2.매니폴드학습
			
			그래서 롤을 펼쳐서 까는작업을 매니폴드 학습이라고 함
			
			매니폴드는 국부적으로 d차원 초평면으로 보이는 n차원 공간임
			즉 롤케이크에서 롤을 펼치면 2차원(2차원 초평면)이지만 롤케이크 자체는 d차원(3차원)임
			국부적으로는 2d평면이지만 3차원으로 말려있음
			
			이런 매니폴드들을 모델링하는식으로 매니폴드 학습은 작동함
			이는 실제 고차원데이터셋이 더 낮은 매니폴드에 가깝게 놓여있다는 매니폴드 가정에 근거함
			그리고 처리해야할 작업이 저차원의 매니폴드로 표현되면 더 간단해질거라는 가정을 하는데,
			이게 꼭 그렇진않음
			
			즉 모델을 훈련시키기 전에 차원을 감소시키면 훈련속도는 빨라지지만 항상 더 낫거나 간단한 솔루션이 된다는 보장은 없음
			이건 데이터셋에 달린문제임
			
	3.PCA
		PCA는 데이터에 가장 가까운 초평면을 정의한다음,데이터를 이 평면에 투영시킴
		1.분산보존
			훈련세트를 투영하기 전에 먼저 올바른 초평면을 선택해야하는데,이 선택은 가장 분산을 최대로 보존하는 선을 찾으면 됨
			(원본데이터셋과 투영된것 사이의 평균제곱거리를 최소화하는 축)
		2.주성분
			먼저 분산이 최대인 축을 찾고,첫번째 축에 직교하고 남은 분산을 최대한 보존하는 축을 찾아서 2차원을 만듬
			이런식으로 3차원이면 두개에 직교하고 분산이 제일큰거를 고르고 이렇게 n차원까지 n번째 축을 찾음
			(PCA는 데이터셋의 평균이 0이라고 가정하는데 사이킷런은 이작업을 대신해주는데,
			다른라이브러리나 직접만들면 데이터 0에 맞춰야됨)
			
		3.d차원투영
			주성분을 모두 추출해냈으면,d개의 주성분으로 정의한 초평면에 투영해서 데이터차원을 d차원으로 축소할수있음
			이 초평면은 분산을 가능한 최대로 보존하는 투영임을 보장함
			
		4.사이킷런에서 사용법
				from sklearn.decomposition import PCA
				
				pca=PCA(n_components=차원수)
				X2d=pca.fit_transform(데이터)
			이러면 n_components의 수치만큼 줄여진 데이터가 나옴
			학습후에는 components_에 W^d의 전치가 담겨있음
			
			explained_variance_ratio_에는 주성분의 분산의 비율이 들어있음
			현재 압축한데이터의 총 분산을 1로봤을때 각 선별로 얼마나 들어있는지 나오니까 급격하게 줄어서 4차원인데 2개로 99퍼면
			2개 더 날리는식으로 할수있음
			
		5.차원수 선택
			그래서 막 저렇게 보고 줄이고 하면 귀찮으니까 몇%이상까지만 잘라서 보통 함
			코드는
				pca=PCA()
				pca.fit(x)
				cumsum=np.cumsum(pca.explained_variance_ratio_)
				d=np.argmax(cumsum>=)+1
				
				pca=PCA(n_components=d)
				
			근데 이게 더 편하고 좋음
				pca=PCA(n_components=0.95)
				xr=pca.fit_transform(x)
			
			이거말고 다른방법은 분산을 차원수에 대한 함수로 그리는것(cumsum으로 그리면됨 )
			거기서 변곡점에서 멈추면됨
		6.압축을위한 PCA
			차원을 축소하면 훈련세트가 줄어드는데 분산 95%하면 막 원본크기의 20%되고 그럼
			그리고 압축된 데이터셋에 투영을 반대로써서 원래차원수로 되돌릴수는 있는데,이미 일정량의 정보는 날아갔기때문에
			원본 데이터셋을 얻을수는 없지만 매우 비슷한 데이터를 얻을수있음
			그 원본데이터셋과 압축후 원복한데이터 사이의 평균제곱거리를 재구성오차라고 함
			
		7.랜덤PCA
			svd_solver 매개변수를 randomized 로 넣으면 랜덤pca라고 부르는 확률적 알고리즘을 써서
			처음 d개의 주성분에 대한 근삿값을 빠르게 찾음
			이건 줄일 차원수가(d가) 원래차원수(n)보다 많이 작으면 엄청빠름(O(m*n^2)+O(n^3)->O(m*d^2)+O(d^3))
			
			사이킷런의 기본값은 auto임
			m이나 n이 500보다 크고 d가 m이나 n의 80%보다 작으면 랜덤 PCA를 씀
			그게싫으면 svd_solver를 full로 주면 완전한SVD를씀
		
		8.점진적 PCA
			PCA의 문제점은 svd(특이값분해)를 쓰기위해 전체 훈련세트를 메모리에 올려야 하는데,이걸 보완하기위해
			미니배치로 나눠서 하나씩 하는 점진적 PCA가 개발됨
			이런방식은 온라인으로 PCA를 적용할수있고 훈련세트가 클떄 유용함
			
			사용법은
			
				from sklearn.decomposition import IncrementalPCA
				
				n_batches=배치수
				incpca=IncrementalPCA(n_components=n_batches)
				
				for xbatch in np.array_spilt(x,n_batches):
					incpca.partial_fit(xbatch)
				
				xred=incpca.transform(x)
			다른방법은 넘파이의 memmap를 써서 하드디스크에 있는 파일을 메모리처럼 쓰는것
				xm=np.memmap(filename,dtype="float32",mode="readonly",shape=(m,n))
				
				batchsize=m//n_batches
				incpca=IncrementalPCA(n_components=배치수,batch_size=batchsize)
				incpca.fit(xm)
	
	4.커널PCA
		PCA도 커널트릭을 써서 사기칠수있는데 n차원을 n+3차원 이런식으로 늘린후에 쉬운걸 찾아서 그거선택하는식으로 사기칠수있음
		이걸 커널PCA(KPCA)라고 함
		
		이건 투영된 후의 샘플의 군집을 유지하거나,꼬인 매니폴드같은걸 펼칠때도 유용함
		
		사용법은
			from sklearn.decomposition import kernelPCA
			
			rbfpca=kernelPCA(n_components=차원수,kernel="rbf(커널선택)",gamma=0.04 증가하면 종이 좁아져서 샘플의 영향범위가 줄어듬,즉 규제, 과소면 증가 과대면 감소)
			커널은 선형,rbf,시그모이드 등을 씀
		1.커널선택과 하이퍼파라미터튜닝
			kpca는 비지도 학습이기떄문에 좋은 커넣과 하이퍼파라미터선택을 위한 명확한 성능측정기준이 없음
			하지만 차원축소는 지도학습의 전처리로 활용되므로 그리드탐색으로 주어진문제에서 성능이 가장 좋은 커널과 하이퍼파라미터를
			선택할수있음
			
			사용법은
				clf=PipeLine([
				('kpca',kernelPCA(n_components=2)),
				('logreg',LogisticRegression())
				])
			
				paramgrid=[{
					"kpca__gamma":np.linspace(0.03,0.05,10),
					"kpca__kernel":["rbf",sigmoid]
				}]
				gridsearch=GridSearchCV(clf,paramgrid,cv=3)
				gridsearch.fit(X,y)
			이런식으로 그리드서치로 그중제일나은걸 선택할수있음
			가장 좋은 커널과 하이퍼파라미터는 best_params_에 저장됨
	
			완전한 비지도학습으로 가장 낮은 재구성오차를 만드는 커널과 파라미터를 선택할수도있는데 어려움
			하지만,커널트릭으로 무한차원의 특성공간에 맵핑한다음에 변환된 데이터셋을 선형PCA를 사용해 2D로 투영하는식으로 날먹할수있음
			
			축소된 공간에 있는 선형PCA를 역전시키면(복원) 재구성된 데이터포인트는 원본공간이 아닌 공간에 놓이게 되는데,
			저 공간은 무한차원이라서 계산할수없고 재구성에 따른 에러를계산할수있는데,저 포인트와 가까운 원본공간의 포인트를 찾을수있음
			이걸 재구성원상이라고 함
			원상을 얻으면 원본샘플과의 제곱거리를 측정해서 오차를 알수있는데,이걸 최소로하는 커널과 하이퍼파라미터를 선택할수있음
			
			하는법은 fit_inverse_transform=True를 넣으면됨
			
				rbfpca=kernelPCA(n_components=2,kernel="rbf",gamma=0.0433,
									fit_inverse_transform=True)
				xre=rbfpca.fit_transform(x)
				xpre=rbfpca.inverse_transform(xre)
			그다음 재구성원상오차를 계산할수있음
				
				mean_squared_error(x,xpre)
			이렇게 오차를 구할수있으니 최소화를 시키는 커널과 하이퍼파라미터를 그리드탐색으로 찾을수있음
			
	5.LLE
		LLE는 투영에 의존하지 않는 매니폴드 학습임
		LLE는 각 훈련샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어있는지 측정한다음,
		국부적인 관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾음
		이방법은 특히 잡음이 너무 많지 않을때 꼬인 매니폴드를 펼치는데 잘 작동함
		
		사이킷런 사용법은
			from sklearn.manifold import LocallyLinearEmbedding
			
			lle=LocallyLinearEmbedding(n_components=2,n_neighbors=찾을이웃수)
			xre=lie.fit_transform(x)
			
		써보면 펼쳐지기도 잘 펼쳐지고 지역적으로는 샘플간 거리유지가 되는데 전역적으로는 잘 유지가 안되긴하지만
		동작은 나름 잘됨
		
		lle의 작동방식은 각 훈련샘플에 대해 가장 가까운 k(n_neighbors)개의 샘플을 찾은뒤,
		이 이웃에 대한 선형함수로 훈련샘플을 재구성함(k들 거리의합이 최소화되는 가중치를찾음)
		이걸 전체에대해 반복
		
		그리고 나선 이 관계가 최대한 유지되도록 d차원(낮은차원)으로 맵핑함
		
		근데 이알고리즘은 대량의 데이터셋에 적용하긴 어려움(O에 m^2가있음)
		
	6.다른 차원축소기법
		랜덤투영
			랜덤한 선형투형으로 데이터를 저차원공간으로 투영함
			나름 잘작동하고 초기차원수에 의존적이지않다고함
			
		다차원스케일링
			샘플간의 거리를 보존하면서 차원을축소함
			
		isomap
			각 샘플을 가장 가까운이웃과 연결하는식으로 그래프를 만들고,
			두노드사이의 최단경로를 이루는 노드의수를 유지하면서 차원을축소함
			
		t-SNE
			비슷한샘플은 가까이 비슷하지않은건 멀리 떨어지도록 하면서 차원을 축소함
			데이터 시각화에 많이 사용되고,고차원공간의 군집을 시각화할때 사용됨
			
		선형판별분석
			분류임,하지만 훈련과정에서 클래스사이를 가장 잘 구분하는 축을 학습함
			이 축은 데이터가 투영되는 초평면을 정의하는데 사용할수있음
			이알고리즘의 장점은 투영을 통해 가능한 클래스가 멀리 떨어지도록 유지하므로 다른 분류기알고리즘을(SVM같은)
			적용시키기전에 차원을 축소시킬떄 좋음


9.비지도학습
	머신러닝의 난이도순은 강화학습-지도학습-비지도학습 순
	비지도학습에는 대표적으로 군집과 가우시안혼합모델이 있음
	
	1.군집
		군집은 샘플들의 위치에 따라서 묶어서 분류하는식으로 작동함
		그리고 비슷한샘플들의 집합을 클러스터라고 부름
		
		군집은 고객분류,데이터분석,차원축소,이상치 탐지,준지도학습,검색엔진,이미지분할등에 사용됨

		클러스터에 대한 보편적인 정의는 없고 대충 모여있으면 클러스터라고 부르고,알고리즘에 따라 다른 종류의 클러스터를 감지함
		k평균은 센트로이드라고 부르는 특정 포인트 주변에 모인 샘플을 찾고,dbscan은 샘플이 밀집되어 연속된 영역을 찾음
		
		1.k평균
			k평균은 클러스터의 갯수를 받아서 랜덤한 샘플위치를 센트로이드로 잡고 주변에 클러스터인덱스(비지도에서는 이걸 레이블이라고함)
			를 주고 센트로이드를 같은 클러스터에 속한애들의 중앙으로 보내고 다시 근처 애들 클러스터인덱스 주는식으로 반복해서
			변화가 없을떄까지 돌림
			
			하드군집은 샘플이 속한 클래스를 리턴하는거고 소프트군집은 클래스에 속할 확률을 리턴함
			
			일반적으로 빠르게 수렴하긴 하는데 최적의 솔루션으로 수렴하지못할수도있음(지역최적점)
			그래서 반복횟수를 줘서(n_init) 그 반복횟수 내에서 제일 좋은값을 뽑음
			비지도학습인데 성능지표가 있냐면,부정확하긴한데 있긴함
			각 샘플과 가장 가까운 센트로이드의 거리의 평균제곱거리를 모델의 이니셔라고 부르고 성능지표로 씀
			사이킷런에서 kmeans.score(훈련세트)하면 음수값 이니셔를 받을수있음(사이킷런에선 값이큰게 좋아야하기때문에 음수로바꿈)
			
			KMeans에서 그냥 돌리면 n_init번 실행해서 이니셔가 가장 낮은 모델을 선택함
			
			kmeans의 변종으로,전체 데이터셋을 사용하지 않고 미니배치를 사용해서 속도를 올리거나,
			메모리에 안들어가는 데이터셋을쓸수있음
			MiniBatchKMeans()를 쓰면됨
			근데 초기화도 여러번해야하고 가장 좋을거 직접골라야해서 할게많음
			꼭필요할때만 쓰는게편함
			그리고 미니배치가 훨씬빠르긴한데 일반적으로 이니셔는 조금더 나쁨
			
			1.클러스터 갯수찾기
				클러스터 갯수를 찾으려면 딱 몇갠지 데이터봤을때 보이면 모르겠지만,보통 이렇게하면안되니까 코드로해야하는데
				만약 이니셔를 사용해서 가장 작은 이니셔를 찾는다고 하면,
				문제가 이니셔는 클러스터 갯수가 많아질수록 좋아지기때문에(샘플과 클러스터사이 거리라서 클러스터가 늘어나면 거리가 줄어듬)
				그냥 바로 이니셔 최저인걸 쓸순없고
				이니셔 그래프의 기울기가 꺽이는 지점에서 그 지점을 택하는 방법을 쓸순있음 엉성하지만
				
				더 정확한 방법은(계산비용이 많이들지만) 실루엣점수임
				실루엣점수는 모든 샘플에 대한 실루엣 계수의 평균인데,
				실루엣 계수는 
				(자기가 속한 클러스터를 제외한 가장가까운 클러스터의 샘플까지 평균거리(a)-클러스터 내부의 평균거리(b))/max(a,b)
				임
				실루엣계수는 -1부터 +1까지 바뀔수있고,
				+1에 가까우면 자신의 클러스터 안에 잘속해있고 다른클러스터와는 멀리있다는거고
				0에 가까우면 클러스터 경계에 있다는거고
				-1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미임
				
				실루엣 점수를 계산하려면 
					from sklearn.metrics import silhouette_score
					
					silhouette_score(훈련세트,k평균.labels_)
				하면나옴
				
				여기서 그냥 실루엣점수 가장 높은거 선택할수도있고,
				각 클러스터와 계수값으로 그래프 그려서(실루엣 다이어그램) 평균적으로 높고 실루엣점수도 꽤 높은걸 선택할수도있음
				
				만약 애들이 각 클러스터의 개수에 해당하는 실루엣점수보다 낮은애들이 많으면,나쁜클러스터임
				일반적으로 다 비슷하게 높은게 막 한두개가 엄청높아서 슈퍼캐리하는애들보다 일반화가 잘됨 
				
			2.k평균의 한계 
				k평균은 속도가 빠르고 확장이 용이한데,
				단점으로는 최적이 아닌 솔루션을 피하기위해 반복을 여러번해야하고,클러스터 갯수를 정해야함
				그리고 k평균은 클러스터의 크기나 밀집도가 다르거나,원형이 아니면 잘 작동하지 않음
				원형이 아닌 타원형같은건 가우시안혼합모델이 잘 작동함
				
			3.군집을 사용한 이미지분할
				이미지분할은 이미지를 세그먼트 여러개로 분할하는 작업임(산,구름,꽃 등 같은 군집에 속한 픽셀들 분리)
				특히 이런식으로 동일한종류의 물체에 속한 픽셀을 분리하는걸 시맨틱 분할이라고 함
				시맨틱분할에서 최고수준성능을 내려면 합성곱신경망을 쓰는 복잡한모델을 써야함
				
				저런거말고 그냥 색상분할같은거 하려면 그냥 클러스터 갯수주고 픽셀들의 색으로 군집을 만든다음,그 픽셀들을 원본사이즈로
				출력하면 됨
				막 rgb/30이런거랑 다른거는,k군집은 비슷한 크기의 클러스터를 만드는 경향이 있기때문에 막 한두픽셀 색이 튄다고
				그걸 따로 잡아주지않고 크기가 작으면 다른데다가 넣어버림 클러스터갯수에따라서
				
			4.군집을 사용한 전처리
				군집은 차원축소에도 효과적임
				특히 지도학습 알고리즘을 적용하기전에 전처리단계로 사용할수있음
				
				간단히 이미지에서 사용하려면 파이프라인으로 만들어서 이미지를 클러스터n개로 모으고,
				이미지를 n개클러스터까지 거리로 바꾼뒤 회귀나 분류모델에 적용하면됨
				
				좀 더 올릴려면 클러스터 갯수를 찾는건데
				전처리로 쓸려면 명확한 성능측정기준이 있기때문에(교차검증에서 가장 좋은 분류성능을 내는값)
				GridSearchCV써서 최적의 클러스터 갯수를 찾으면됨
			
			5.군집을 사용한 준지도학습
				이경우에도 클러스터로 훈련세트를 나눈뒤에,각 클러스터마다 센트로이드에 가장 가까운 이미지를 찾아서 이런 이미지만
				따로떼서 레이블을 붙여주고,이걸 지도학습으로 돌리면 엄청 정확도가 올라감
				
				한단계 더 나아가서 동일한 클러스터 내에 있고,중앙에서 가까운 애들(20%라던가)만 레이블을 전파하고,그거만떼서 돌리면
				정확도가 더 올라감,이걸 레이블 전파라고 함 
				
				tmi(
					모델과 훈련세트를 지속적으로 향상하기위해서,
					수집된 레이블된 샘플에서 모델을 훈련하고 이 모델을 써서 레이블안된 샘플에 대한 예측을 만든후
					모델이 가장 불확실하게 예측한 샘플을 직접 레이블을 붙인다음
					레이블이 부여하는게 의미없어졋다싶을때까지 반복함
					이걸 불확실성 샘플링이라고 함
					
					다른방법으로는 모델을 가장 크게 바꾸는샘플이나,모델의 검증점수를 가장 크게 떨어뜨리는 샘플,앙상블에서 여러개의
					모델이 다른값을 내는 샘플들을 레이블 붙이는방식이 있음 
				
				)
		
		2.DBSCAN
			dbscan은 밀집된 연속된 지역을 클러스터로 잡음
			방법은
				알고리즘이 각 샘플마다 작은 거리(입실론)내에 몇개 놓여있는지 세고,이걸 입실론-이웃이라고 부름
				자신을 포함해서 입실론이웃내에 min_samples개의 샘플이 있다면 이걸 핵심샘플이라고 간주함,즉 밀집된지역에 있는샘플
				핵심샘플의 이웃에 있는 모든 샘플들은 동일한 클러스터에 속함,이웃에는 다른 핵심샘플이 포함될수있음
				즉 핵심샘플의 이웃의 이웃은 계속해서 하나의 클러스터를 형성함
				핵심샘플도 아니고 이웃도 아닌샘플은 이상치로 판단
				
			이 알고리즘은 모든 클러스터가 충분히 밀집되어있고,밀집되지않은곳과 잘 구분될때 좋은 성능을 냄
			사용법도 그냥 
				dbscan=DBSCAN(eps=0.05(입실론거리),min_samples=최소핵심샘플수)
				dbscan.fit(훈련세트)
			하면됨 
			
			일부 샘플의 클러스터인덱스가 -1나오는데,이건 이상치로 판단했다는 의미임
			
			DBSCAN은 predict를 제공하지않고 fit_predict를 제공함
			즉 새로운 샘플에 대해 클러스터를 예측할수없음
			이유는 다른 분류알고리즘이 더 좋기때문
			
			그래서 예측을 하고싶을떈 다른 예측기를 선택해야함
			만약 KNeighborsClassifier을 쓰면
			
				knn=KNeighborsClassifier(n_neighbors=50)
				knn.fit(dbscan.components_,dbscan.labels_[dbscan.core_sample_indices_]])
			하면 훈련되고
			저기다가 predict써서 예측하면됨
			즉 전처리로 사용하는게 일반적인 사용법인듯
			
			
			얘는 이상치에 안정적이고,클러스터의 모양과 갯수에 상관없이 감지할수있고,하이퍼파라미터가 두개뿐인게 장점이고
			클러스터간의 밀집도가 크게 다르면 모든 클러스터를 올바르게 잡아내는게 불가능한게 단점임
			계산복잡도는 O(m^2)임
			
		3.다른 군집 알고리즘
			1.병합군집
				병합군집은 대규모 샘플과 클러스터에 잘 확장되며,다양한 형태의 클러스터를 감지할수있고,
				특정클러스터수를 선택하는데 도움이되는 클러스터트리를 만들수있으며,어떤 짝거리와도 사용할수 있음
				하지만 연결행렬이 없으면 대규모 데이터셋으로 확장하기 어려움
				
			2.BIRCH
				BIRCH는 대규모 데이터셋을 위해 고안됨
				특성갯수가 너무 많지 않으면 배치k평균보다 빠르고 비슷한 결과를 만듬
				훈련과정에서 새로운 샘플을 클러스터에 빠르게 할당할수있는 정보를 담은 트리를 만듬
				
			3.평균이동
				이건 각 샘플을 중심으로 원을 그리고,원 안에 포함된 샘플의 평균을 구한뒤 원 중심을 평균점으로 이동시키고
				원이 안움직일때까지 반복함
				DBSCAN과 비슷한방식
				근데 O(m^2)라서 대규모데이터셋에는 적합하지않음
			4.유사도전파
				샘플은 자기랑 비슷한애한테 투표하고 걔들끼리 클러스터를 형성함
				이건 크기가 다른 여러개의 클러스터를 감지할수있지만 O(m^2)임
			5.스펙트럼군집
				샘플사이의 유사도 행렬을 받아 저차원 임베딩(차원을 축소함)을 만듬
				그리고 그 저차원에서 군집을 사용함
				이건 복잡한 클러스터구조를 감지하고 그래프컷을 찾는데 사용할수있음(소셜네트워크에서 친구의 클러스터찾기 등)
				이건 샘플갯수가 많거나 클러스터의 크기가 매우 다르면 잘 동작하지않음
				
	2.가우시안혼합
		가우시안 혼합은 샘플이 파라미터를 모르는 여러개의 혼합된 가우시안분포(종모양그래프)에서 생성되었다고 가정하는 확률 모델임
		하나의 가우시안 분포에서 생성된 모든 샘플은 하나의 클러스터를 형성함
		가우시안 혼합은 타원형에서 잘 작동함
		기본적인 사용법은 
			from sklearn.mixture import GaussianMixture
			
			gm=GaussianMixture(n_components=클러스터수,n_init=반복수)
				
		얘도 k평균과 비슷하게 동작하는데
		클러스터 파라미터를 랜덤하게 초기화하고 수렴할때까지 반복함
		
			1.샘플을 클러스터에 할당하고(기댓값)
			2.클러스터를 업데이트함(최대화)
		즉 클러스터의 중심뿐만아니라 크기 모양 방향 클러스터의 가중치까지를 넣은 K평균이라고 봐도 됨
		
		가우시안혼합은 하드클러스터가 아니라 소프트클러스터를 사용해서 확률을 뱉음
		
		즉,기댓값 단계에서 현재 클러스터 파라미터에 기초해 각 클러스터에 속할 확률을 예측하고,
		최대화 단계에서 각 클러스터가 데이터셋에 있는 모든 샘플을 사용해 업데이트됨
		
		클러스터에 속할 추정확률로 샘플에 가중치가 적용됨
		이 확률을 샘플에 대한  클러스터에 책임이라 부르고 
		최대화단계에서 클러스터 업데이트는 책임이 가장 많은 샘플에 크게 영향을받음
		
		이거도 k평균처럼 반복돌리는거라서 지역최대값으로 수렴할수있음
		
		소프트군집으로 받으려면 predict
		하드군집으로 받으려면 predict_proba
		
		가우시안 혼합모델은 생성모델이라서 이 모델에서 새로운 샘플을 만들수 있음(가우시안그래프에 대충넣으면되니까)
		그리고 주어진 위치에서 모델의 밀도를 추정할수있음 score_samples()쓰면됨
		샘플이 주어지면 그 위치의 확률밀도함수의 로그를 예측함
		
		만약 특성이나 클러스터가 많거나 샘플이 적으면 최적의솔루션으로 수렴하기 어려우니까 규제를 해야함
		그중하나는 클러스터의 모양과 범위를 제한하는거임
		covariance_type매개변수에
			spherical
				모든 클러스터가 원형이고,하지만 지름이 다를수있음(분산이 다를수있음)
			diag
				클러스터는 크기에 상관없이 어떤 타원형도 가능함,하지만 타원의 축은 좌표축과 나란해야함
			tied
				모든 클러스터가 동일한 타원모양,크기,방향을 가짐
		기본값은 full로 되어있음(제약이없음)
		
		1.가우시안혼합을 사용한 이상치 탐지
			이상치탐지는 보통과 많이 다른 샘플을 찾는것
			가우시안 혼합에서 이상치를 찾는법은 그냥 밀도가 낮은지역에 있는 모든 샘플을 이상치로 잡으면됨
			그러려면 밀도 임계값을 정해줘야되는데 이건 알아서 정하면됨(불량비율이라던가)
			
			만약 거짓양성이 많으면 임계값을 낮추고 거짓음성이 많으면 임계값을 올리면됨(정밀도재현율 트레이드오프)
		2.클러스터갯수선택
			가우시안혼합에서는 이너셔나 실루엣을 사용할수없음,클러스터가 타원형이나 크기가 다를때 안정적이지 않기때문
			그래서 BIC나 AIC같은걸 씀
			
			둘다 학습할 파라미터가 많은(클러스터가 많은)모델에 벌칙을 가하고 데이터에 잘 학습하는 모델에 보상을 더함
			보통 둘다 동일한모델을 선택하는데,선택이 다를경우 bic가 더 간단하지만 데이터에 완벽히 맞지 않을수있음
			
				gm.bic(훈련세트)	
				gm.aic(훈련세트)	
			로 값을 받을수있는데 이거로 그래프그려서 최소값인거 고르면됨
			
		3.베이즈가우시안혼합모델
			클러스터 갯수를 정하는 다른 방법은,
			클러스터를 엄청크게 잡고나서 불필요한 클러스터의 가중치를 0으로 만드는 BayesianGaussianMixture를 사용하는거임
			
			이 모델에서 클러스터 파라미터는 고정된 모델파라미터가 아니라 잠재확률변수로 취급됨
			
			베타분포는 고정범위 안에 놓인값을 가진 확률변수를 모델링할때 자주 사용됨
			이경우 0~1사이
			
			만약 a=[0.3,0.6,0.5]일때 샘플의 30퍼센트가 클러스터 0에할당되고,남은샘플의 60퍼가 1에할당될떄,
			이프로세스는 새 샘플이 작은클러스터보다 큰 클러스터에 합류할 가능성이 높은 데이터셋에 잘 맞는 모델임
			농도가 크면 a값이 0에 가깝게되고 많은 클러스터가 생기고,농도가 낮으면  a가 1에 가깝게되고 몇개의 클러스터만 생김
			
			이렇게 클러스터가 많을지 적을지 사전믿음을 weight_concentration_prior매개변수에 넣어서 파라미터를 줄수있음
			그렇지만 데이터가 많을수록 사전믿음은 중요하지않음
			
			가우시안 혼합모델은 타원형 클러스터에 잘 작동하지만 다른모양을 가진 데이터셋은 나쁜결과가 나옴
			
		4.이상치탐지와 특이치탐지를 위한 다른 알고리즘
			PCA
				일반샘플의 재구성오차와 이상치의 재구성오차를 비교하면 후자가 훨씬 커서 큰거를 거르는식으로 쉽게만들수있음
			Fast-MCD
				보통샘플이 혼합된게 아닌 하나의 가우시안분포에서 나왔다고 가정하고,여기에 안속하는걸 다 이상치로 잡음
			아이솔레이션 포레스트
				고차원에서 효율적인 알고리즘
				무작위로 랜덤포레스트를 만들고,각노드에서 특성을 랜덤하게 선택하고 랜덤한임계값으로 데이터셋을 둘로 나눔
				이렇게 모든샘플이 다른샘플과 격리될때까지 진행함
				보통 이상치는 다른샘플보다 멀리떨어져있으므로 중앙에서 멀리있는거 가지치기하면됨
			LOF
				주어진샘플주위의 밀도와 이웃주위의 밀도를 비교해서 거름
			one-class SVM
				모든샘플을 고차원공간에 매핑하고 이 고차원공간에서 선형svm으로 두클래스를 분리함
				만약 새 샘플이 여기 보통공간에 안놓이면 이상치로 날림
				이거도 svm이라서 대규모데이터셋에는 넣기어려움
				


10.케라스 인공신경망























	
		

