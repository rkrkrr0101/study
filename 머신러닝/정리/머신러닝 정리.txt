1.기초

머신러닝은 작업의 성능이 경험으로 인해 변하면 머신러닝 프로그램임

시스템이 학습할때 사용하는 샘플은 훈련세트
각 훈련데이터는 훈련샘플(샘플)

스팸메일분류기에서는
작업=스팸메일구분
경험=훈련데이터
성능=정확도(이건 자기가 원하는거로 직접 정해야하는데 분류에선 정확도 보통씀)

머신러닝을 통해 우리가 몰랐던 연관관계나 추세를 발견하는것은 데이터 마이닝

레이블의 종류에 따른 분류

	지도학습:훈련데이터에 레이블(정답인지 아닌지 표시)가 있는데이터로 학습하는방법

		대표적으로 분류가 전형적인 지도학습임(스팸메일분류같은)

		또 다른 예는 회귀가 있음
		회귀는 예측변수를 통해 타깃수치를 예측하는것
		예를들어 차 브랜드,주행거리등을 파라미터로 받아서 중고차값을 뱉는식
		
	비지도학습:비지도학습은 훈련데이터에 레이블이 없는상태에서 학습하는것
			대표적으로 군집,시각화등이 있음
			시각화는 레이블이 없는 데이터를 넣으면 눈으로 볼수있는 표현을 만들어줌
			예로 강아지 고양이 트럭등을 군집지어져있는걸 시각화시킨다던지 할수있음
			
	준지도학습:준지도학습은 일부만 레이블이 있는 데이터고 나머지는 레이블이 안된 데이터인상태에서 학습하는것
			예를들어서 사진여러개에서 사진들에 사람a,사람b가 사진 1,2에 있는지는 비지도학습으로 할수있지만,
			그사람이 누군지는 지도학습으로 해야하니까 
			사진에 사람abc태그하는건 비지도학습으로 하고 그 사람 이름같은거 적는건 지도학습으로 하는식
			
	강화학습:강화학습은 환경을 관찰해서 행동을 실행하고,거기따른 보상또는 벌점을 받아서 큰 보상을 따르게 행동방식을 수정하면서 반복하는것
		  게임ai나 보행로봇등에 주로 쓰임

실시간인지 아닌지에 따른 분류

	배치학습:시스템을 오프라인에서 학습 다 시킨다음에 그걸 적용하는식,온라인상에서 입력이들어와도 학습하지않고 출력만 함
		  배치학습이 새로운데이터를 학습하려면 전체데이터를 써서 처음부터 다시훈련해야함
		
	온라인학습:시스템을 하나씩,또는 일정 묶음단위로(미니배치) 주입해서 시스템을 계속 훈련시킴
		   이건 빠른 변화에 적응해야하는 시스템에 적합함
		   
		   온라인학습은 데이터를 학습하고나면 그 데이터는 필요없으니까(전으로 돌릴필요가없으면)삭제해도 됨
		   
		   그리고 메인메모리에 들어가지 않는 크기의 데이터셋을 학습할때도 이걸 사용할수있음
		   미니배치를 계속 넣는식으로 돌리면됨(외부메모리학습,배치학습인데 데이터클때 온라인학습방식으로 돌리는거)
		   
		   온라인학습에서 중요한 파라미터는 변화하는 데이터에 얼마나 빨리 적응할것인지를 나타내는 학습률
		   학습률을 높게하면 새로운 데이터에 빨리 적응하지만,이전 데이터를 금방 잊어버림
		   학습률을 낮게하면 새로운 데이터에 적응하는데는 오래걸리지만,새로운 데이터의 잡음같은거에 덜민감해짐
		   
		   온라인학습의 가장 큰 단점은 악의적데이터가 주입될때 시스템성능이 나빠질수 있다는것
		   그래서 시스템을 모니터링하거나 입력데이터를 모니터링해서 이런걸 걸러내야함

어떻게 일반화 되는가에 따른 분류
	사례 기반 학습:훈련샘플을 기억해서 유사도등을 측정해서 비교하는식으로 일반화 하는 방식
			  특성1과 특성2가 있는 그래프가 있을때,주변에 있는 값 중 가장 가깝거나 많은거로 분류하는식
	
	모델 기반 학습:샘플들의 모델을 만들어서 예측에 사용하는 방식
			  식을 만들어서  거기에 특성을 넣고 가공해서(절편을주거나,민감도를 조절하거나 해서) 대충 근처값이 나오게 하는식임
			  
			  모델이 얼마나 좋은지는 측정지표를 정해야 하는데
			  모델이 얼마나 좋은지 측정하는 효용함수나,모델이 얼마나 나쁜지 측정하는 비용함수가 있음
			  
			  선형회귀에서는 선형모델의 예측과 훈련데이터 사이의 거리를 재는 비용함수를 사용해서 
			  파라미터를 조절해 이 거리를 최소화하는게 목표임 이걸 모델을 훈련시킨다고 함
			  
즉, 머신러닝은
	데이터를 분석해서
	모델을 선택하고
	훈련데이터로 모델을 훈련시킴(비용함수가 제일 작아지는값을 찾음)
	마지막으로 새로운 데이터를 넣어서 예측을하고 잘되기를 기대함
	
머신러닝에서 문제가 되는 가장 큰 두가지는
	나쁜데이터:
		충분하지 않은량의 데이터
		
		대표성이 없는 훈련 데이터
			우리가 일반화 하기를 원하는 새로운 사례를 잘 대표하는 데이터를 넣어야함
			근데 샘플이 작으면 샘플링 잡음(우연에의한 대표성없는 데이터)
			샘플이 커도 추출방법이 잘못되면 샘플링 편향(무슨 이유등으로 한쪽에 몰린 데이터들만 추출된다거나)
			등으로 문제가 생길수있음
		
		낮은 품질의 데이터
			데이터가 에러,이상치,잡음등으로 가득하면 이상한패턴을 찾거나 패턴을 찾기 어려워하기때문에 
			데이터 정제에 시간을 투자해야함(일부샘플이 이상치인게 확실하면 지우거나,특성이 빠져있으면 샘플을 무시하거나 중간값채우거나)
			
		관련 없는 특성
			특성이 관련없는특성만 가득하거나 그러면 막 이상한값이 나올수도있음
			그래서 훈련에 사용할 좋은 특성들을 찾고,특성들을 합쳐서 더 유용한 특성을 만들어야함(특성추출)
			
	나쁜 알고리즘:
		데이터 과대적합
			훈련 데이터에 너무 잘 맞지만, 일반성이 떨어진다는 것
			세트에 잡음이 많거나 데이터셋이 너무 작을떄(샘플링잡음)잡음이 섞인 패턴을 감지하게되면(잡음의 양에 비해 모델이 너무복잡하면),
			당연히 새로운 샘플에 일반화가 안됨
			그래서 특성을 잘 골라야하고 훈련데이터의 잡음을 줄이고,훈련데이터 양을 늘려야함
			
			모델을 단순하게하고,과대적합의 위험을 감소시키기위해 모델에 제약을 가하는걸 규제 라고 함
			만약 a+bxn=c 라는 식에서 n이 특성이면,건드릴수있는건 a와 b인데 이 두개를 바꿀수 있는 자유도를 모델에 주면,(자유도2)
			모델은 직선의 절편과 기울기를 조절할수 있는데,우리가 여기서 a=0으로 고정시켜버리면 할수있는 자유도가 줄어드니까(자유도1)
			모델이 간단해지고 막 데이터에 끼워맞춘 식을 만들기 어려워지게됨
			딱 0 이렇게 안해도 수정은 할수있지만 값의 최소,최대값을 정해두면 자유도 1과 2사이 어딘가에 위치한 모델이 됨
			
			여기서 저렇게 딱 0이렇게 모델에서 변하지않고 상수로 주는걸 하이퍼파라미터라고 함
			이건 학습알고리즘에 영향을 받지않고,훈련전에 정해져있고,훈련하는동안 상수로 남아있음
			머신러닝에서 하이퍼파라미터 튜닝은 매우 중요한 과정임
			
		데이터 과소적합
			과대적합의 반대,모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할때 일어남
			이걸 해결하려면 모델파라미터를 늘리거나,더 좋은 특성을 제공하거나(특성추출등을 해서),모델의 제약을 줄여야함
		
	테스트와 검증
		
		모델이 얼마나 잘 일반화 될지 아는 방법은 가장 확실한건 실제서비스에 넣고 모니터링하는거지만,리스크가 크니까
		훈련데이터를 훈련세트와 테스트 세트로 나눠서
		훈련세트만 가지고 훈련을 한 다음에 테스트 세트를 일반화샘플처럼 사용해서 모델을 테스트함
		새로운 샘플에 대한 오류 비율을 일반화 오차라고 하는데,테스트세트로 평가해서 이 오차에 대한 추정값을 얻을수 있음
		훈련 오차가 낮지만(훈련세트에서 모델의 오차가 적지만)일반화 오차가 크다면 이건 과대적합됐다는것
		
		보통 데이터의 80퍼센트를 훈련에 쓰고,20퍼센트를 테스트용으로 떼어두는데,데이터크기에 따라 테스트데이터를 줄여도됨
		
		하지만 일반화오차를 테스트세트에서 여러번 측정하면,모델과 하이퍼파라미터가 테스트세트에 최적화된 모델을(과대적합된)만들기떄문에
		모델이 새로운 데이터에 잘 작동하지 않을수있음
		그래서 보통 홀드아웃 검증이라는 훈련세트의 일부를 떼어내서 여러 후보모델을 평가하고 가장 좋은 하나를 선택함
		이 홀드아웃세트를 검증세트라고 함
		
		구체적으로는 검증세트가 빠진 훈련세트로 다양한 하이퍼파라미터값을 가진 여러 모델을 훈련해서 프로토타입1을 만들고,
		검증세트에서 가장 높은 성능을 내는 모델을 선택해서 전체훈련세트에서 다시 훈련해 최종모델(프로토타입2)를 만들고,
		이걸 테스트세트에서 평가해서 일반화오차를 추정함
		
		이건 보통 잘 작동하는데,검증세트가 너무작으면 모델이 정확하게 평가가 안되고,검증세트가 너무 크면 훈련세트가 너무 작아지기떄문에
		최종모델은 전체훈련세트에서할건데 너무 작은데서 하면 적절하지 않음
		그래서 이걸 해결하는 방법은, 작은 검증 세트를 여러개 만들어서 사용해 반복적인 교차검증을 수행하는것
		모든 검증세트를 모든 모델에 돌린다음 값을 평균내서 고르면 훨씬 정확한 성능을 측정할수있지만,단점으로 훈련시간이 많이늘어남
		
		데이터 불일치
			만약 사진찍어서 뭔지 맞추는걸 하고싶은데 웹에서 긁어오면 엄청나게 정확한 데이터들만 있기때문에
			실제 데이터를 완벽하게 대표하지 못할수 있음
			이럴때 진짜 찍은 사진을 반반나눠서 검증세트와 테스트세트에 넣고(중복되거나 비슷한사진이 들어가면안됨)돌려야하는데
			보통 이러면 성능이 매우 나쁘게 나옴
			
			근데 이게 데이터불일치때문인지,과대적합인지 알기 어려운데 이걸 구분하는 방법은
			웹에서 긁은 데이터를 떼어내서(훈련데이터를 떼서)훈련개발세트를 만들고(훈련데이터에서 분리함)
			훈련세트에서 돌린다음 훈련개발세트에서 평가했는데 잘 작동하면 과대적합이 아니고 데이터불일치니까 데이터 전처리를 해야하고
			만약 성능이 나쁘면 과대적합이니까 규제하거나 데이터를 늘리거나 데이터정제를 해야함
			
			그래서 머신러닝은 학습용 데이터셋에서 종속변수와 독립변수의 관계를 분석해서 이거로 모델을 만드는거기떄문에
			철저하게 학습용 데이터셋에 종속됨
			
2.머신러닝 처음부터끝까지(회귀)
		
머신러닝의 순서는
	1.큰그림보기
	2.데이터 구하기
	3.데이터에서 뭘쓸지 알기위해 탐색하고 시각화하기
	4.데이터 전처리
	5.모델 선택,훈련
	6.모델 파라미터조정
	7.솔루션 제시
	8.런칭,유지보수
순서
1.큰그림보기
	
	무슨 데이터를 써서 무슨 값을 뽑을건지 알고,
	이걸 만드는 이유(이걸 최종값으로 쓸건지 다음모델에 인풋값으로 쓸건지 등) 
	이유를 알고 얼마나 정확하게 넣어야하는지 시간투자 얼마나해야하는지 등을 판단,
	현재 솔루션이 있는지,있다면 해결방법에 관한 정보랑 참고성능으로 사용할수있음
	데이터셋이 어떻게 구성된건지 
	등을 토대로 사용할 모델과 방식을 찾아야함
	
	다음으로 성능 측정 지표를 선택해야함
	회귀에서 주로 사용하는건 평균 제곱근 오차(RMSE)를 사용,만약 이상치로 보이는 구역이 많으면 평균 절대 오차(MAE)를 사용할수있음
	
	마지막으로 지금까지 짜둔 플랜을 적어서 확인해봐야됨
	만약 다음거에 값으로 들어갈줄알았는데 분류로 들어가면 아예 문제가 달라지니(회귀->분류) 확인해야함
	
2.데이터 가져오기
	
	주피터,넘파이,판다스,맷플롤립,사이킷런등을 설치하고(그냥 아나콘다 주피터노트북쓰는게 젤편할듯)
	
	1.데이터 가져오기 
		데이터 가져오는 함수 만들어두는게 편함
		
		import os
		import tarfile
		import urllib
		
		다운로드루트='url'
		저장위치=os.path.join("폴더","폴더2")
		다운로드파일=다운로드루트+"받을파일"
		
		def 데이터저장함수 (다운로드파일,저장위치):
			os.makedirs(저장위치,exist_ok=True) 저장위치로 디렉토리만들고
			저장파일=os.path.join(저장위치,"파일명") 파일저장할거 만들어두고
			urllib.request.urlretrieve(다운로드파일,저장파일) 파일 받아서 저장파일에 넣고(.tgz파일)
			파일저장=tarfile.open(저장파일)
			파일저장.extractall(path=저장위치) 파일생성(csv파일)
			파일저장.close() 닫기
		
	
	2. 데이터 읽기 
		데이터 읽기함수는
		
		import pandas as pd
		
		def 데이터읽기함수(저장위치):
			파일=os.path.join(저장위치,파일명)
			return pd.read_csv(파일)
		
		하면 데이터프레임 객체를 리턴함
		
		데이터를 받았으면 데이터를 대충 보고(값이랑 자료형같은거,오브젝트면 csv니까 문자열일거고,그게 랜덤문자인지 범주인지 확인)
		
		a=데이터읽기함수(저장위치)
		a.head() 5개만출력
		a.info() 자료형과 널이아닌값등 확인가능
		a.describe()숫자형 특성의 요약정보(max,min,평균,표준편차,25%,50%,75% 등)
		
		%matplotlib inline  주피터노트북에서 바로 출력하게 함 주피터노트북의 매직커맨드 
		import matplotlib.pyplot as plt
		a.hist(bins=50,figsize=(20,15)) 히스토그램 출력
		plt.show()
		
		히스토그램을 보고 데이터의 특이사항을 찾아야함
		데이터가 어떤식으로 스케일링 되어있는지,상한 하한이 있는지,상한하한이 있다면 어떻게 처리할것인지,한계값밖은 어떻게처리할것인지
		특성들 스케일 어떻게 맞출것인지,왼쪽이나 오른쪽이 두껍거나 그러면 종모양으로 어떻게 바꿀것인지 등
	
	
	
	
	3.테스트세트 만들기 
		대충 보고 테스트세트를 떼어놓고 나선 테스트세트는 절대 들여다보면 안됨
		막 테스트세트의 패턴보고 끼워맞추기 할수있게되기때문
		
		테스트세트 만드는 가장 쉬운방법은 랜덤으로 퍼센트만큼 꺼내는것
		
		import numpy from np
		
		def 랜덤테스트분할(데이터,테스트비율):
			데이터셔플=np.random.permutation(len(데이터))
			테스트사이즈=int(len(데이터)*테스트비율)
			테스트세트=데이터셔플[:테스트사이즈]
			트레인세트=데이터셔플[테스트사이즈:]
			리턴 데이터.iloc[트레인세트],데이터.iloc[테스트세트]
		
		트레인,테스트=랜덤테스트분할(데이터,0.2)
			
		이렇게 만들면됨
		
		단 이렇게하면 여러번 만들면(매번시드가 달라서)전체를 다 보게 되니까 
		샘플의 키값의 해시값으로 나누는게 젤 정확함(데이터가 늘어날수도있으니)
		
		from zlib import crc32
		
		def 해시체크(id,테스트비율):
			return crc32(np.int64(id))& 0xffffffff<test_ratio *2**32
		def 해시데이터분할(데이터,테스트비율,id컬럼이름):
			id=데이터[id컬럼이름]
			테스트세트=id.apply(lambda id_:해시체크(id_,테스트비율)
			return 데이터.loc[~테스트세트],데이터.loc[테스트세트]  트레이세트 테스트세트 순

		데이터프레임.loc[]는  a.loc[로우,컬럼]순으로 문자열로 검색 iloc는 인덱스로 검색
		
		만약 프라이머리키가 없어서 만들어야할때 행의 인덱스를 사용하면 새 데이터는 반드시 데이터의 끝에 추가해야하고,
		어떤데이터도 삭제되면 안됨
		이게 불가능하면 여러개를 조합해서 만들어야하는데 두개조합하면 유일성이 보장되어야함
		
		그냥 제일 편하게 분할하는건 사이킷런꺼 쓰는건데
		
		from sklearn.model_selection import train_test_split
		
		트레인세트,테스트세트=train_test_split(데이터,test_size=0.2,random_state=42) 하면 42시드로 20%만큼 분할해서나옴 
	
	
		3-2.계층샘플링
			근데 이렇게 완전히 무작위로 하면 데이터셋이 많이 크면 괜찮은데,안그러면 데이터 편향이 생길 확률이 큼
			만약 여자비율이 61퍼센트고 남자가 39퍼센트면, 여자61명 남자 39명을 뽑아야 하는데 랜덤돌리면 오차가 생겨서 여자 55명 남자 45명
			이렇게 뽑힐수가 있음(숫자가 작을때)
			
			그래서 계층에서 비율을 맞춰서 랜덤으로 뽑아야 하는데 이걸 계층적 샘플링이라고 함
			
			계층을 만드는 방법은
			
			데이터["새로만드는컬럼"]=pd.cut(데이터["목표컬럼"],
									bins[0,1.5,3.0,np.inf], 계층 범위
									labels=[1,2,3,4]) 계층별이름
			
			데이터['새로만드는컬럼'].hist() 데이터 보기
			
			이렇게 만들어서 계층샘플링을 하면 됨
			
			
			계층샘플링은 
			from sklearn.model_selection import StratifiedShuffleSplit
			
			split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42) 트레인세트1개 테스트사이즈0.2 시드42
			for 트레인인덱스,테스트인덱스 in split.split(데이터,데이터["새로만드는컬럼"]):
				트레인세트=데이터.loc[트레인인덱스]
				테스트세트=데이터.loc[테스트인덱스]
			
			이렇게하면 새로만드는컬럼에 있는 비율대로 랜덤하게 잘라서 트레인테스트 나눠담음
			
			그리고 나서 새로만드는컬럼을 삭제해주면됨
			
			for 세트 in (트레인세트,테스트세트):
				세트.drop("새로만드는컬럼",axis=1,inplace=True) axix는 행인지열인지 선택 1이면 열을따라동작(세로줄제거)
														  inplace는 자기자신에 바로 적용할건지 선택(세트=세트drop()안해도된다는거)
			
			
3.데이터 시각화
	1.지리적 데이터 시각화 
		일단 훈련세트떼뒀는데 건드리지 않게 복사본 만들어서 사용(훈련세트는 더이상 직접 작업하지않음 이상치제거같은거 빼곤)
		
		데이터=트레인세트.copy()
				  
		위도 경도같은게 있으면
		
		데이터.plot(kind="scatter"(종류=산점도), x="위도", y="경도",alpha=0.1(하나당 밝기=0.1)
				  s=데이터["구역인구"]/100(원의반지름=),label="구역인구"(라벨),figsize=(10,7)(차트크기),
				  c="가격"(색="가격),cmap=plt.get_cmat("jet")(색상종류뭐쓸건지),colorbar=True(옆에 바 띄울건지)
		)
		plt.legend() 범례띄우기
		이렇게 산점도로 나타내서 볼수있음
	   
	2.상관계수
		데이터셋이 별로 안크면 모든 특성간의 표준 상관계수를 corr()로 구할수있음
		
		corr값=데이터.corr()
		
		corr값["원하는컬럼"].sort_values(ascending=False)
		
		하면 원하는컬럼의 증가감소에 따른 상관관계가 나오는데,선형적인거만 알수있지 비선형적인건 알수없음(x2=y같은 2차함수이상)
		그리고 상관관계는 기울기와는 상관없이 진짜 얼마나 관계있는지가 나옴
		
		이걸 그래프로 보는 방법은
		
		from pandas.plotting import scatter_matrix
		
		원하는컬럼=["1번컬럼","2번컬럼","3번컬럼","4번컬럼"]
		scatter_matrix(데이터[원하는컬럼],figsize=(12,8))
		
		하면 그래프가 나오는데 여기서 이상치나 상관관계같은거 체크하면됨,이상치가 있으면 없애주는게 좋음
	
	
	3.특성 조합
		마지막으로 데이터를 섞어서 좀 더 높은 상관계수를 뽑아낼수 있을수도있음
		특정 지역의 방 갯수는 사람수를 모르면 주택 가격에 영향을 미치는게 낮지만,
		특성 조합으로 가구당 방 갯수를 구할수있으면 상관관계가 높아짐
		
		대충 데이터["가구당방갯수"]=데이터["총 방수"]/데이터[가구수]
		이런식으로 특성을 조합해서 추가해서 상관관계를 확인하면 좋은특성이 나올수있음
		
4.데이터 전처리
	데이터 전처리는 수동으로 하드코딩해서 하면 안되고 어지간하면 함수만들어서 자동화 해야함
	이유는
		어떤 데이터셋이 들어와도(새로운 데이터셋이) 데이터변환이 쉬움
		다른 프로젝트에 사용할수있는 변환 라이브러리를 쌓게됨
		실제 시스템에서 알고리즘에 새 데이터를 주입하기 전에 변환시킬떄 이함수를 사용할수 있음
		여러 데이터 변환을 쉽게 시도할수있고 어떤 조합이 가장 좋은지 확인하기 편리함
	
	이거도 맨처음에 훈련세트를 복사하고
	레이블값을 떼어내서 따로 넣어두고 데이터에서 레이블값을 제거함
	
	1.데이터 정제
		데이터에 누락된값(null)이 있으면 이상하게 동작하니까 이걸 처리해줘야하는데 방법은 3개임
			1.해당 로우를 제거              
			2.해당 컬럼을 제거              
			3.어떤 값으로 채우기(0,평균값 등)   
		
			데이터.dropna(subset=['원하는컬럼'])       1번
			데이터.drop('원하는컬럼',axis =1)          2번
			데이터['원하는컬럼'].fillna(0,inplace=True)3번
			
			만약 3번으로 채웠으면 저 채운값을저장해둬야 나중에 테스트세트나 실제 운영할때 새로운데이터에 있는 누락된값을 채울수있음
			
			사이킷런을 쓰면 채우기 쉬운데 중간값같은건 수치형에서만 계산 가능하기때문에 텍스트같은건 제외해서 뽑아둔다음 계산하고 합쳐야함
			
			from sklearn.impute import SimpleImputer
			
			imputer=SimpleImputer(strategy="median") 중간값으로 세팅
			데이터숫자=데이터.drop(문자열데이터컬럼들,axis=1)
			imputer.fit(데이터숫자)
			
			이러면 imputer안에 누락된값들이 다 중간값으로 채워져있음
			이걸 보고싶으면
			imputer.statistics_ 하면 각 특성의 평균값을 볼수있음
			
			이걸 넘파이 배열로 받을수도 있는데 
			
			X=imputer.transform(데이터숫자) 이러면 배열로 받아지고 다시 데이터 프레임으로 바꿀려면
			
			데이터=pd.DataFrame(X,columns=데이터숫자.columns,index=데이터숫자.index)
			이러면 데이터프레임으로 되돌릴수있음

			
			당장은 한두개특성만 비어있는칸이 있다고해도 실제 동작할때는 어떤값이 누락될지 알수없으니까 모든값에 imputer돌리는게 바람직
			그리고 물론 테스트세트에서는 훈련세트에서의 평균값을 그대로 넣어야함 테스트세트의 평균값넣으면안됨
			
	tmi)사이킷런은
		추정기:데이터셋으로 모델 파라미터를 추정하는객체를 추정기라고 부름,위에서 평균값을 알아내는 imputer은 추정기임
			추정은 fit로 이뤄지고 하나의 매개변수로 하나의 데이터셋만 전달하고,추정과정에서 필요한 다른 매개변수들은 전부 
			하이퍼파라미터로 간주되고 인스턴스변수로 저장됨
		변환기:데이터셋을 변환하는 추정기를 변환기라고 함,imputer은 변환기임
			 데이터셋을 변환할때는 데이터셋을 매개변수로 받은 transform()메서드가 수행하고 변환된 데이터셋을 반환함
		예측기:어떤 추정기들은 주어진 데이터셋에 대해 예측을 만들수가 있는데 전장에 나온 선형모델이 예측기임
			예측기의 predict()는 새로운 데이터셋을 받아 이에 맞는 예측값을 반환함
			그리고 테스트세트를 사용해 예측의 품질을 측정하는 score()메서드를 가지고있음
		보통 그래서 사이킷런으로 만들때는 pipeline를 써서 여러 변환기를 연결한다음 마지막에 추정기를 배치하는식으로 쉽게만들수있고,
		사이킷런은 대부분의 매개변수에 기본값이 있어서 어지간하면 동작함
	
	2.텍스트와 범주형특성
		범주형 특성들은 그냥 숫자로 변환시켜서 사용하는게 좋음 
		대부분 머신러닝 알고리즘은 숫자를 다루기때문
		
		숫자로 바꾸려면
		
		from sklearn.preprocessing import OrdinalEncoder
		ord=OrdinalEncoder()
		데이터2=ord.fit_transform(데이터)
		
		이러면 모든 범주형 특성들이 같은거끼리 1,2,3,4,5등으로 표시되고
		범주형특성들의 카테코리 목록은 ord.categories_를 하면 리스트가 반환됨
		
		근데 이렇게하면 알고리즘이 붙어있는거끼리 가깝다고 생각하기때문에,
		실제로 그렇지않다면 카테고리별 이진특성을 만들어 해결함(특성을 전부 만들어 두고 하나가 1이면 나머지가 다 0이되는식)
		이걸 원-핫 인코딩이라고 부름
		
		하는방법은
		
		from sklearn.preprocessing import OneHotEncoder
		
		OHenco=OneHotEncoder()
		데이터원핫=OHenco.fit_transform(데이터)
		
		이러면 원핫식으로 변환된게 나옴
	3.나만의 변환기
		변환기를 만들려면 사이킷런은 덕타이핑을 지원하므로
			fit() self를 반환
			transform()
			fit_transform()
		메서드를 구현한 클래스를 만들면 됨 
		마지막 fit_transform는 TransformerMixin을 상속하면 자동으로 생성되고
		BaseEstimator을 상속하고 생성자에 *args나 **kargs를 사용하지 않으면 하이퍼파라미터 튜닝에 필요한
			get_params()
			set_params()
		를 추가로 얻을수있음 
		
		예시로
		from sklearn.base import BaseEstimator,TransformerMixin

		rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6

		class CombinedAttributesAdder(BaseEstimator,TransformerMixin):
			def __init__(self,add_bedrooms_per_room=True): #생성자
				self.add_bedrooms_per_room=add_bedrooms_per_room
			def fit(self,X,y=None):
				return self
			def transform(self,X):
				rooms_per_household=X[:,rooms_ix]/X[:,households_ix]
				population_per_household=X[:,population_ix]/X[:,households_ix]
				if self.add_bedrooms_per_room:
					bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]
					return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]
				else:
					return np.c_[X,rooms_per_household,population_per_household]
				
				
		
		
		위에서 특성 조합하는거 자동화한 함수
		init에서 베드퍼룸특성을 만들어주고 
		fit은 자기자신을 보내주는거만 하고
		트랜스폼에서 특성을 만들어서 리턴해줌
		그리고 get_params과 set_params은 파이프라인과 그리드탐색에 꼭 필요한 메소드라서 무조건 BaseEstimator를 상속해야함
		
	4.특성 스케일링
		몇가지 알고리즘을 빼면 머신러닝은 입력 숫자들의 스케일이 다르면 잘 작동하지 않음
		그래서 숫자들의 스케일링을 맞춰줘야하는데
		대표적으로
			min-max스케일링(정규화)
			표준화
		가 있음
		
			
	
	
			
			
			







