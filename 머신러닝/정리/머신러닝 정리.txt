1.기초

머신러닝은 작업의 성능이 경험으로 인해 변하면 머신러닝 프로그램임

시스템이 학습할때 사용하는 샘플은 훈련세트
각 훈련데이터는 훈련샘플(샘플)

스팸메일분류기에서는
작업=스팸메일구분
경험=훈련데이터
성능=정확도(이건 자기가 원하는거로 직접 정해야하는데 분류에선 정확도 보통씀)

머신러닝을 통해 우리가 몰랐던 연관관계나 추세를 발견하는것은 데이터 마이닝

레이블의 종류에 따른 분류

	지도학습:훈련데이터에 레이블(정답인지 아닌지 표시)가 있는데이터로 학습하는방법

		대표적으로 분류가 전형적인 지도학습임(스팸메일분류같은)

		또 다른 예는 회귀가 있음
		회귀는 예측변수를 통해 타깃수치를 예측하는것
		예를들어 차 브랜드,주행거리등을 파라미터로 받아서 중고차값을 뱉는식
		
	비지도학습:비지도학습은 훈련데이터에 레이블이 없는상태에서 학습하는것
			대표적으로 군집,시각화등이 있음
			시각화는 레이블이 없는 데이터를 넣으면 눈으로 볼수있는 표현을 만들어줌
			예로 강아지 고양이 트럭등을 군집지어져있는걸 시각화시킨다던지 할수있음
			
	준지도학습:준지도학습은 일부만 레이블이 있는 데이터고 나머지는 레이블이 안된 데이터인상태에서 학습하는것
			예를들어서 사진여러개에서 사진들에 사람a,사람b가 사진 1,2에 있는지는 비지도학습으로 할수있지만,
			그사람이 누군지는 지도학습으로 해야하니까 
			사진에 사람abc태그하는건 비지도학습으로 하고 그 사람 이름같은거 적는건 지도학습으로 하는식
			
	강화학습:강화학습은 환경을 관찰해서 행동을 실행하고,거기따른 보상또는 벌점을 받아서 큰 보상을 따르게 행동방식을 수정하면서 반복하는것
		  게임ai나 보행로봇등에 주로 쓰임

실시간인지 아닌지에 따른 분류

	배치학습:시스템을 오프라인에서 학습 다 시킨다음에 그걸 적용하는식,온라인상에서 입력이들어와도 학습하지않고 출력만 함
		  배치학습이 새로운데이터를 학습하려면 전체데이터를 써서 처음부터 다시훈련해야함
		
	온라인학습:시스템을 하나씩,또는 일정 묶음단위로(미니배치) 주입해서 시스템을 계속 훈련시킴
		   이건 빠른 변화에 적응해야하는 시스템에 적합함
		   
		   온라인학습은 데이터를 학습하고나면 그 데이터는 필요없으니까(전으로 돌릴필요가없으면)삭제해도 됨
		   
		   그리고 메인메모리에 들어가지 않는 크기의 데이터셋을 학습할때도 이걸 사용할수있음
		   미니배치를 계속 넣는식으로 돌리면됨(외부메모리학습,배치학습인데 데이터클때 온라인학습방식으로 돌리는거)
		   
		   온라인학습에서 중요한 파라미터는 변화하는 데이터에 얼마나 빨리 적응할것인지를 나타내는 학습률
		   학습률을 높게하면 새로운 데이터에 빨리 적응하지만,이전 데이터를 금방 잊어버림
		   학습률을 낮게하면 새로운 데이터에 적응하는데는 오래걸리지만,새로운 데이터의 잡음같은거에 덜민감해짐
		   
		   온라인학습의 가장 큰 단점은 악의적데이터가 주입될때 시스템성능이 나빠질수 있다는것
		   그래서 시스템을 모니터링하거나 입력데이터를 모니터링해서 이런걸 걸러내야함

어떻게 일반화 되는가에 따른 분류
	사례 기반 학습:훈련샘플을 기억해서 유사도등을 측정해서 비교하는식으로 일반화 하는 방식
			  특성1과 특성2가 있는 그래프가 있을때,주변에 있는 값 중 가장 가깝거나 많은거로 분류하는식
	
	모델 기반 학습:샘플들의 모델을 만들어서 예측에 사용하는 방식
			  식을 만들어서  거기에 특성을 넣고 가공해서(절편을주거나,민감도를 조절하거나 해서) 대충 근처값이 나오게 하는식임
			  
			  모델이 얼마나 좋은지는 측정지표를 정해야 하는데
			  모델이 얼마나 좋은지 측정하는 효용함수나,모델이 얼마나 나쁜지 측정하는 비용함수가 있음
			  
			  선형회귀에서는 선형모델의 예측과 훈련데이터 사이의 거리를 재는 비용함수를 사용해서 
			  파라미터를 조절해 이 거리를 최소화하는게 목표임 이걸 모델을 훈련시킨다고 함
			  
즉, 머신러닝은
	데이터를 분석해서
	모델을 선택하고
	훈련데이터로 모델을 훈련시킴(비용함수가 제일 작아지는값을 찾음)
	마지막으로 새로운 데이터를 넣어서 예측을하고 잘되기를 기대함
	
머신러닝에서 문제가 되는 가장 큰 두가지는
	나쁜데이터:
		충분하지 않은량의 데이터
		
		대표성이 없는 훈련 데이터
			우리가 일반화 하기를 원하는 새로운 사례를 잘 대표하는 데이터를 넣어야함
			근데 샘플이 작으면 샘플링 잡음(우연에의한 대표성없는 데이터)
			샘플이 커도 추출방법이 잘못되면 샘플링 편향(무슨 이유등으로 한쪽에 몰린 데이터들만 추출된다거나)
			등으로 문제가 생길수있음
		
		낮은 품질의 데이터
			데이터가 에러,이상치,잡음등으로 가득하면 이상한패턴을 찾거나 패턴을 찾기 어려워하기때문에 
			데이터 정제에 시간을 투자해야함(일부샘플이 이상치인게 확실하면 지우거나,특성이 빠져있으면 샘플을 무시하거나 중간값채우거나)
			
		관련 없는 특성
			특성이 관련없는특성만 가득하거나 그러면 막 이상한값이 나올수도있음
			그래서 훈련에 사용할 좋은 특성들을 찾고,특성들을 합쳐서 더 유용한 특성을 만들어야함(특성추출)
			
	나쁜 알고리즘:
		데이터 과대적합
			훈련 데이터에 너무 잘 맞지만, 일반성이 떨어진다는 것
			세트에 잡음이 많거나 데이터셋이 너무 작을떄(샘플링잡음)잡음이 섞인 패턴을 감지하게되면(잡음의 양에 비해 모델이 너무복잡하면),
			당연히 새로운 샘플에 일반화가 안됨
			그래서 특성을 잘 골라야하고 훈련데이터의 잡음을 줄이고,훈련데이터 양을 늘려야함
			
			모델을 단순하게하고,과대적합의 위험을 감소시키기위해 모델에 제약을 가하는걸 규제 라고 함
			만약 a+bxn=c 라는 식에서 n이 특성이면,건드릴수있는건 a와 b인데 이 두개를 바꿀수 있는 자유도를 모델에 주면,(자유도2)
			모델은 직선의 절편과 기울기를 조절할수 있는데,우리가 여기서 a=0으로 고정시켜버리면 할수있는 자유도가 줄어드니까(자유도1)
			모델이 간단해지고 막 데이터에 끼워맞춘 식을 만들기 어려워지게됨
			딱 0 이렇게 안해도 수정은 할수있지만 값의 최소,최대값을 정해두면 자유도 1과 2사이 어딘가에 위치한 모델이 됨
			
			여기서 저렇게 딱 0이렇게 모델에서 변하지않고 상수로 주는걸 하이퍼파라미터라고 함
			이건 학습알고리즘에 영향을 받지않고,훈련전에 정해져있고,훈련하는동안 상수로 남아있음
			머신러닝에서 하이퍼파라미터 튜닝은 매우 중요한 과정임
			
		데이터 과소적합
			과대적합의 반대,모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할때 일어남
			이걸 해결하려면 모델파라미터를 늘리거나,더 좋은 특성을 제공하거나(특성추출등을 해서),모델의 제약을 줄여야함
		
	테스트와 검증
		
		모델이 얼마나 잘 일반화 될지 아는 방법은 가장 확실한건 실제서비스에 넣고 모니터링하는거지만,리스크가 크니까
		훈련데이터를 훈련세트와 테스트 세트로 나눠서
		훈련세트만 가지고 훈련을 한 다음에 테스트 세트를 일반화샘플처럼 사용해서 모델을 테스트함
		새로운 샘플에 대한 오류 비율을 일반화 오차라고 하는데,테스트세트로 평가해서 이 오차에 대한 추정값을 얻을수 있음
		훈련 오차가 낮지만(훈련세트에서 모델의 오차가 적지만)일반화 오차가 크다면 이건 과대적합됐다는것
		
		보통 데이터의 80퍼센트를 훈련에 쓰고,20퍼센트를 테스트용으로 떼어두는데,데이터크기에 따라 테스트데이터를 줄여도됨
		
		하지만 일반화오차를 테스트세트에서 여러번 측정하면,모델과 하이퍼파라미터가 테스트세트에 최적화된 모델을(과대적합된)만들기떄문에
		모델이 새로운 데이터에 잘 작동하지 않을수있음
		그래서 보통 홀드아웃 검증이라는 훈련세트의 일부를 떼어내서 여러 후보모델을 평가하고 가장 좋은 하나를 선택함
		이 홀드아웃세트를 검증세트라고 함
		
		구체적으로는 검증세트가 빠진 훈련세트로 다양한 하이퍼파라미터값을 가진 여러 모델을 훈련해서 프로토타입1을 만들고,
		검증세트에서 가장 높은 성능을 내는 모델을 선택해서 전체훈련세트에서 다시 훈련해 최종모델(프로토타입2)를 만들고,
		이걸 테스트세트에서 평가해서 일반화오차를 추정함
		
		이건 보통 잘 작동하는데,검증세트가 너무작으면 모델이 정확하게 평가가 안되고,검증세트가 너무 크면 훈련세트가 너무 작아지기떄문에
		최종모델은 전체훈련세트에서할건데 너무 작은데서 하면 적절하지 않음
		그래서 이걸 해결하는 방법은, 작은 검증 세트를 여러개 만들어서 사용해 반복적인 교차검증을 수행하는것
		모든 검증세트를 모든 모델에 돌린다음 값을 평균내서 고르면 훨씬 정확한 성능을 측정할수있지만,단점으로 훈련시간이 많이늘어남
		
		데이터 불일치
			만약 사진찍어서 뭔지 맞추는걸 하고싶은데 웹에서 긁어오면 엄청나게 정확한 데이터들만 있기때문에
			실제 데이터를 완벽하게 대표하지 못할수 있음
			이럴때 진짜 찍은 사진을 반반나눠서 검증세트와 테스트세트에 넣고(중복되거나 비슷한사진이 들어가면안됨)돌려야하는데
			보통 이러면 성능이 매우 나쁘게 나옴
			
			근데 이게 데이터불일치때문인지,과대적합인지 알기 어려운데 이걸 구분하는 방법은
			웹에서 긁은 데이터를 떼어내서(훈련데이터를 떼서)훈련개발세트를 만들고(훈련데이터에서 분리함)
			훈련세트에서 돌린다음 훈련개발세트에서 평가했는데 잘 작동하면 과대적합이 아니고 데이터불일치니까 데이터 전처리를 해야하고
			만약 성능이 나쁘면 과대적합이니까 규제하거나 데이터를 늘리거나 데이터정제를 해야함
			
			그래서 머신러닝은 학습용 데이터셋에서 종속변수와 독립변수의 관계를 분석해서 이거로 모델을 만드는거기떄문에
			철저하게 학습용 데이터셋에 종속됨
			
2.머신러닝 처음부터끝까지(회귀)
		
머신러닝의 순서는
	1.큰그림보기
	2.데이터 구하기
	3.데이터에서 뭘쓸지 알기위해 탐색하고 시각화하기
	4.데이터 전처리
	5.모델 선택,훈련
	6.모델 파라미터조정
	7.솔루션 제시
	8.런칭,유지보수
순서
1.큰그림보기
	
	무슨 데이터를 써서 무슨 값을 뽑을건지 알고,
	이걸 만드는 이유(이걸 최종값으로 쓸건지 다음모델에 인풋값으로 쓸건지 등) 
	이유를 알고 얼마나 정확하게 넣어야하는지 시간투자 얼마나해야하는지 등을 판단,
	현재 솔루션이 있는지,있다면 해결방법에 관한 정보랑 참고성능으로 사용할수있음
	데이터셋이 어떻게 구성된건지 
	등을 토대로 사용할 모델과 방식을 찾아야함
	
	다음으로 성능 측정 지표를 선택해야함
	회귀에서 주로 사용하는건 평균 제곱근 오차(RMSE)를 사용,만약 이상치로 보이는 구역이 많으면 평균 절대 오차(MAE)를 사용할수있음
	
	마지막으로 지금까지 짜둔 플랜을 적어서 확인해봐야됨
	만약 다음거에 값으로 들어갈줄알았는데 분류로 들어가면 아예 문제가 달라지니(회귀->분류) 확인해야함
	
2.데이터 가져오기
	
	주피터,넘파이,판다스,맷플롤립,사이킷런등을 설치하고(그냥 아나콘다 주피터노트북쓰는게 젤편할듯)
	
	1.데이터 가져오기 
		데이터 가져오는 함수 만들어두는게 편함
		
		import os
		import tarfile
		import urllib
		
		다운로드루트='url'
		저장위치=os.path.join("폴더","폴더2")
		다운로드파일=다운로드루트+"받을파일"
		
		def 데이터저장함수 (다운로드파일,저장위치):
			os.makedirs(저장위치,exist_ok=True) 저장위치로 디렉토리만들고
			저장파일=os.path.join(저장위치,"파일명") 파일저장할거 만들어두고
			urllib.request.urlretrieve(다운로드파일,저장파일) 파일 받아서 저장파일에 넣고(.tgz파일)
			파일저장=tarfile.open(저장파일)
			파일저장.extractall(path=저장위치) 파일생성(csv파일)
			파일저장.close() 닫기
		
	
	2. 데이터 읽기 
		데이터 읽기함수는
		
		import pandas as pd
		
		def 데이터읽기함수(저장위치):
			파일=os.path.join(저장위치,파일명)
			return pd.read_csv(파일)
		
		하면 데이터프레임 객체를 리턴함
		
		데이터를 받았으면 데이터를 대충 보고(값이랑 자료형같은거,오브젝트면 csv니까 문자열일거고,그게 랜덤문자인지 범주인지 확인)
		
		a=데이터읽기함수(저장위치)
		a.head() 5개만출력
		a.info() 자료형과 널이아닌값등 확인가능
		a.describe()숫자형 특성의 요약정보(max,min,평균,표준편차,25%,50%,75% 등)
		
		%matplotlib inline  주피터노트북에서 바로 출력하게 함 주피터노트북의 매직커맨드 
		import matplotlib.pyplot as plt
		a.hist(bins=50,figsize=(20,15)) 히스토그램 출력
		plt.show()
		
		히스토그램을 보고 데이터의 특이사항을 찾아야함
		데이터가 어떤식으로 스케일링 되어있는지,상한 하한이 있는지,상한하한이 있다면 어떻게 처리할것인지,한계값밖은 어떻게처리할것인지
		특성들 스케일 어떻게 맞출것인지,왼쪽이나 오른쪽이 두껍거나 그러면 종모양으로 어떻게 바꿀것인지 등
	
	
	
	
	3.테스트세트 만들기 
		대충 보고 테스트세트를 떼어놓고 나선 테스트세트는 절대 들여다보면 안됨
		막 테스트세트의 패턴보고 끼워맞추기 할수있게되기때문
		
		테스트세트 만드는 가장 쉬운방법은 랜덤으로 퍼센트만큼 꺼내는것
		
		import numpy from np
		
		def 랜덤테스트분할(데이터,테스트비율):
			데이터셔플=np.random.permutation(len(데이터))
			테스트사이즈=int(len(데이터)*테스트비율)
			테스트세트=데이터셔플[:테스트사이즈]
			트레인세트=데이터셔플[테스트사이즈:]
			리턴 데이터.iloc[트레인세트],데이터.iloc[테스트세트]
		
		트레인,테스트=랜덤테스트분할(데이터,0.2)
			
		이렇게 만들면됨
		
		단 이렇게하면 여러번 만들면(매번시드가 달라서)전체를 다 보게 되니까 
		샘플의 키값의 해시값으로 나누는게 젤 정확함(데이터가 늘어날수도있으니)
		
		from zlib import crc32
		
		def 해시체크(id,테스트비율):
			return crc32(np.int64(id))& 0xffffffff<test_ratio *2**32
		def 해시데이터분할(데이터,테스트비율,id컬럼이름):
			id=데이터[id컬럼이름]
			테스트세트=id.apply(lambda id_:해시체크(id_,테스트비율)
			return 데이터.loc[~테스트세트],데이터.loc[테스트세트]  트레이세트 테스트세트 순

		데이터프레임.loc[]는  a.loc[로우,컬럼]순으로 문자열로 검색 iloc는 인덱스로 검색
		
		만약 프라이머리키가 없어서 만들어야할때 행의 인덱스를 사용하면 새 데이터는 반드시 데이터의 끝에 추가해야하고,
		어떤데이터도 삭제되면 안됨
		이게 불가능하면 여러개를 조합해서 만들어야하는데 두개조합하면 유일성이 보장되어야함
		
		그냥 제일 편하게 분할하는건 사이킷런꺼 쓰는건데
		
		from sklearn.model_selection import train_test_split
		
		트레인세트,테스트세트=train_test_split(데이터,test_size=0.2,random_state=42) 하면 42시드로 20%만큼 분할해서나옴 
	
	
		3-2.계층샘플링
			근데 이렇게 완전히 무작위로 하면 데이터셋이 많이 크면 괜찮은데,안그러면 데이터 편향이 생길 확률이 큼
			만약 여자비율이 61퍼센트고 남자가 39퍼센트면, 여자61명 남자 39명을 뽑아야 하는데 랜덤돌리면 오차가 생겨서 여자 55명 남자 45명
			이렇게 뽑힐수가 있음(숫자가 작을때)
			
			그래서 계층에서 비율을 맞춰서 랜덤으로 뽑아야 하는데 이걸 계층적 샘플링이라고 함
			
			계층을 만드는 방법은
			
			데이터["새로만드는컬럼"]=pd.cut(데이터["목표컬럼"],
									bins[0,1.5,3.0,np.inf], 계층 범위
									labels=[1,2,3,4]) 계층별이름
			
			데이터['새로만드는컬럼'].hist() 데이터 보기
			
			이렇게 만들어서 계층샘플링을 하면 됨
			
			
			계층샘플링은 
			from sklearn.model_selection import StratifiedShuffleSplit
			
			split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42) 트레인세트1개 테스트사이즈0.2 시드42
			for 트레인인덱스,테스트인덱스 in split.split(데이터,데이터["새로만드는컬럼"]):
				트레인세트=데이터.loc[트레인인덱스]
				테스트세트=데이터.loc[테스트인덱스]
			
			이렇게하면 새로만드는컬럼에 있는 비율대로 랜덤하게 잘라서 트레인테스트 나눠담음
			
			그리고 나서 새로만드는컬럼을 삭제해주면됨
			
			for 세트 in (트레인세트,테스트세트):
				세트.drop("새로만드는컬럼",axis=1,inplace=True) axix는 행인지열인지 선택 1이면 열을따라동작(세로줄제거)
														  inplace는 자기자신에 바로 적용할건지 선택(세트=세트drop()안해도된다는거)
			
			
3.데이터 시각화
	1.지리적 데이터 시각화 
		일단 훈련세트떼뒀는데 건드리지 않게 복사본 만들어서 사용(훈련세트는 더이상 직접 작업하지않음 이상치제거같은거 빼곤)
		
		데이터=트레인세트.copy()
				  
		위도 경도같은게 있으면
		
		데이터.plot(kind="scatter"(종류=산점도), x="위도", y="경도",alpha=0.1(하나당 밝기=0.1)
				  s=데이터["구역인구"]/100(원의반지름=),label="구역인구"(라벨),figsize=(10,7)(차트크기),
				  c="가격"(색="가격),cmap=plt.get_cmat("jet")(색상종류뭐쓸건지),colorbar=True(옆에 바 띄울건지)
		)
		plt.legend() 범례띄우기
		이렇게 산점도로 나타내서 볼수있음
	   
	2.상관계수
		데이터셋이 별로 안크면 모든 특성간의 표준 상관계수를 corr()로 구할수있음
		
		corr값=데이터.corr()
		
		corr값["원하는컬럼"].sort_values(ascending=False)
		
		하면 원하는컬럼의 증가감소에 따른 상관관계가 나오는데,선형적인거만 알수있지 비선형적인건 알수없음(x2=y같은 2차함수이상)
		그리고 상관관계는 기울기와는 상관없이 진짜 얼마나 관계있는지가 나옴
		
		이걸 그래프로 보는 방법은
		
		from pandas.plotting import scatter_matrix
		
		원하는컬럼=["1번컬럼","2번컬럼","3번컬럼","4번컬럼"]
		scatter_matrix(데이터[원하는컬럼],figsize=(12,8))
		
		하면 그래프가 나오는데 여기서 이상치나 상관관계같은거 체크하면됨,이상치가 있으면 없애주는게 좋음
	
	
	3.특성 조합
		마지막으로 데이터를 섞어서 좀 더 높은 상관계수를 뽑아낼수 있을수도있음
		특정 지역의 방 갯수는 사람수를 모르면 주택 가격에 영향을 미치는게 낮지만,
		특성 조합으로 가구당 방 갯수를 구할수있으면 상관관계가 높아짐
		
		대충 데이터["가구당방갯수"]=데이터["총 방수"]/데이터[가구수]
		이런식으로 특성을 조합해서 추가해서 상관관계를 확인하면 좋은특성이 나올수있음
		
4.데이터 전처리
	데이터 전처리는 수동으로 하드코딩해서 하면 안되고 어지간하면 함수만들어서 자동화 해야함
	이유는
		어떤 데이터셋이 들어와도(새로운 데이터셋이) 데이터변환이 쉬움
		다른 프로젝트에 사용할수있는 변환 라이브러리를 쌓게됨
		실제 시스템에서 알고리즘에 새 데이터를 주입하기 전에 변환시킬떄 이함수를 사용할수 있음
		여러 데이터 변환을 쉽게 시도할수있고 어떤 조합이 가장 좋은지 확인하기 편리함
	
	이거도 맨처음에 훈련세트를 복사하고
	레이블값을 떼어내서 따로 넣어두고 데이터에서 레이블값을 제거함
	
	1.데이터 정제
		데이터에 누락된값(null)이 있으면 이상하게 동작하니까 이걸 처리해줘야하는데 방법은 3개임
			1.해당 로우를 제거              
			2.해당 컬럼을 제거              
			3.어떤 값으로 채우기(0,평균값 등)   
		
			데이터.dropna(subset=['원하는컬럼'])       1번
			데이터.drop('원하는컬럼',axis =1)          2번
			데이터['원하는컬럼'].fillna(0,inplace=True)3번
			
			만약 3번으로 채웠으면 저 채운값을저장해둬야 나중에 테스트세트나 실제 운영할때 새로운데이터에 있는 누락된값을 채울수있음
			
			사이킷런을 쓰면 채우기 쉬운데 중간값같은건 수치형에서만 계산 가능하기때문에 텍스트같은건 제외해서 뽑아둔다음 계산하고 합쳐야함
			
			from sklearn.impute import SimpleImputer
			
			imputer=SimpleImputer(strategy="median") 중간값으로 세팅
			데이터숫자=데이터.drop(문자열데이터컬럼들,axis=1)
			imputer.fit(데이터숫자)
			
			이러면 imputer안에 누락된값들이 다 중간값으로 채워져있음
			이걸 보고싶으면
			imputer.statistics_ 하면 각 특성의 평균값을 볼수있음
			
			이걸 넘파이 배열로 받을수도 있는데 
			
			X=imputer.transform(데이터숫자) 이러면 배열로 받아지고 다시 데이터 프레임으로 바꿀려면
			
			데이터=pd.DataFrame(X,columns=데이터숫자.columns,index=데이터숫자.index)
			이러면 데이터프레임으로 되돌릴수있음

			
			당장은 한두개특성만 비어있는칸이 있다고해도 실제 동작할때는 어떤값이 누락될지 알수없으니까 모든값에 imputer돌리는게 바람직
			그리고 물론 테스트세트에서는 훈련세트에서의 평균값을 그대로 넣어야함 테스트세트의 평균값넣으면안됨
			
	tmi)사이킷런은
		추정기:데이터셋으로 모델 파라미터를 추정하는객체를 추정기라고 부름,위에서 평균값을 알아내는 imputer은 추정기임
			추정은 fit로 이뤄지고 하나의 매개변수로 하나의 데이터셋만 전달하고,추정과정에서 필요한 다른 매개변수들은 전부 
			하이퍼파라미터로 간주되고 인스턴스변수로 저장됨
		변환기:데이터셋을 변환하는 추정기를 변환기라고 함,imputer은 변환기임
			 데이터셋을 변환할때는 데이터셋을 매개변수로 받은 transform()메서드가 수행하고 변환된 데이터셋을 반환함
		예측기:어떤 추정기들은 주어진 데이터셋에 대해 예측을 만들수가 있는데 전장에 나온 선형모델이 예측기임
			예측기의 predict()는 새로운 데이터셋을 받아 이에 맞는 예측값을 반환함
			그리고 테스트세트를 사용해 예측의 품질을 측정하는 score()메서드를 가지고있음
		보통 그래서 사이킷런으로 만들때는 pipeline를 써서 여러 변환기를 연결한다음 마지막에 추정기를 배치하는식으로 쉽게만들수있고,
		사이킷런은 대부분의 매개변수에 기본값이 있어서 어지간하면 동작함
	
	2.텍스트와 범주형특성
		범주형 특성들은 그냥 숫자로 변환시켜서 사용하는게 좋음 
		대부분 머신러닝 알고리즘은 숫자를 다루기때문
		
		숫자로 바꾸려면
		
		from sklearn.preprocessing import OrdinalEncoder
		ord=OrdinalEncoder()
		데이터2=ord.fit_transform(데이터)
		
		이러면 모든 범주형 특성들이 같은거끼리 1,2,3,4,5등으로 표시되고
		범주형특성들의 카테코리 목록은 ord.categories_를 하면 리스트가 반환됨
		
		근데 이렇게하면 알고리즘이 붙어있는거끼리 가깝다고 생각하기때문에,
		실제로 그렇지않다면 카테고리별 이진특성을 만들어 해결함(특성을 전부 만들어 두고 하나가 1이면 나머지가 다 0이되는식)
		이걸 원-핫 인코딩이라고 부름
		
		하는방법은
		
		from sklearn.preprocessing import OneHotEncoder
		
		OHenco=OneHotEncoder()
		데이터원핫=OHenco.fit_transform(데이터)
		
		이러면 원핫식으로 변환된게 나옴
	3.나만의 변환기
		변환기를 만들려면 사이킷런은 덕타이핑을 지원하므로
			fit() self를 반환
			transform()
			fit_transform()
		메서드를 구현한 클래스를 만들면 됨 
		마지막 fit_transform는 TransformerMixin을 상속하면 자동으로 생성되고
		BaseEstimator을 상속하고 생성자에 *args나 **kargs를 사용하지 않으면 하이퍼파라미터 튜닝에 필요한
			get_params()
			set_params()
		를 추가로 얻을수있음 
		
		예시로
		from sklearn.base import BaseEstimator,TransformerMixin

		rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6

		class CombinedAttributesAdder(BaseEstimator,TransformerMixin):
			def __init__(self,add_bedrooms_per_room=True): #생성자
				self.add_bedrooms_per_room=add_bedrooms_per_room
			def fit(self,X,y=None):
				return self
			def transform(self,X):
				rooms_per_household=X[:,rooms_ix]/X[:,households_ix]
				population_per_household=X[:,population_ix]/X[:,households_ix]
				if self.add_bedrooms_per_room:
					bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]
					return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]
				else:
					return np.c_[X,rooms_per_household,population_per_household]
				
				
		
		
		위에서 특성 조합하는거 자동화한 함수
		init에서 베드퍼룸특성을 만들어주고 
		fit은 자기자신을 보내주는거만 하고
		트랜스폼에서 특성을 만들어서 리턴해줌
		그리고 get_params과 set_params은 파이프라인과 그리드탐색에 꼭 필요한 메소드라서 무조건 BaseEstimator를 상속해야함
		
	4.특성 스케일링
		몇가지 알고리즘을 빼면 머신러닝은 입력 숫자들의 스케일이 다르면 잘 작동하지 않음
		그래서 숫자들의 스케일링을 맞춰줘야하는데
		대표적으로
			min-max스케일링(정규화)
			표준화
		가 있음
		
		정규화는 가장 간단한데 모든값이 0과 1 범위 사이에 들어가도록 값을 이동하고 스케일을 조정하면됨
		즉,데이터에서 최소값을 빼고,최댓값-최소값의 차이로 나누면 이렇게 됨
		사이킷런에서는 MinMaxScaler변환기를 제공함
		
		표준화는 평균을 빼고 표준편차로 나눠서 결과분포의 분산이 1이되게 만듬
		표준화는 범위의 상한 하한이없어 어떤 알고리즘에선 문제가 될수있지만 이상치에 영향을 덜 받음
		사이킷런에선 StandardScaler변환기를 제공함
		
	5.파이프라인
		이렇게 막 해야할게 많고 정확한순서로 실행해야하니까 PipeLine으로 순서대로 실행할수있음
		근데 일반 파이프라인은 숫자특성만 처리할수있음
		사용법은
		
		from sklearn.pipeline import PipeLine
		from sklearn.preprocessing import StandardScaler
		
		숫자파이프라인=PipeLine([
			('빈칸채우기',SimpleImputer(strategy='median')),
			('특성추가',CombinedAttributesAdder()),
			('표준화변환기',StandardScaler()),
		
		]
		)
		데이터숫자트랜스폼=숫자파이프라인.fit_transform(데이터숫자)
		
		파이프라인은 이렇게 이름과 추정기 쌍을 입력으로 받아서(마지막단계는 변환기 추정기 둘다쓸수있지만 그전엔 전부 변환기여야함),
		전단계의 출력을 다음단계의 입력으로 넣어주고(마지막단계 전엔 fit_transform 마지막단계는 fit)
		
		그리고 범주형과 숫자형을 묶어주고싶으면

		from sklearn.compose import ColumnTransformer
		
		숫자애트리뷰트=list(데이터숫자)
		범주애트리뷰트=["범주애트리뷰트1",...]
		
		풀파이프라인=ColumnTransformer([
			('숫자',숫자파이프라인,숫자애트리뷰트),
			('범주',OneHotEncoder(),범주애트리뷰트)
			
		])
		총데이터=풀파이프라인.fit_transform(데이터)
		
		이런식으로 묶어서 받을수있음
		
		여기서 OneHotEncoder는 희소행렬을 반환하는데 숫자파이프라인은 밀집행렬을 반환함
		이렇게 두개가 섞여있을땐 최종행렬의 밀집정도를 추정해서(0이아닌 원소의비율) 밀집도가 일정수치보다 낮으면 희소행렬을 반환함
		높으면 밀집행렬을 반환
		
5.모델선택과 훈련
	1.훈련세트에서 훈련과 평가
		앞에서 전처리랑 다해놨으면 그냥
		from sklearn.linear_model import LinearRegression
		
		선형모델=LinearRegression()
		선형모델.fit(총데이터,데이터레이블)
		
		하면 끝임
		
		이제 훈련세트에 있는 샘플 몇개를 넣어보려면
		
		a데이터=데이터.iloc[:5]
		a라벨=데이터레이블.iloc[:5]
		a전처리데이터=풀파이프라인.transform(a데이터)
		선형모델.predict(a전처리데이터)
		
		하고나서 a라벨과 선형모델값을 비교해보면 됨
		
		값을 아까 봤던 RMSE로 측정해보려면
		
		from sklearn.metrics import mean_squared_error
		
		데이터예측값=선형모델.predict(총데이터)   데이터 넣고 값만 받아보기 훈련x 내부에 영향끼치지않음
		선형mse=mean_squared_error(데이터레이블,데이터예측값)
		선형rmse=np.sqrt(선형mse)
		
		하면 값이 나옴
		
		여기서 오차가 크면 과소적합된거니 더 강력한모델을 쓰거나 더 좋은특성을 넣거나 규제를 감소시켜야됨
		우리가 지금한거는 규제가 없으니 더 강력한모델이나 더 좋은특성을 넣어야함
		
		만약 더 강력한 모델을 썼는데 과대적합나면 교차검증을 사용할수있음
	2.교차검증
		사이킷런에는 훈련세트를 n개로 나눠서 n번 훈련하는데 그중 n-1개로 훈련하고 n개로 평가하는식으로 할수있는 기능이 있음
		사용법은
		
		from sklearn.model_selection import cross_val_score
		
		스코어=cross_val_score(사용모델,총데이터,데이터레이블,scoring="neg_mean_squared_error",cv=10)
		모델스코어=np.sqrt(-스코어)
		
		사이킷런의 교차검증은 클수록 좋은 효용함수기때문에 mse의 반대값인 neg_mean_squared_error를 사용하고,
		계산하기전에 -붙여서 부호바꿔줘야함
		
		결과보는법은
		print(모델스코어) 점수배열 리턴되고
		print(모델스코어.mean()) 평균 리턴
		print(모델스코어.std()) 표준편차 리턴

		
		근데 이방식은 엄청 오래걸려서 비용이 비싸니까 막 아무때나 쓸수있는건 아님
		
	tmi)여러 다른 모델들을 모아서 하나의 모델을 만드는걸 앙상블 학습이라고 함
	
6.모델 세부 튜닝
	1.그리드탐색
		가장 무식한방법은 괜찮은 값이 나올때까지 수동으로 하이퍼파라미터값 다넣어보는건데 누가 이렇게해
		
		보통 저렇게 하고싶으면 GridSearchCV를 사용함
		
		사용법은
		
		from sklearn.model_selection import GridSearchCV
		
		파라미터배열=[
			{'n_estimators':[3,10,30],'max_features':[2,4,6,8]},
			{'bootstrap':[False],n_estimators:[3,10],'max_features':[2,3,4]}
		]
		
		사용모델=모델()
		
		그리드서치=GridSearchCV(사용모델,파라미터배열,cv=5,scoring='neg_mean_squared_error',
							return_train_score=True)
		그리드서치.fit(총데이터,데이터레이블)
		
		이러면 파라미터 배열 위에있는 3x4개를 평가하고 두번째에서 부트스트랩을 false로 한다음 2x3개를 평가함
		총 18개를 5번씩 90번 훈련함
		
		여기서 만약 max_features:8 n_estimators:30 이렇게 최대값이 나왔으면 올라갈수록 점수가 향상될 가능성이 있으니까
		값을 올려서 다시돌려주는게좋음
		
		만약 최적의 추정기에 직접 접근하고싶으면
			그리드서치.best_estimator_
		하면 나옴
		
		평가점수는 
			그리드서치.cv_results_
		하면 나옴
		
	2.랜덤탐색
		랜덤으로 탐색하고 싶으면 RandomizedSearchCV를 사용하면 됨 GridSearchCV와 거의 똑같음
		특히 규제처럼 설정값이 연속형이면 랜덤탐색이 권장됨
		
		장점은 1000회반복하도록하면 파라미터마다 각기 다른 1000개의 값을 탐색함
		반복횟수를 조절하는거만으로 얼마나 오래돌릴건지를 제어하기 편함
	
	3.앙상블
		최상의 모델을 연결해보는것 최상의 단일모델보다 모델의 그룹이 더 나은 성능을 발휘할때가 많음
	
	4.최상의 모델 분석
	최상의 모델을 분석하면 좋은 특성등 좋은 통찰을 얻는 경우가 많음
	예를들어 
		그리드서치.best_estimator_.feature_importances_
	하면 특성의 상대적인 중요도가 나오는데 이걸 바탕으로 덜 중요한 특성을 제외할수있음
	
	5.테스트세트 사용하기
		이거도똑같이 예측변수와 레이블을 따고 풀파이프라인에 넣고(여기서 학습하면안되니까 fit_transform이 아니라 transform해야함)
		테스트세트에서 모델 평가하면됨
		
			파이널모델=그리드서치.best_estimator_
			
			테스트데이터=테스트세트.drop('레이블',axis)
			테스트레이블=테스트세트['레이블'].copy()
			
			테스트전처리데이터=풀파이프라인.transform(테스트데이터)
			
			파이널예측=파이널모델.predict(테스트전처리데이터)
			
			파이널mse=mean_squared_error(테스트레이블,파이널예측)
			파이널rmse=nq.sqrt(파이널mse)
		
		만약 이런 추정이 얼마나 정확한지 알고싶으면 scipy.stats.t.interval()을 써서 일반화오차 95%신뢰구간을 계산할수있음
		
			from scipy import stats
			신뢰구간=0.95
			제곱오차=(파이널예측-테스트레이블)**2
			np.sqrt(stats.t.interval(신뢰구간,len(제곱오차)-1,loc=제곱오차.mean(),scale=stats.sem(제곱오차))
			
		보통 하이퍼파라미터 튜닝을 많이하면 교차검증보다 성능이 조금낮은게 보통임
		그래도 테스트세트 점수올릴라고 하이퍼파라미터 튜닝하면 안됨 그러면 새로운데이터 일반화가 안됨
7.런칭 유지보수
	전체 전처리 파이프라인과 예측파이프라인이 포함된 사이킷런 모델을 joblib를 사용해서 저장하고,
	이거를 상용환경에서 로드하고 predict를 사용해서 예측을 만들면 됨
	웹을쓰든 뭘쓰든 저거만 연결시켜주고 rest api같은거든 뭐든 연결만 시켜주고 인풋아웃풋만 있으면 됨
	
	배포를 했으면 시스템의 성능을 체크하고 성능이 떨어졋을때 알림을 줄수있는 모니터링 코드를 만들어야함
	
	



3.분류
1.mnist
	mnist는 분류에서 제일 테스트하기 좋은 데이터셋
	받으려면
		from sklearn.datasets import fetch_openml
		mnist=fetch_openml('mnist_784',version=1)
	하면 받아짐
	보통 데이터셋의 구조는
	
		데이터셋을 설명하는 DESCR
		샘플이 있는 data
		레이블이 있는 target
	
	그리고 데이터갯수와 특성수를 확인하고싶으면
	mnist.shape
	하면 (데이터갯수,특성수)가 나옴
	
	이게 픽셀식이면 그려볼수도 있는데 그릴려면
	
		import matplotlib as mpl
		import matplotlib as plt
		
		x=mnist['data']
		y=mnist['target']
		
		샘플=x[0]
		샘플이미지=샘플.reshape(가로픽셀수,세로픽셀수)
		
		plt.imshow(샘플이미지,cmap='binary')
		plt.axos("off")
		plt.show()
	이러면 픽셀이 그려짐
	
	만약 레이블이 문자로 들어가 있으면,알고리즘은 숫자를 기대하기떄문에 정수로 바꿔줘야함
		y=y.astype(np.uint8)
	
	그리고 이제 테스트세트랑 트레인세트를 나눠야하는데,mnist는 이미 나눠뒀으니까 
	앞에 6만개 트레인으로 쓰고 뒤에 1만개 테스트세트로쓰면됨
		xtrain,xtest,ytrain,ytest=x[:60000],x[60000:],y[:60000],y[60000:]
	훈련세트는 이미 섞여있으니까 모든 교차검증폴드가 비슷하게 나옴
	그리고 어떤알고리즘들은 샘플의 순서에 민감해서 비슷한애들이 연이어 나오면 성능이 나빠짐,
	단 날씨나 주식같은 연속성있는애들은 당연히 섞으면안됨
	
2.간단한분류테스트(이진분류)	
	스팸처럼 true false 분류하는걸 이진분류기라고함
	
	확률적 경사 하강법(SGD)는 매우 큰 데이터셋을 효율적으로 처리하는 장점을 가지고있음(그래서 온라인학습에 잘맞음)
	사용법은
		from sklearn.linear_model import SGDClassifer
		
		sgd모델=SGDClassifer(random_state=시드값)
		sgd모델.fit(트레인세트,트레인레이블)
		
	그리고 사용할땐 predict쓰면됨
		sgd모델.predict([이미지])
	
3.성능측정
	1.교차검증
		분류를 cross_val_score로 교차검증하면 정확도가 막 95%이렇게 나오는데
		분류에서는 막 전부 트루주고 그래도 90퍼센트이상 나오는경우가 있기때문에,분류에서는 정확도를 성능측정지표로 잘 사용하지 않음
		특히 불균형한 데이터셋을 쓸때 특히 더 그럼
	2.오차 행렬
		그래서 교차검증말고 사용하는 방법이 오차행렬인데,오차행렬은 클래스a의 샘플이(5가)클래스b로(3으로) 분류된 횟수를 센 행렬임
		만약 5가 3으로 잘못분류된 횟수를 알고싶으면 5행3열을 보면 됨
		
		오차행렬 사용법은 
		
			from sklearn.model_selection import cross_val_predict
			
			예측값=cross_val_predict(모델,트레인세트,트레인레이블,cv=반복횟수) 
			
			from sklearn.metrics import confusion_matrix
			
			confusion_matrix(레이블,예측값)
		cross_val_predict는 cross_val_score처럼 k겹 교차 검증을 하지만 
		평가점수를 반환하지않고 각 테스트폴드에서 얻은 예측을 반환함,즉 3이 5로 예측된 수,5가 5로 예측된 수등을 묶어서 리턴함
		만약 tf만 있는걸 오차행렬로 만들면
			[진짜음성,가짜양성]
			[가짜음성,진짜양성]
		이렇게됨
		만 숫자분류기처럼 10개씩 있는건 왼쪽위부터 대각선이 제대로 들어간값이고 나머지가 오류값
		완벽한 분류기는 대각선을 제외한 모든게 0임
	3.정밀도와	재현율
		좀 더 간략한 지표는 
			양성예측의 정확도,즉 정밀도 ->정밀도= 진짜양성/(진짜양성+가짜양성)
			정확하게 감지한 양성샘플의 비율,즉 재현율->재현율=진짜양성/(진짜양성+가짜음성)
		을 사용함
		
		사이킷런에서 사용하려면
			from sklearn.metrics import precision_score,recall_score
			
			precision_score(레이블,예측값)  #정밀도
			recall_score(레이블,예측값)#재현율
			
		이거 두개를 묶은걸 f1점수라고 하는데 이건 정밀도와 재현율의 조화평균임
		
		사용법은
		
			from sklearn.metrics import f1_score
			f1_score(레이블,예측값)
		
		평균적으로 높은값을 뽑아내야하면 f1점수를 사용하면 되는데,상황에따라 정밀도가 더 중요할수도 있고 재현율이 더 중요할수도있음
		만약 애들유튜브 분류기라고하면 다른 이상한거 막 다 잡는다고 쳐도 최대한 정밀도를 높게 가져가야하고,
		도둑이미지 분석같은걸 하면 아무때나 막 알람 울려도 최대한 재현율 높게 가져가야하는거처럼 상황마다 다름
		
		둘다 높으면 좋겠지만,정밀도를 올리면 재현율이 줄고 재현율을 올리면 정밀도가 줄어듬
	
	4.정밀도재현율 트레이드오프
		
		보통 머신러닝에서 임계값을 잡아두고(이거 이상이면 양성)그걸 움직이면서 정밀도 재현율을 조절함
		사이킷런에선 임계값을 직접 정할수는 없지만 예측점수를 받아볼수는 있음
			임계값점수=sgd모델.decision_function([샘플])
			임계값점수
			
			결과::: array([2412.2434])
			
		저렇게 받아서 후처리하는식으로 조절할수는있음
			임계값세팅=8000
			임계값후처리=(임계값점수>임계값세팅)
			
			결과:::array([False])
		이런식으로 받아볼수있음
		
		우리가 적절한 임계값을 정하려면,cross_val_predict로 모든 샘플의 점수를 구해야하는데,
		이번엔 예측결과가 아니라 결정점수를 받아야함
		
			결정점수=cross_val_predict(sgd모델,트레인세트,트레인레이블,cv=3,method='decision_function')
			#method='decision_function'하면 결정점수 리턴함
		
		이 점수로 precision_recall_curve()함수를 사용하면 모든 임계값에 대한 정밀도 재현율이 나옴
			from sklearn.metrics import precision_recall_curve
			
			정밀도,재현율,임계값=precision_recall_curve(레이블,결정점수)
			
		이제 임계값으로 정밀도 재현율 그래프를 그릴수있음
			def 정밀도재현율그래프(정밀도,재현율,임계값):
				plt.plot(임계값,정밀도[:-1],'b--')
				plt.plot(임계값,재현율[:-1],'g-')
				
			정밀도재현율그래프(정밀도,재현율,임계값)
			plt.show()
			
			
		tmi)(또 다른 방법은 재현율에 대한 정밀도 곡선을 그리는거임,
			그러면 average_precision_score()를 써서 곡선의 아래면적을 계산할수있어서 
			다른 두 모델을 비교하는데 도움이됨)
			
		보면 그래프가 급격하게 꺾이는점이 있는데 그거 직전을 트레이드오프로 선택하는게 좋음
		
		물론 프로젝트성격따라 달라지는데 만약 정밀도 90% 달성하는게 목표면 
			임계값90퍼=임계값[np.argmax(정밀도>=0.90)] 
		하면 최대값의 첫번째 인덱스를(가장 재현율이 높은)반환함
		
		트레인세트에 대한 예측을 만들려면 분류기의 predict를 쓰는게 아니라 
			y트레인90퍼=(예측점수>=임계값90퍼)
		하면됨
		
		근데 막 정밀도 높이면 재현율이 엄청낮아지니까 잘생각해야함
		
	5.ROC곡선
		ROC곡선은 가짜양성에 대한 진짜양성의 그래프임
		이건 1에서 진짜음성을 뺀값과 같음(1-진짜음성을 특이도라고 부름)
		
		즉,roc곡선은 민감도에 대한 1-특이도 그래프임
		
		
		roc곡선을 그리려면 먼저 roc_curve()를 사용해서 임계값에서 재현율(tpr),특이도(tnr)을 계산해야함
		
			from sklearn.metrics import roc_curve
			
			재현율,특이도,임계값=roc_curve(레이블,예측점수)
		
		그리고나서 맷플롯립으로 tpr에대한 fpr곡선을 그리면됨
		
			def roc커브그래프(재현율,특이도,label=none):
				plt.plot(재현율,특이도,linewidth=2,label=label)
				plt.plot([0,1],[0,1],'k--')#대각선,완전랜덤분류기임
		
			roc커브그래프(fpr,tpr)
			plt.show()
		
		여기서도 트레이드오프가 있는데,재현율이 높을수록 분류기가 만드는 거짓양성(fpr)이 늘어남
		좋은 분류기는 랜덤분류기(대각선)에서 최대한 멀리 떨어져있어야함
		
		곡선 아래 면적(AUC)을 측정하면 분류기를 비교할수있음,완벽한 분류기는 1이고 완전한 랜덤분류기는 0.5임
		사이킷런에서 계산하려면
			from sklearn.metrics import roc_auc_score
			
			roc_auc_score(레이블,예측점수)
		하면됨
		
		정밀도재현율과 roc곡선중 어떤때 뭘 선택하냐면
		양성클래스가 드물거나,거짓음성보다 거짓양성이 중요하면 정밀도재현율을 쓰고,그렇지않으면 ROC를 사용함
		
		RandomForestClassifer에서 predict_proba()메서드는 샘플이 행 클래스가 열이고,
		샘플이 주어진 클래스에 속할 확률을 담은 배열을 리턴함(어떤 이미지가 5일 확률 70% 이런식)

4.다중분류		
	다중분류는 이중분류랑 다르게 둘 이상의 클래스를(숫자를 구별한다던지)구별할수있음
	알고리즘마다 여러개를 처리할수 있는 알고리즘(sgd,랜덤포레스트,나이브베이즈)들이 있고,
	이진분류만 가능한 알고리즘(로지스틱회귀,서포트벡터머신)이 있음
	하지만 이진분류기 여러개를 써서 다중클래스를 분류하는 기법도 많음
	
	만약 특성숫자 하나만 구분하는 숫자 이진분류기가 10개면 0부터 10까지 이미지분류를 할수있음,
	이미지 분류할때 가장 점수높은걸 선택하면됨,이런방식을 OvR(혹은 OvA)라고 함
	
	또 다른 방법은 0과 1구별,0과 2구별,1과 2구별 이런식으로 조합마다 이진분류기를 훈련시키는것,이걸 OvO라고 함
	클래스가 N개면 Nx(N-1)/2개가 필요함
	mnist에서 이미지 하나를 분류하려면 45개((10x9)/2)를 통과시켜서 가장 많이 양성으로 분류된 클래스를 선택함
	OvO의 가장 큰 장점은 각 분류기의 훈련에 전체 훈련세트중에서 구별할 두 클래스에 해당하는 샘플만 필요하다는것
	
	일부 알고리즘은 훈련세트크기에 민감해서 큰 훈련세트에서 몇개의 분류기를 훈련시키는거보다,작은훈련세트에서 많은 분류기를 훈련시키는게
	더 빨라서  OvO를 선호함,하지만 대부분은 OvR을 선호함
	
	분류기에서 점수=svm모델.decision_function([샘플])
	하면 분류기 클래스마다 점수를 보여줌,이 점수중 가장 높은점수를 리턴

	사이킷런에서 OvO나 OvR을 사용하도록 강제하려면 OneVsOneClassifier이나 OneVsRestClassifier을 사용하면됨
	sveone=OneVsOneClassifier(SVC())이런식으로 안에다가 알고리즘넣는식

	그리고 2장에서도 그랬지만,입력의 스케일을 조정하면(StandardScaler를써서) 정확도가 올라감
	
5.에러분석
	모델의 성능을 향상시키는 한가지 방법은 에러의 종류를 분석하는거임
	첫번째로,오차행렬을 살펴볼수있음
	아까처럼 cross_val_predict로 예측을 만들고,confusion_matrix로 오차행렬을 만들수있음
	
	보기편하게하려면
		plt.matshow(오차행렬,cmap=plt.cm.gray)
		plt.show()
	하면 오차행렬이 색으로 나옴 밝으면 많이들어갔다는거

	에러부분에 초점을 맞추려면 오차행렬의 각 값을 대응되는 클래스의 이미지갯수로(에러갯수가아님)나눠서 에러비율을 비교(갯수로하면 이미지많은
	애들이 나쁘게보임)
	
		행의합=오차행렬.max(axis=1,keepdims=True)
		나눈값=오차행렬/행의합

	여기서 주대각선만 0으로 채워서 그래프그리면됨
		np.fill_diagonal(나눈값,0)
		plt.matshow(나눈값,cmap=plt.cm.gray)
		plt.show()

	열이 밝으면 그거로 잘못 분류된 이미지가 많다는소리,오차행렬은 반드시 대칭인것은 아님
	
	오차행렬을 분석하면 성능향상방안에 대한 통찰을 얻을수있음,만약 8로 잘못분류되는게 많다면 8처럼보이지만 8이 아닌 훈련데이터를 구해서
	분류기를 학습시킬수도 있고,8분류에 도움이 될만한 특성을 추가할수도있음

6.다중레이블분류
	분류기가 샘플마다 여러개의 클래스를 출력해야할수도있음(얼굴인식 분류기면 사진마다 사람수만큼 [0,1,1,0...]이런식으로)
	이렇게 여러개의 이진태그를 분류하는 시스템을 다중레이블분류라고 함
	
	다중레이블분류를 평가하는 방법은 많지만 가장 편한건 f1점수를 모든 레이블에 대해 평균낸값임
	
	근데 다중레이블 분류에서 단순히 그냥평균내면 더 많이나온애에 대한 점수에 높은 가중치가 붙으니까,
	타깃레이블에 속한 샘플수(지지도)를 가중치로 주면(average='weighted'넣으면됨)됨
	
7.다중출력분류
	
	다중출력분류는 다중출력 다중클래스분류임,즉 다중레이블분류에서 한 레이블이 값을 두개이상 가질수 있도록 일반화한것
	예를들어 이미지에서 잡음을 제거하는 시스템이 있음,잡음이 많은 이미지를 입력으로 받고,깨끗한이미지를 픽셀의 강도를 담은 배열로 출력할때
	분류기의 출력이 다중레이블(픽셀당 한 레이블이 모인 배열)이므로 다중출력분류임




4.모델훈련

1.선형회귀
	보통 선형회귀모델은 (x_0은 1)y=a_0x_0 + a_1x_1+...+a_nx_n  이런식으로 y=n+mx식을 띄게됨
	즉,입력특성의 가중치(특성과 가중치의 곱)의 합과 편향(절편)상수를 더해서 만들어짐
	
	모델을 훈련시킨다는건 훈련세트에 가장 잘 맞도록 모델 파라미터(가중치와 편향)를 설정하는것
	선형회귀에서 가장 널리사용되는 성능측정지표는 RMSE,그렇지만 MSE가 같은결과를 내면서 간단해서 MSE씀
		MSE=모든샘플x( (가중치x특성)-레이블 )/전체샘플갯수
		
	즉 모든샘플의 각각모든 특성에 특성에 따른 가중치를 씌워서 레이블이랑 비교한후,총합해서 가장 값이 작아지는 가중치를 찾으면됨
	
	1.정규방정식
		저 비용함수를 최소화하는 값을 찾는 수학공식이 있긴한데,그걸 정규방정식이라고 함
		근데 잘안씀 쓰기도힘들고 코스트도 비싸서
		
		대충 O(n^2.8)쯤 되는거같음 역행렬쳐서 뭐시기뭐시기하는데 넘어갈래  쓰지도않는거같고
	
2.경사하강법(GD)
	경사하강법은 여러문제에서 최적의해법을 찾을수있는 일반적인 최적화 알고리즘임
	경사하강법은 비용함수를 최소화 하기위해 반복해서 파라미터를 조정해가는것
		
	즉,현재 파라미터에 대해 비용함수의 현재 기울기를 계산하고,
	기울기가 감소하는 방향으로 진행하다 기울기가 0이되면 최소값에 도달한것
	
	구체적으로는 임의의 값으로 시작해서(무작위 초기화),
	한번에 조금씩(학습률에따라)비용함수가 감소하는 방향으로 진행해서,
	알고리즘이 최소값에 수렴할때까지 점진적으로 향상시킴
	
	경사하강법에서 중요한 파라미터는 스텝(한번에 얼마나 파라미터를 바꿀지)의 크기로,학습률 하이퍼파라미터로 결정됨
	만약 학습률이 너무 작으면 알고리즘이 수렴하기위해 반복을 많이해야해서 시간이 오래걸림
	하지만 학습률이 너무 크면 그래프의 반대편으로 건너뛰어서 이전보다 더 높은곳으로 올라갈수도 있음,이러면 발산돼서 답이 안나오게됨 
	
	모든 비용함수가 2차함수그래프는 아니라서 막 기울기가 0이되는게 2군데이상 있거나,중간에 평평한곳이 있거나 그럴수 있는데,
	그러면 최소값으로 수렴하기 매우 어려워짐
	만약 지역최소값이 있고 그쪽으로 접근하면 지역최소값에 걸려서 전역최소값을 찾을수없어지고,
	평지에 걸리면 평지를 지나는데 시간이 오래걸리고 일찍 멈춰서 전역최소값에 도달할수없어짐
	
	그래도 선형회귀에서의 mse함수는 볼록함수(2차함수그래프)라서 지역최소값이 없고,하나의 전역최소값만 있고,연속된함수에,기울기가 변하지않음
	즉,학습률이 너무 높지않고 충분한 시간이 주어졌을때 경사하강법이 전역최소값에 가깝게 접근할수 있다는걸 보장함
	
	그리고 경사하강법은 특성들의 스케일을 맞춰줘야함
	특성들의 스케일이 다르면 시간이 많이 오래걸림(StandardScaler사용)

	1.배치경사하강법
		배치경사하강법은 매 스탭마다 훈련 데이터 전체를 사용해서 계산
		즉, 큰 훈련세트에서는 엄청 느려짐,하지만 특성수에는 그렇게 민감하지 않아서 특성이 수십만개면 경사하강법쓰는게좋음
		
		경사하강법을 구현하려면 각 모델파라미터에 대해 비용함수의 기울기를 계산해야함
		즉, 모델파라미터가 조금 바뀔때 비용함수가 얼마나 바뀌는지를 계산해야함 이걸 편도함수 라고 함
		이걸 모든 파라미터에 대해서 계산함
		
		경사하강법은 기울기가 0이되는 지점을 찾는거기때문에, 현재기울기가 양수면 왼쪽으로 가야하고,음수면 오른쪽으로 가야하니
				다음스탭=모델파라미터-(학습률x기울기벡터xMSE )
				그냥 일반적으로 가볍게보면
				x다음위치=x현재위치-(학습률x기울기값)
				기울기값은 최소값에 가까워질수록 줄어드니 처음엔 크게움직이고,나중엔 조금움직이는거에도 적합함
		해야함
		기울기의 반대로 가야하니 현재모델파라미터에서 빼준것
		
		경사하강법에서 반복횟수는 그리드식으로 찾을수 있지만 너무오래걸리면,
		반복횟수를 엄청크게 잡고 벡터값이 허용수치보다 작아지면 최소값에 도달했다치고 알고리즘을 종료시키면됨
	
	2.확률경사하강법
		배치경사하강법의 가장 큰 문제는 매스탭마다 전체훈련데이터를 사용해야해서 엄청느리다는것임
		그래서 확률경사하강법이라는것도 있는데,이건 매 스탭마다 하나의샘플을 무작위로 선택하고,
		그 하나의 샘플에 대한 기울기를 계산함
		이건 매 반복마다 다뤄야할 데이터가 매우 적기때문에 훨씬빠르고,
		매반복에서 하나의샘플만 있으면되니까 훈련세트가 아무리커도 상관없음
		
		하지만 확률적이기떄문에 배치보다 훨씬 불안정하고,비용함수가 최소값에 다다를때까지 위아래로 요동치면서 평균적으로 감소함
		시간이 지나면 최소값에 매우 근접하겠지만,최소값에 안착하지는 못함
		
		그리고 지역최소값이랑 전역최소값이 있으면 랜덤으로 배치하기때문에 
		확률적 경사하강법이 배치경사하강법보다 전역최소값찾을 가능성이 높음

		즉,전역최소값에 정확히 일치시킬수는 없지만 지역최소값을 피할확률은 높음
		
		이걸 해결하려면 학습률을 점진적으로 감소시키는방법을 생각해볼수있음
		
		시작할떄는 학습률을 크게하고(수렴을 빠르게하고 지역최소값에 빠지지않게함),점차 줄여서 전역최소값에 도달하게 함
		
		이렇게 매 반복에서  학습률을 결정하는 함수를 학습 스케줄 이라고 함
		학습률이 너무빨리 줄어들면 지역최소값에 갇히거나 최소값가기전에 멈출수있고,
		학습률이 너무 천천히 줄어들면 최소값 주변을 맴돌거나 훈련이 너무빨리끝나서 지역최소값에 머무를수있음
		
		확률경사하강법은 일반적으로 한반복에서 훈련세트샘플수만큼 되풀이되고,각 반복을 에포크 라고 함
		
		샘플을 무작위로 선택하기때문에 어떤샘플은 한 에포크에서 여러번 선택될수있고,어떤샘플은 전혀 선택되지못할수도있음
		
		사이킷런에서 확률경사하강법을 쓰려면 SGDRegressor을 쓰면됨
		
	3.미니배치경사하강법
		미니배치경사하강법은 전체훈련세트나 하나의 샘플이 아닌,미니배치라고 부르는 임의의 작은 샘플 세트에 대해 기울기를 계산함
		미니배치의 장점은 gpu를 사용해서 얻는 성능 향상임
		
		미니배치를 어느정도 크게하면,sgd(확률경사하강법)보다 덜 불규칙적으로 움직이고 sgd보다 최소값에 더 가까이 도달함
		하지만 지역최소값에서 빠져나오기는 더 힘들수도있음
	4.알고리즘비교
		선형회귀에서 알고리즘비교
		
		알고리즘명   샘플이클때   외부메모리학습   특성수가많을때   하이퍼파라미터수   스케일조정필요   사이킷런
		정규방정식    빠름         no        느림           0           no         없음 
		SVD       빠름         no        느림           0           no       LinearRegression
		배치경사     느림         no        빠름           2           yes      SGDRegressor
		확률적경사    빠름        yes        빠름           >=2         yes      SGDRegressor
		미니배치경사   빠름        yes        빠름           >=2         yes      SGDRegressor
		
		이알고리즘들은 훈련결과에 차이가 없고,매우 비슷한모델을 만들고 같은방식으로 예측을 함
		
3.다항회귀
	가진 데이터가 직선보다 복잡한 형태라도,즉 비선형데이터라도 학습하는데 선형모델을 사용할수 있음
	제일 간단한 방법은 각 특성의 거듭제곱을 새 특성으로 추가하고,이 특성을 포함한 데이터셋에 선형모델을 훈련시키는것
	이런걸 다항회귀라고 함(즉 제곱을 넣어서 제곱으로 선을 그리니 직선이 아닌 n차함수그래프를 만들수있음)
	
	거듭제곱을 추가하는법은
		from sklearn.preprocessing import PolynomialFeatures
		
		poly=PolynomialFeatures(degree=2(차수),include_bias=False(편향을 위한 변수 제거(상수제거)))
		xpoly=poly.fit_transform(x)
	이러면 원래특성과,특성의 제곱이 포함됨
	
	이걸 그냥 LinearRegression으로 돌리면
		linreg=LinearRegression()
		linreg.fit(xpoly,y)
	이러면 곡선형태의 그래프가 나오는데(직선이 아닌)
	이렇게 할수있는 이유는,
	PolynomialFeatures가 특성 a,b가 있고 차수가 3일때 
	a^2,b^2,a^3,b^3만이 아닌 ab,a^2b,ab^2등 교차항을 추가하기떄문
	그래서 서로간의 관계에 따른 추세도 그래프에 반영시킬수 있음
	
	보통 그래프가 2차함수그래프면 차수도 2에맞춰주는게좋고,3차함수면 3에맞춰주는게 좋음
	
	그리고 PolynomialFeatures를 쓸때 특성이 많으면 엄청나게 늘어나니까 조심해야함
4.학습곡선
	물론 몇차함수로 생성된 그래프인지 알긴 어렵고,고차 다항모델을 쓰면 훈련데이터엔 잘 맞지만,과대적합돼버림
	그래서 좀 높은 차수를 쓴다음에 규제하거나 이런식으로 하는듯
	
	모델의 성능을 추정할때 교차검증을 쓸수도있지만(훈련엔 좋지만 교차검증점수가 나쁘면 과대적합,둘다나쁘면 과소적합)
	학습곡선을 쓸수도 있음
	학습곡선은 훈련세트와 검증세트의 모델 성능을 훈련세트크기나 훈련반복의 함수로 나타냄,
	즉 rmse같은 평가함수를 y 세트반복수를 x로 잡고 그래프를 그림
	
	학습곡선에서 과소적합된 그래프는 훈련데이터의 rmse가 금방금방 상승해서 
	평평해질떄까지 시간이 얼마 안걸리고,평균오차가 크게 나아지지않음
	검증세트에서도  초창기에 엄청 크다가 금방 내려와서 훈련데이터와 비슷하게 평행해서 나감
	
	역시 과소적합되면 금방금방 붙긴하는데 그냥 에러치 자체가 높은게 문제임
	
	학습곡선에서 과대적합된 그래프는 훈련데이터의 rmse가 처음엔 아예 0이다가(차수까지는) 서서히 올라가고,
	검증세트에서는 초창기는 엄청나게 큰데,훈련데이터가 쌓일수록 내려와서, 훈련데이터가 많이 쌓였을땐 과소적합보다 오차가 훨씬 낮고
	데이터가 많아질수록 훈련세트와 검증세트의 선이 붙음
	
	
	tmi)오차는 총 3가지가 있는데
		편향(절편과다름):잘못된 가정으로 인한 오차
			데이터가 2차인데 선형으로 가정한다던지 할경우
			편향이 큰 모델은 과소적합되기 쉬움
		분산:훈련데이터의 작은 변동에 모델이 과도하게 민감해서 나타남
			자유도가 높은(차수가높은)모델이 분산이 높기쉬워서 데이터에 과대적합되는 경향이 있음
		줄일수없는오차:데이터 자체에 있는 잡음,
				  고치는법은 데이터에서 잡음을 제거하는거밖에없음


5.규제가 있는 선형모델
	과대적합을 줄이는 좋은방법은 모델을 규제하는것,
	다항회귀모델을 규제하는 간단한방법은 다항식의 차수를 감소시키는것
	대부분 규제가 있는 모델은 스케일에 민감하기때문에 스케일을 맞춰줘야함
	
	선형회귀모델에서는 보통 모델의 가중치를 제한해서 규제를 가함
	
	1.릿지 회귀
		릿지회귀는 규제가 추가된 선형회귀의 한 버전임
		규제항 a가 비용함수에 추가되는데(a*모델파라미터),
		이는 학습 알고리즘을 데이터에 맞추는것뿐만 아니라 모델의 가중치가 가능한 작게 유지되도록 노력함
		
		식은 가중치벡터의L2노름(벡터 두개의 직선거리)의 제곱을 2로 나누면 되는데 뭐 신경쓰지말고 대충이해하자
		
		규제항은 훈련하는 동안에만 비용함수에 추가되고,훈련이 끝나면 모델의 성능을 규제가 없는 지표로 평가함(보통 훈련할때쓰는 
		비용함수와 테스트에서 사용되는 성능지표는 다름,훈련에 사용되는 비용함수는 최적화를위해 미분가능해야하고,
		테스트에 사용되는 성능지표는 최종목표에 최대한 가까워야 하기 때문)
		
		a는 모델을 얼마나 많이 규제할지 조절함,a가 0이면 선형회귀랑 같고,
		a가 엄청크면 모든 가중치가 0이되고 데이터의 평균을 지나는 수평선이 됨
		즉,a가 커지면 오차에서 분산이 줄고 편향이 커짐(과소적합이늘고 과대적합이 줄어듬)
		
		그리고 편향(절편)은 규제되지않음 그러니까 y=절편+a모델파라미터 꼴이 됨
		
		확률적경사하강법에서 릿지회귀를 쓰는법은
			sgdreg=SGDRegressor(penalty='l2'(엘투))
			하고 그대로 fit해서 진행하면됨
	
	2.라쏘회귀
		선형회귀의 다른 규제버전임
		얘도 릿지회귀처럼 비용함수에 규제항을 더하지만 L1노름(각 파라미터의 차이의 절대값의 합)을 사용함 
		ex(p=(3,1,-3) q=(5,0,7)이면
		3-5 1-0 -3-7 = 2+1+10=13)
		
		라쏘회귀의 중요한 특징은 덜 중요한 특징의 가중치를 제거하려고 한다는점(가중치가 0이됨)
		즉,라쏘회귀는 자동으로 특성선택을 하고 희소모델을 만듬(즉 0이아닌 특성의 가중치가 작음)
		
		라쏘회귀(L1)는 파라미터1이 2 파라미터 2가 0.5일때,
		[1.9 0.4],[1.8 0.3] 이렇게 선형적으로 줄다가 
		[1.5 0.0]이되면 파라미터1을 0으로 보내는식으로 작동한다면(같은수치만큼 줄어듬)
		
		릿지회귀(L2)는 파라미터1이 2 파라미터 2가 0.5일때, 30%라고하면
		[1.643 0.35123], [1.2231 0.221]... [0.002 0.001] 
		이렇게 두 벡터의 직선으로 가지만 0에는 도달할수없고,
		처음엔 줄어드는값이 크지만 점점 줄어드는값이 작아짐,즉 경사하강법이 자동으로 느려지고 수렴에 도움이 됨
		그리고 a가 커질수록 원점에 가까워짐
		
		(라쏘를 쓸때 최적점 근처에서 진동하는걸 막으려면 훈련하는동안 점진적으로 학습률을 감소시키면 스텝이 작아지니까 결국 수렴하게됨)
		
		라쏘의 비용함수는 파라미터가 0일때 미분가능하지 않지만,
		서브그레이언트 벡터(대충 근처값 긁어서 중간값)를 사용하면 경사하강법을 적용할수있음
		
	3.엘라스틱넷
		엘라스틱넷은 릿지와 라쏘를 절충한 모델임
		단순히 릿지와 라쏘의 규제항을 더해서 사용하고,혼합비율 r을 조절해서 사용함
		r=0이면 릿지회귀고 r=1이면 라쏘회귀와 같음
		
		보통 규제중에서 뭘고르냐면 
		보통 규제가 약간이라도 있는게 좋아서 평범한 선형회귀는 쓰지않고,릿지가 기본이지만 사용할 특성이 몇개밖에 없다고 생각되면,
		랏쏘나 엘라스틱넷이 나음(불필요한특성의 가중치를 0으로 만들어주기때문)
		특성수가 훈련샘플수보다 많거나 특성 몇개가 강하게 연관되어있으면 라쏘가 문제를 일으키니까 엘라스틱넷을 선호함
		
		엘라스틱넷을 사용하는법은
			from sklearn.linear_model import ElasticNet
			
			eleast=ElasticNet(alpha=0.1(알파값),L1_ratio=0.5(혼합비율))
			eleast.fit(데이터,레이블)
			
	4.조기종료
		경사하강법같은 반복적인 학습 알고리즘을 규제하는 다른방식은 검증에러가 최솟값에 도달하면 바로 훈련을 중지시키는것
		이걸 조기종료라고 부름
		
		간단히 매번 RMSE 최저값이랑 세팅 저장한다음에 현재값이 그거보다 커지면 되돌리기한다음에 그거리턴하고 종료하면됨
		
6.로지스틱 회귀
	로지스틱 회귀는 분류에서 사용할수 있는 회귀임
	작동방식은 샘플이 특정 클래스에 속할 확률을 추정함(이메일이 스팸일 확률)
	보통 스팸처럼 0,1만 있는 분류기를 이진분류기라고함
	
	1.확률추정
		로지스틱회귀는 선형회귀와 같이 입력특성의 가중치합을 계산하고 편향을 더함,대신 바로 결과를 출력하지않고
		결과값의 로지스틱(0과 1사이값을 출력하는 시그모이드함수)을 출력함(1/1+exp(-t)) 
		exp는 대충 0~1만들기위한 자연로그가지고한거 t는 logodd(양성클래스확률과 음성클래스확률사이의 로그비율) 몰라도됨 
		
		그러면 y가 1부터0까지고 파라미터에 따라 1에서 0사이 어딘가에 있는 함수가 나오는데,
		임계값을 넘기면(기본임계값은 0.5) 양성,넘기지못하면 음성 이런식으로 예측할수있음
	2.훈련과 비용함수		
		즉 우리는 로지스틱회귀에서 양성샘플은 높은확률을 주고 음성샘플은 낮은확률을 주는 파라미터벡터를 찾는것
	
		로지스틱회귀의 비용함수는 로그손실이라고 부르는데,이건 따로 최소값 계산하는 해가 없는데,볼록함수니까 경사하강법을 쓰면
		전역최소값을 찾는걸 보장함
		
	3.결정 경계
		로지스틱회귀는 특성을 기반으로 선을 긋고(절편+파라미터1*특성1+파라미터2*특성2...)=0을 만족하는 x의 집합) 
		그선을 넘으면 15퍼센트로 스팸이다 50퍼센트로 스팸이다 이런식으로 함
		
		다른 선형모델처럼 로지스틱회귀도 L1 L2패널티를 써서 규제할수있음,사이킷런은 L2가 기본값임
		(사이킷런의 LogisticRegression의 규제강도는 alpha가 아니아 그 역수인 C임 C가 높으면 모델의 규제가 줄어듬)
	4.소프트맥스 회귀
		로지스틱회귀는 여러개의 이진분류기를 연결하지않고 직접 다중클래스를 지원하도록 일반화할수있음, 이를 소프트맥스회귀라고함
		
		개념은 샘플이 주어지면 소프트맥스모델이 각 클래스에 대한 점수를 계산하고,그 점수를 소프트맥스함수에 넣어서 각 클래스의 확률을
		추정함
		
		(각 클래스는 자신만의 파라미터벡터가 있는데 이벡터들은 파라미터행렬에 행으로 저장됨)
		
		소프트맥스 함수는 각 점수에 지수함수를 적용한후 정규화(모든지수함수 결과의 합으로 나눔)함
		보통 저 점수를 로그오즈(로짓)라고  부름
		
		즉 점수뽑아서 함수에넣어서 모든클래스에 대한 확률로 변형한뒤에 가장 확률이 높은 클래스를 선택함(다중클래스지 다중출력이 아님)
		
		그러니까 타깃클래스에 대해서는 높은확률,다른클래스에 대해서 낮은확률을 추정하도록 만드는게 훈련의 목적임
		여기엔 크로스엔트로피 비용함수를 잘 쓰는데,
		크로스엔트로피는 추정된 클래스의 확률이 타깃클래스에 얼마나 잘 맞는지 측정하는 용도로 잘 사용됨
		
		사이킷런의 LogisticRegression은 클래스가 둘 이상일떄 기본적으로 OvA를 쓰는데 
		multi_class='multinomial'을 넣어주면 소프트맥스 회귀를 쓸수있음
		
		소프트맥스 회귀를 쓰려면 solver매개변수에 'lbfgs'같은 소프트맥스회귀를 지원하는 알고리즘을 지정해야함
		그리고 기본적으로 C를 사용해서 조절할수있는 L2규제가 적용됨
		
		주의점은 추정경계에 가까울수록 확률이 별차이없는걸 예측할수도 있음(3개일떄 34퍼센트인걸 리턴할수있음)
	

	
5.서포트벡터머신	
1.선형svm분류
	
	서포트벡터머신(svm)은 클래스들 사이에 가장 폭이 넓은 도로를 찾는 일
	각각 클래스중 가장 가까운걸 택해서 그 두개를 기준으로 선을 긋고 그거로 나눔(라지마진분류)
	그래서 가장 가까운애한테만 영향을 받지,막 3만개가 더들어온다해도 경계선이 갱신되는게 아니면 전혀 영향을 주지않음
	즉,경계에 있는 애들한테만 영향을 받고 이런애들을 서포트 벡터라고 함
	
	svm은 특성의 스케일에 매우 민감해서,스케일조정을 해줘야함
	
	만약 모든 샘플이 바로 선으로 나눌수있게 분류 되어있으면 이걸 하드 마진 분류라고 함
	하드마진 분류의 문제는 데이터가 선형적으로 구분할수있어야하고,이상치에 매우민감함
	
	만약 상대클래스쪽에 이상치로 한개가 섞여있으면 하드마진분류가 불가능하고(선을 그을수가없음)
	그정도는 아니더라도 상대클래스에 엄청붙어있으면 도로폭이 좁아지고 그래서 일반화가 잘 되지않음
	
	그래서 도로의폭과 마진오류의 트레이드오프를 해서 한두개는 상대클래스로 넘어가도 무시하고 막 도로건너편에 있어도 무시하는걸
	소프트 마진 분류 라고 함
	
	사이킷런에서는 하이퍼파라미터 C를 조절하면 마진오류허용치를 조절할수있음
	
	사이킷런에서 쓰려면 
		LinerSVC(C=1(마진오류허용치),loss="hinge"(힌지손실))
	하면됨
	
	svm은 로지스틱회귀와 다르게 클래스에 대한 확률을 제공하지않음
	
2.비선형 svm 분류
	비선형이면 일반적으론 분류를 할수없지만,다항회귀처럼 다항특성을 추가하면(x^2라던지) 선형적으로 구분되는 데이터셋이 만들어질수있음
	
	사이킷런에선 PolynomialFeatures에 차수넣고 StandardScaler LinerSVC에 c,loss넣은거 
	파이프라인으로 연결해서 넣으면됨
	
	1.다항식 커널
		다항특성을 추가하는건 쉽고 모든알고리즘에서 다 잘 동작하지만,낮은차수의 다항식은 안나올확률이 높고,높은차수는 엄청느려짐
		
		근데 svm은 커널트릭이라는 사기스킬로(데이터를 고차원으로 매핑해서 차이를 찾음)
		특성을 추가하지않고 다항식을 많이 넣은것과 같은 결과를 얻을수있음
		
		사용법은
			SVC(kernel='poly'(무슨커널쓸건지),degree=3(차수),coef0=1(높은차수 낮은차수 얼마나 영향받을지),C=5)
		모델이 과소적합이면 차수를 늘려야하고,과대적합이면 차수를 줄여야함
	2.유사도 특성
		또 다른 방법은 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도함수로 계산한 특성을 추가하는것
		이 함수는 0(랜드마크와 멀리떨어진경우)부터 1(랜드마크와 같은위치)까지 변화하며 종모양으로 나타남
	
		계산법은 랜드마크와 얼마나 떨어져있는지를 변수로잡고 유사도함수에 집어넣으면 랜드마크와 샘플이 얼마나 떨어져있는지 값이 나오는데
		이걸 토대로 선그어서 구분하는방식
		
		랜드마크값은 제일쉬운방법은 데이터셋에 있는 모든샘플위치에 랜드마크 설정하는것
		이러면 차원이 매우커지고,그래서 변환된 훈련세트가 선형적으로 구분될 가능성이 높음
		단점은 n개의 특성을가진 m개의 샘플이 m개의 특성을 가진 m개의 샘플로 변환된다는것(원본샘플제외했을때)
		즉 훈련세트가 크면 동일한크기의 아주 많은 특성이 만들어짐
		
	3.가우시안 RBF커널
	유사도도 연산비용이 많이드는데 그냥 커널트릭으로 사기칠수있음
	SVC(kernel='rbf',gamma=5(증가하면 종이 좁아져서 샘플의 영향범위가 줄어듬,즉 규제 과소면 증가 과대면 감소),C=0.001) 
	
	커널 선택은 linear을 제일 먼저 해봐야하고,훈련세트가 그렇게 크지않으면 가우시안RBF하면됨
	
	4.계산복잡도
		커널트릭을 안쓸거면 LinerSVC를 씀(훈련샘플과 특성수에 수행시간이 선형적으로 증가 O(m*n)))
		정밀도를 높이면 알고리즘 수행시간이 길어짐(하이퍼파라미터 tol)보통은 기본값두면 잘 작동함
		
		SVC는 커널트릭을 쓸때 쓰는데,시간복잡도가 O(m^2*n)이라서 훈련샘플수가 커지면 엄청나게 느려짐,
		하지만 특성은, 특히 희소특성(0이아닌특성이 몇개없는경우)에는 잘 확장됨
	
3.svm회귀
	svm은 회귀로도 쓸수있는데 회귀는 목표를 반대로 하면 됨
	즉 도로 안에 가능한 많은 샘플이 들어가게 학습시키면됨
	이거도 마진안에서는 훈련샘플이 추가되어도 예측엔 영향을 주지않음
	
	사용법은 
		svmreg=LinearSVR(epsilon=1.5(도로의폭),tol=0.001(허용오차))
	SVR은 SVC의 회귀버전 LinearSVR은 LinearSVC의 회귀버전
	
4.SVM이론
	svm은 초평면(n+1차원의 평면)인 결정함수(초평면과 데이터셋평면이 교차하는부분(결정함수의 값이 0인지점이 결정경계))
		
	커널트릭은 벡터곱을 하면 차원이 하나 더생기는것((a,b)*(a,b)=a^2 ab b^2 )을 이용한것
		
		
		
		
		
		
		
		
		
		
		
		




