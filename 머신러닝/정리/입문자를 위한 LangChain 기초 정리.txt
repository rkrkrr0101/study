1.체인(chain)에 대한 이해 : 기본 LLM 체인 (Prompt + LLM) | 멀티 체인
  기본체인이란 프롬프트와 llm이 결합되어있는 구조를 말함
  즉 프롬프트로 사용자의 입력을 받고({name}은 몇살입니까? 이런식),
  입력된 지시문을 llm에게 전달해주고,
  그 지시문을 통해 llm이 질문에 대한 답변을 만드는 구조가 기본체인임
  
  체인으로 나온 응답은 어플리케이션으로 반환되고,이 응답을 직접 사용할수도,생성된 텍스트만 추출할수도,요약할수도 있음
  
  랭체인코어에서 ChatPromptTemplate를 사용해서 시스템메시지와 유저메시지를 분리해서 사용할수있음
  구현적으로는 시스템 유저 분리보단 해당 키값으로 파싱해서 넣는느낌에 가깝지만
  
  그리고 최종출력에서 텍스트만 추출하는 파서가 StrOutputParser()임
  그래서 체인 구성은
    chain=prompt | llm | output_parser
  이런식으로 구성하면됨
  그리고
    chain.invoke("input":"질문")//input는 질문의 키값을 넣으면됨
  이런식으로 사용하면,프롬프트를 키값으로 수정하고,llm에 던지고,output_parser로 출력을 가공해서 리턴함
  
  멀티체인은 기본체인을 하나가 아니라 두개이상 연결해서 사용하는걸 말함
  그래서 좀 더 복잡한 작업을 할수있어짐
  즉 체인1에서 출력한 값을,체인2에 입력값으로 사용해서 최종출력을 뽑아내는식임
  이건
    chain2= {"eng_word":chain1} | prompt2 | llm | output_parser
	chain2.invoke({"kor_word":"미래"})
  이런식으로 구성하면 chain2를 실행하기전에 먼저 chain1이 실행되고,그 결과물이 "eng_word"에 담겨져서 prompt2로 전달되는식으로 체인이 일어남
  
2.프롬프트(prompt) 만들기 : Prompt Template 이해 및 적용 
  프롬프트를 어떤 틀(템플릿)을 만들어두면 재사용이 편함 
  즉 변하는 부분만 값을 넣어가면서 쓰는것,{aaa}이렇게 쓰고 format메서드로 키값으로 변수를 넣어서 쓰면됨
  또한 기본적으로 스트링이라서 +로 더할수있고,그래서 각 블록별로 모듈화를 시켜서 조합해서 쓸수가있음
  
  이걸 그냥 체인에 넣어서 쓰면됨
  이렇게 기본적인건 PromptTemplate고,
  좀 복잡한건 ChatPromptTemplate도 있음
  이건 여러 메시지를 가진 리스트로 구성되고,각 메시지는 역할과 내용으로 구성됨
  이건
    시스템메시지(페르소나등)
	휴먼메시지(사용자 질문)
	AI메시지(모델의 응답)
	펑션메시지(함수호출결과)
	툴메시지(툴콜링 결과)
  로 나뉨
  각 메시지는 롤을 프롬프트에 표시하고있어서(시스템메시지인지 실제 질의,즉 휴먼메시지인지 등) llm이 쉽게 구분할수있음
  
  
3.LLM 모델 구조 : LLM 클래스와 ChatModel 모델 클래스 구분
  랭체인의 모델 클래스엔 LLM과 ChatModel이 있음
  LLM은 단일요청에 대한 복잡한 출력을 생성하는데 적합하고,ChatModel은 사용자와의 상호작용을 통한 연속적인 대화에 적합함
  
  LLM은 스트링을 입력으로 받아서 처리하고,스트링을 반환함,이건 광범위한 언어 이해 및 텍스트 생성작업에 사용됨(문서요약,컨텐츠생성등)
  ChatModel은 메시지리스트(ChatPromptTemplate)를 입력으로 받고,하나의 메시지를 반환함,
  이건 대화형상황에 최적화되어있고 연속적인 대화를 처리하는데 사용됨,즉 맥락을 유지하면서 응답을 생성하는데 중점을 둠
  
4.LLM 모델 튜닝: 모델 파라미터(model parameter) 설정  
  모델 파라미터에는
    Temperature:텍스트의 다양성 조정
	MaxTokens:생성할 최대 토큰 수 조정
	TopP:생성과정에서 텍스트의 다양성 조정을 임계값 조정 방식으로 하는것
	Frequency Penalty:이미 등장한 단어나 구절이 등장할 확률의 감소시킴,반복을 줄일수있음
	Presence Penalty:텍스트내에서 단어의 존재 유무에 따라 그 단어의 선택확률을 조정함
	Stop Sequences:특정 단어나 구절이 등장하면 생성을 멈춤
  등이 있음
  Temperature와 TopP는 거의 같은느낌이고,Frequency Penalty와 Presence Penalty도 거의 같은느낌임
  
  또한 모델객체를 만들때 파라미터를 설정할수도있지만,이걸 변경할수도 있음
  모델의 bind 메서드를 통해서 특정 파라미터의 값을 변경할수있음
  
5.RAG (Retrieval-Augmented Generation) 기법 이해: 웹 문서에 대한 QA 챗봇 만들기 
  rag모델의 구조는
    검색단계:사용자의 질문이나 컨텍스트를 입력으로 받아,이와 관련된 외부 데이터를 검색,이때 검색api나 db등의 소스를 사용함
	  검색된 데이터는 질문에 대한 답변을  생성하는데 적합하고 상세한 정보를 포함하는것을 목표로함
	생성단계:검색된 데이터를 기반으로 검색된 정보와 기존지식을 결합하여 주어진 질문에 대한 답변을 생성함
  이런식으로 구성됨
  rag를 사용하면 환각을 방지하고,실시간정보를 반영하고,풍부한 정보를 제공할수있음
  
  rag는 보통 검색이나 벡터db를 사용하는데,벡터db는 텍스트를 청크단위로 나눠 임베딩해서 저장하고,쿼리에 가장 가까운 값을 뽑아내는식으로 사용됨
  rag는
    데이터 가져오기(뉴스나 뭐 이런거)
	텍스트 쪼개기
	인덱싱(임베딩)
	검색
	생성
  으로 구성됨,인덱싱까지는 사전준비로 할수있음
  또한 스플릿을 할떄,RecursiveCharacterTextSplitter를 사용하면 문장이 끝나는 지점에서 끊어줌(쉼표,줄바꿈등)
  그리고 rag를 사용하는 검색을 할떈 Temperature를 0에 가깝게(0도 됨) 낮추는게 좀 더 일관성있는 대답을 함
  
  랭체인에선 
    retriever=vectorstore.asretriener()
	rag_chain=(
	  {'context11':retriever|docs,'question11':RunnablePassthrough()} //RunnablePassthrough()는 입력을 그대로 다음단계로 전달
	  | prompt
	  | model
	  | StrOutputParser()
	)//context11는 prompt에서 만들어둔 키값
  이런식으로 사용하면됨 
  
  
6.LCEL 문법: RunnableSequence, RunnablePassthrough, RunnableParallel, RunnableLambda 핵심 설명 
  lcel은 랭체인의 기본문법임
  이떄까지 |로 연결하던것도 이것중 하나임
  대표적으론
    선언적 체이닝:여러 컴포넌트를 | 로 연결해서 파이프라인을 구성할수있고,실행은 순차적으로 이뤄짐
	재사용성:lcel로 정의된 체인은 독립적인 컴포넌트(객체)로 취급되어 재사용이 가능
	유연한 입출력 처리:.invoke() .batch() .stream()등 다양한 실행방식을 지원함
	배치처리 최적화:여러 입력을 동시에 처리할때 자동으로 최적화를 수행하여 효율성을 높임
  이 있음
  
  기본적인 체인은 
    prompt | llm | 출력파서 
  이게 가장 기본체인이고,체인의 invoke에 프롬프트의 변수 키와 값을 넣어주는게 가장 기본적인 실행법임
  출력파서에선 json,xml,string등 다양한 형식이 있음
  이걸쓰면 잘못된 형식의 응답을 필터링해서 안정적인 데이터처리를 보장함(예외처리를 가능하게함)
  또한 추론모델에서의 추론부분(think태그등)을 제거하고 응답만 사용하는것도 간단하게 가능해짐
  
  모든 랭체인 컴포넌트는 Runnable 인터페이스를 구현하여 일관된 방식으로 실행됨
  이 Runnable 인터페이스엔 .invoke() .batch() .stream() .astream()등 다양한 실행방식의 메서드가 있음
  또한 이걸 통해서 |로 서로 연결할수있어짐
  대표적인 Runnable로는
    RunnableSequence:여러 Runnable을 순차적으로 실행
	RunnablePassthrough:입력을 그대로 다음단계로 전달
	RunnableParallel:여러 Runnable을 병렬로 실행
	RunnableLambda:함수를 Runnable로 래핑해서 체인에서 사용
  가 있음
  
  RunnableSequence은 가장 기본이 되는 방식으로 | 임,말고도 함수로도 만들수있는데,이떄 middle는 리스트로 전달해야하는거에 주의
  즉 각 구성요소의 출력을 다음단계의 입력으로 사용하는 가장 기본적인 방식
  이건 동기와 비동기실행을 지원하고,배치,스트리밍도 지원함
  이건 기본적인 체인을 구성하거나,데이터 전처리/후처리 파이프라인을 만들거나,멀티스텝작업(여러단계를 거쳐서 실행되어야하는 작업)을 자동화할때 사용됨
  
  RunnableParallel은 실행가능한 객체들을 딕셔너리로 구성해서 병렬처리를 하는 컴포넌트임
  즉 모든 값이 동시에 실행되어 효율적인 처리가 가능함
  입력값은 모든 병렬실행 컴포넌트에 동일하게 전달되고,각 실행결과는 원래키와 매핑된 새 딕셔너리로 반환됨
  이건 데이터변환과 파이프라인구성에 유용하고,한 Runnable의 출력을 다음임력에 맞게 조정할때 유용함
  즉 한 체인을 실행할때,거기에서 값을 넣기위한 서로 다른 두개이상의 체인을 실행시켜야할때,이걸 병렬실행하는 용도로 생각하면됨
  질문에서 이 질문의 언어와,이 질문의 종류를 받아서 프롬프트에 컨텍스트로 넘기려고 할떄(프롬프트를 보강할때),
  이 두가지 정보는 서로 관련이 없으니 병렬실행하고싶을때 사용하면됨
  즉 이렇게 한 체인을 완성한후에 시퀸스로 실행을 하는식임(즉 특정부분만 병렬처리가 가능)
  
  RunnablePassthrough는 입력값을 그대로 통과시킴(입력프롬프트를 그대로 리턴)
  즉 원본입력을 보존할때 사용하고,RunnableParallel과 조합하여 사용될때 유용하고,입력데이터를 병렬실행맵의 새로운 키로 전달하는 기능을 제공함
  이건 데이터흐름을 투명하게 만들고,그래서 디버깅과 파이프라인구성을 단순화하는 장점이 있음
  즉 질문을 보존해서 저장하고,답변과 같이 받아보고싶을때 사용하기좋음(특히 병렬에서 쓰기편함)
  또한 키값으로 받으면 키와 값을 모두 보존해서 리턴하고,값만 받으면 값만 보존해서 리턴함(전혀 수정하지않고 그대로 리턴)
  
  RunnableLambda는 일반함수를 Runnable객체로 변환하여 체인에서 사용할수있게해줌
  즉 데이터 전처리,후러치,변환등의 후처리로직을 체인에 쉽게 통합할수있게해줌
  뭐 영어일경우 si단위계를 미국단위계로 변환한다던가,텍스트를 소문자로 바꾸고 공백을 제거한다던가,llm리턴값의 글자수를 센다던가 하는걸 할수있어짐
  이건 그냥
    RunnableLambda(함수명)
  이렇게 쓰면되고 그대로 Runnable쓰는거처럼 쓰면됨 |로 연결해도되고(그냥 체인연결하면됨)
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
