1.체인(chain)에 대한 이해 : 기본 LLM 체인 (Prompt + LLM) | 멀티 체인
  기본체인이란 프롬프트와 llm이 결합되어있는 구조를 말함
  즉 프롬프트로 사용자의 입력을 받고({name}은 몇살입니까? 이런식),
  입력된 지시문을 llm에게 전달해주고,
  그 지시문을 통해 llm이 질문에 대한 답변을 만드는 구조가 기본체인임
  
  체인으로 나온 응답은 어플리케이션으로 반환되고,이 응답을 직접 사용할수도,생성된 텍스트만 추출할수도,요약할수도 있음
  
  랭체인코어에서 ChatPromptTemplate를 사용해서 시스템메시지와 유저메시지를 분리해서 사용할수있음
  구현적으로는 시스템 유저 분리보단 해당 키값으로 파싱해서 넣는느낌에 가깝지만
  
  그리고 최종출력에서 텍스트만 추출하는 파서가 StrOutputParser()임
  그래서 체인 구성은
    chain=prompt | llm | output_parser
  이런식으로 구성하면됨
  그리고
    chain.invoke("input":"질문")//input는 질문의 키값을 넣으면됨
  이런식으로 사용하면,프롬프트를 키값으로 수정하고,llm에 던지고,output_parser로 출력을 가공해서 리턴함
  
  멀티체인은 기본체인을 하나가 아니라 두개이상 연결해서 사용하는걸 말함
  그래서 좀 더 복잡한 작업을 할수있어짐
  즉 체인1에서 출력한 값을,체인2에 입력값으로 사용해서 최종출력을 뽑아내는식임
  이건
    chain2= {"eng_word":chain1} | prompt2 | llm | output_parser
	chain2.invoke({"kor_word":"미래"})
  이런식으로 구성하면 chain2를 실행하기전에 먼저 chain1이 실행되고,그 결과물이 "eng_word"에 담겨져서 prompt2로 전달되는식으로 체인이 일어남
  
2.프롬프트(prompt) 만들기 : Prompt Template 이해 및 적용 
  프롬프트를 어떤 틀(템플릿)을 만들어두면 재사용이 편함 
  즉 변하는 부분만 값을 넣어가면서 쓰는것,{aaa}이렇게 쓰고 format메서드로 키값으로 변수를 넣어서 쓰면됨
  또한 기본적으로 스트링이라서 +로 더할수있고,그래서 각 블록별로 모듈화를 시켜서 조합해서 쓸수가있음
  
  이걸 그냥 체인에 넣어서 쓰면됨
  이렇게 기본적인건 PromptTemplate고,
  좀 복잡한건 ChatPromptTemplate도 있음
  이건 여러 메시지를 가진 리스트로 구성되고,각 메시지는 역할과 내용으로 구성됨
  이건
    시스템메시지(페르소나등)
	휴먼메시지(사용자 질문)
	AI메시지(모델의 응답)
	펑션메시지(함수호출결과)
	툴메시지(툴콜링 결과)
  로 나뉨
  각 메시지는 롤을 프롬프트에 표시하고있어서(시스템메시지인지 실제 질의,즉 휴먼메시지인지 등) llm이 쉽게 구분할수있음
  
  
3.LLM 모델 구조 : LLM 클래스와 ChatModel 모델 클래스 구분
  랭체인의 모델 클래스엔 LLM과 ChatModel이 있음
  LLM은 단일요청에 대한 복잡한 출력을 생성하는데 적합하고,ChatModel은 사용자와의 상호작용을 통한 연속적인 대화에 적합함
  
  LLM은 스트링을 입력으로 받아서 처리하고,스트링을 반환함,이건 광범위한 언어 이해 및 텍스트 생성작업에 사용됨(문서요약,컨텐츠생성등)
  ChatModel은 메시지리스트(ChatPromptTemplate)를 입력으로 받고,하나의 메시지를 반환함,
  이건 대화형상황에 최적화되어있고 연속적인 대화를 처리하는데 사용됨,즉 맥락을 유지하면서 응답을 생성하는데 중점을 둠
  
4.LLM 모델 튜닝: 모델 파라미터(model parameter) 설정  
  모델 파라미터에는
    Temperature:텍스트의 다양성 조정
	MaxTokens:생성할 최대 토큰 수 조정
	TopP:생성과정에서 텍스트의 다양성 조정을 임계값 조정 방식으로 하는것
	Frequency Penalty:이미 등장한 단어나 구절이 등장할 확률의 감소시킴,반복을 줄일수있음
	Presence Penalty:텍스트내에서 단어의 존재 유무에 따라 그 단어의 선택확률을 조정함
	Stop Sequences:특정 단어나 구절이 등장하면 생성을 멈춤
  등이 있음
  Temperature와 TopP는 거의 같은느낌이고,Frequency Penalty와 Presence Penalty도 거의 같은느낌임
  
  또한 모델객체를 만들때 파라미터를 설정할수도있지만,이걸 변경할수도 있음
  모델의 bind 메서드를 통해서 특정 파라미터의 값을 변경할수있음
  
5.RAG (Retrieval-Augmented Generation) 기법 이해: 웹 문서에 대한 QA 챗봇 만들기 
  rag모델의 구조는
    검색단계:사용자의 질문이나 컨텍스트를 입력으로 받아,이와 관련된 외부 데이터를 검색,이때 검색api나 db등의 소스를 사용함
	  검색된 데이터는 질문에 대한 답변을  생성하는데 적합하고 상세한 정보를 포함하는것을 목표로함
	생성단계:검색된 데이터를 기반으로 검색된 정보와 기존지식을 결합하여 주어진 질문에 대한 답변을 생성함
  이런식으로 구성됨
  rag를 사용하면 환각을 방지하고,실시간정보를 반영하고,풍부한 정보를 제공할수있음
  
  rag는 보통 검색이나 벡터db를 사용하는데,벡터db는 텍스트를 청크단위로 나눠 임베딩해서 저장하고,쿼리에 가장 가까운 값을 뽑아내는식으로 사용됨
  rag는
    데이터 가져오기(뉴스나 뭐 이런거)
	텍스트 쪼개기
	인덱싱(임베딩)
	검색
	생성
  으로 구성됨,인덱싱까지는 사전준비로 할수있음
  또한 스플릿을 할떄,RecursiveCharacterTextSplitter를 사용하면 문장이 끝나는 지점에서 끊어줌(쉼표,줄바꿈등)
  그리고 rag를 사용하는 검색을 할떈 Temperature를 0에 가깝게(0도 됨) 낮추는게 좀 더 일관성있는 대답을 함
  
  랭체인에선 
    retriever=vectorstore.asretriener()
	rag_chain=(
	  {'context11':retriever|docs,'question11':RunnablePassthrough()} //RunnablePassthrough()는 키보드입력값 받는 함수
	  | prompt
	  | model
	  | StrOutputParser()
	)//context11는 prompt에서 만들어둔 키값
  이런식으로 사용하면됨 
  
  
6.LCEL 문법: RunnableSequence, RunnablePassthrough, RunnableParallel, RunnableLambda 핵심 설명 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
