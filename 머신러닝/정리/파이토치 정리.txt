텐서=배치사이즈(하나마다 데이터하나),차원(파라미터)
t=(64,256)
이미지3d텐서=(배치사이즈,width,height)
파이토치의 경우에,배치사이즈,가로,깊이값순으로 넣으면됨
nlp3d텐서=(배치사이즈,길이(타임스탭),차원(문장))


numpy
	t.array([1,2,3,4]) 배열생성
	t.ndim ->차원수보기
	t.shape ->형상보기
touch
	t=torch.FloatTensor([0.,1.,2.,3.]) 텐서생성 
	t.dim() 차원수보기
	t.shape 형상보기
	t.size() 형상보기
	t.[2:5] 넘파이랑똑같음(슬라이스)
	t.mean() 전체값평균(단일값출력)
	t.mean(dim=0) 0차원의 평균(배열출력)  1x2에서 앞의 1만 남긴다고보면됨
	t.mean(dim=1) 1차원평균(배열출력)     1x2에서 뒤의 2만 남긴다고보면됨
	t.mean(dim=-1)마지막차원평균(배열출력)
	t.sum(dim) 차원으로 더하기
	t.max(dim) 차원에서 가장큰값  dim이 0이면 세로단에서 max치는거고 1이면 가로단에서 max
	t.argmax(dim) 차원에서 가장큰값의 인덱스,원핫인코딩에서 자주나옴,만약 두개이상일경우 먼저발견되는것
	t.view([-1,3])  reshape임,-1은 알아서하라는거고,3으로 고정하면,총갯수가 12면 4x3이 되고 15면 5x3이 되는식
					즉 와일드카드인데 특성상 전체중에 하나만 들어갈수있음(-1,1,3)이건가능(-1,-1,-1) 이건안됨,단 원래있던크기랑 같아야함
	t.squeeze() 전체배열의 차원에서 갯수가 1인걸 없애줌([[0],[1],[2]])->([0,1,2])
	t.squeeze(dim=1) 1차원에서 dim이 1이면 없앰(그차원만 없앰)
	t.unsqueeze(dim=1) 해당차원에 차원을 추가함
	t.float() float로 타입변경
	t.long()롱으로 변경
	touch.cat([x,y],dim=0) 0차원기준으로 두 텐서를 붙임(concat)
	touch.stack([x,y,z]) 0차원기준으로 텐서들을 쌓음(0차원기준으로 1차원배열처럼 추가됨)[1,2][3,4][5,6]->[[1,2],[3,4],[5,6]]
	touch.stack([x,y,z],dim=1)1차원기준으로 텐서들을쌓음(1차원기준으로 각 층에 추가됨)[1,2][3,4][5,6]->[1,3,5][2,4,6]
	touch.ones_like(x) x를 1로채운 텐서 리턴
	touch.zeros_like(x) x를 0로채운 텐서 리턴
	t.mul(2) 전체x2를 함(새로운 메모리선언하고,원본값은 바뀌지않음)
	t.mul_(2)전체x2를 함(원래 메모리에 덮어씀,깊은복사,즉 원본값이 바뀜) 사실쓸일없을듯
	torch.matmul(w) 텐서간(행렬) 곱
	x_train.matmul(w) xtrain과 w를 곱함
같은 크기의 텐서를 더하거나 빼면 같은위치에서 변동일어나고
텐서에 벡터(단일크기같은)를 더하면 그방향으로 같은값이 복사되어서 텐서로 변경된다음에 더해짐
크기가 다른벡터는 양쪽으로 값을 복사해서(1x2+2x1 ->2x2+2x2) 연산이 진행됨

bt=lt==3 이런 논리문을 텐서에 걸면,각 위치마다 t,f를 넣은 byteTensor이 출력됨

행렬곱은 2x2   * 2x1일떄 앞의 뒷자리와 뒤의 앞자리가 같을때 수행할수있고 그걸 양쪽다없애고 2x1로 출력이나옴
(앞의뒤와 뒤의앞을 합치고 앞의앞과 뒤의뒤만 남음)


cpu디바이스텐서와 gpu디바이스텐서를 연산하면 에러남
gpu+gpu경우에도 다른디바이스면 에러남

선형모델은 가장 모델과 맞는 직선을 찾음
w는 가중치 b는 편향
w=torch.zeros(1,requires_grad=True)
b=torch.zeros(1,requires_grad=True) requires_grad=True는 학습할거라고 명시하는것
hypothesis=x_train*w +b


선형모델에서 로스펑션은 mse사용
cost=torch.mean((hypothesis=y_train)**2) //mse

optimizer=optim.SGD([w,b],lr=0.01) //w,b는 학습할텐서들

optimizer.zero_grad() //기울기초기화
cost.backward() //기울기계산
optimizer.step() //스텝으로 개선(진행)

즉 
xtrain=adada
ytrain=adadad
w=torch.zeros(1,requires_grad=True)
b=torch.zeros(1,requires_grad=True) requires_grad=True는 학습할거라고 명시하는것


optimizer=optim.SGD([w,b],lr=0.01) //w,b는 학습할텐서들
//여기까진 한번만


for epoch in range(1,30):
    hypothesis=x_train*w +b    //계산
	cost=torch.mean((hypothesis=y_train)**2) //mse
	
	optimizer.zero_grad() //기울기초기화
	cost.backward() //기울기계산
	optimizer.step() //스텝으로 개선(진행)
이런식으로 진행됨

비용함수는 0일때 가장 좋음
그래서 그레이디언트는 0에 수렴하게 만들어야함->학습을 해당방향의 기울기x학습률만큼 이동(빼는)하는식으로 함

기본적으로 파이토치에선,옵티마이저를 선언하고(설정하고)
옵티마이저를 초기화하고 코스트펑션으로 기울기를 계산한후에 optimizer.step로 경사하강법을 진행하는식으로 간략화되어있음
	optimizer.zero_grad() //기울기초기화
	cost.backward() //기울기계산
	optimizer.step() //스텝으로 개선(진행)
이거임
즉,옵티마이저에 설정된 파라미터만 저장해서,그걸 계산하고 연산해서 경사하강법을 진행하는거임


만약 입력 파라미터갯수가 여러개일경우
	w=torch.zeros((3,1),requires_grad=True)
	hypothesis=x_train[0]*w[0] +x_train[1]*w[1]+x_train[2]*w[2] +b 
이런식으로 됨
근데 이게 파라미터가 많아지면 다적기힘드니까
	hypothesis=x_train.matmal(w)+b
로 사용함(x_train과 w의 같은항을 묶어서 계산)
이경우에도 비용함수의 계산식은 동일하고
	cost=torch.mean((hypothesis=y_train)**2)
옵티마이저도 동일함
	optimizer=optim.SGD([w,b],lr=0.01) //w,b는 학습할텐서들

	optimizer.zero_grad() //기울기초기화
	cost.backward() //기울기계산
	optimizer.step() //스텝으로 개선(진행)

근데 이 
		hypothesis=x_train.matmal(w)+b
이거도 귀찮으니까,보통은 
	class aamodel(nn.module):
		def __init__(self):
			super().__init__()
			self.linear=nn.Linear(3,1)  //레이어쌓기
			
		def forward(self,x):
			return self.linear(x)
			
	model=aamodel()		
	hypothesis=model(x_train)
	optimizer=optim.SGD(model.parameters(),lr=0.01) //w,b는 학습할텐서들
	for epoch in range(20+1):
		optimizer.zero_grad() //기울기초기화
		cost.backward() //기울기계산
		optimizer.step() //스텝으로 개선(진행)
		
		
이런식으로 forward를 정의하면 hypothesis를 계산해주고,gradient계산은 backward()를 정의하면 파이토치가 알아서해줌

그리고 파이토치 자체에서 비용함수를 제공함
	import torch.nn.functional as F
	
	cost=F.mse_loss(prediction,y_train)
이렇게


데이터가 많아서 메모리에 들어가지않을경우엔,미니배치를 사용해서 데이터를 떼서 조금씩 역전파하는식으로 하고,
데이터를 전부사용하면 한 에포크가 지나는식으로 할수있음(이게 미니배치임)
장점은
	학습이 빠름
	메모리에 다 넣을수있고,저장공간이 덜필요
단점은
	잘못된 학습이 될수있음(좀 학습값이 튐)

미니배치를 파이토치에서 구현할땐
	from torch.utils.data import Dataset
	class cuDataset(Dataset):
		def __init__(self):
			self.x_data= 데이터값
			self.y_data= 데이터레이블값
			
		def __len__(self):
			return len(self.x_data)
			
		def __getitem__(self,idx):
			x=torch.FloatTensor(self.x_data[idx])
			y=torch.FloatTensor(self.y_data[idx])
			return x,y
	
	dataset=cuDataset()
이렇게 init에 데이터넣고 len과 getitem을 구현하고
	from torch.utils.data import DataLoader
	
	dataloader=DataLoader(dataset,batch_size=2,shuffle=True)//데이터셋넣고,배치사이즈 지정하고(보통 2의제곱수),
															//셔플은 에포크마다 데이터 섞는거

그리고 이렇게 한뒤에 학습하는건
	for epoch in range(21):
		for batch_idx,samples in enumerate(dataloader):
			x_train,y_train=samples
			
			prediction=model(x_train)
			
			cost=F.mse_loss(prediction,y_train)
			
			optimizer.zero_grad() //기울기초기화
			cost.backward() //기울기계산
			optimizer.step() //스텝으로 개선(진행)			
			
			print('epoch {:4d} batch{}/{} Cost:{:.6f}'.format(
				epoch,21,batch_idx+1,len(dataloader),cost.item()))
이렇게 순전파를하고 코스트를 구하고 기울기를 계산하고 스탭을 밟음


로지스틱회귀는 0~1의 값을 뽑아내는 회귀문제임,이걸 0.5으로 잘라서 클래시피케이션하면 분류문제가 되는것
그래서 이진분류할때 시그모이드를 사용함(보통 이진분류의 마지막층에서 사용)

이경우
	w=w-lr*미분값(그레이디언트)*cost(w) 
이렇게 학습이 일어남

h(가설함수,y=wx+b 같은,w와 b의 값을 찾을때의 뼈대,레이어에서의 함수랑 같은거임)는
 h=1/(1+torch.exp(-(x_train.matmul(w)+b))->h=torch.sigmoid(x_train.matmal(w)+b)
비용함수는 이진크로스엔트로피
	losses=-(y_train*torch.log(h)+(1-y_train)*torch.log(1-h))
	cost=losses.mean()
이런식
내부함수쓰면
	F.binary_cross_entropy(h,y_train)
	
실제구현은
	w=torch.zeros((2,1),requires_grad=True)
	b=torch.zeros(1,requires_grad=True)
	optimizer=optim.SGD([w,b],lr=1)
	for epoch in range(21):


			h=torch.sigmoid(x_train.matmal(w)+b)
			cost=F.binary_cross_entropy(h,x_train)
			
			optimizer.zero_grad() //기울기초기화
			cost.backward() //기울기계산
			optimizer.step() //스텝으로 개선(진행)			
			
			print('epoch {:4d} batch{}/{} Cost:{:.6f}'.format(
				epoch,21,batch_idx+1,len(dataloader),cost.item()))

이렇게만 하면 회귀고,
prediction=hypothesis>=torch.FloatTensor([0.5])
하면 1아니면 0으로 나옴
그리고
corrent_pre=prediction.float()==y_train
으로 정답확인을 하고,저걸 평균내면 acc(정확도)가 나옴
즉 다더하고 len으로 나누고 100곱하면됨

실제로 구현할땐
	class binaryClassifier(nn.module):
		def __init__(self):
			super().__init__()
			self.liner=nn.Linear(2,1)
			self.sigmoid=nn.Sigmoid()
		def forward(self,x):
			return self.sigmoid(self.linear(x))
이런식으로 레이어를 구현함

다중분류에선 softmax를 사용함
이산확률분포는 1,2,3,4,5,6처럼 .5가 나오지않는것
연속확률분포는 연속적으로 1.1,1.2이렇게 연속적으로 분포하는것
소프트맥스는 이산확률분포를 연속확률분포처럼 근사할수있음(1,2,3일때 0.09,0.25,0.67 이런식으로 잘라서 이산적으로 처리)
	F.softmax(z,dim=0)//나올것의 갯수가들어있는 텐서(1,2,3),그텐서에서 차원 선택
크로스엔트로피는 현재가설분포에서  역전파분포값만큼을 빼는식으로 이상적인가설에 다가가는식

원핫을 만들땐
	yonehot=torch.zeros_like(h)
	yonehot.scatter_(1,y.unsqueeze(1),1)//dim1차원

그리고 크로스엔트로피 비용함수는
	cost=(yonehot * -torch.log(h)).sum(dim=1).mean()
	        3,5           3,5         3,1        1,1
이렇게 나옴
이건 행렬에대해 연산을하고,그걸 더해서 (x,1)로 줄인다음,그걸 평균내서 스칼라값으로 뽑아내는것
파이토치에서 소프트맥스는
	F.softmax(z,dim=0)
	F.log_softmax(z,dim=1)
로 제공함
크로스엔트로피는
	F.nll_loss(F.log_softmax(z,dim=1),y)//z==h y==y_train
	F.cross_entropy(z,y) //z==h  y==y_train
이렇게 사용할수있음
실제구현은
	w=torch.zeros((4,3),requires_grad=True)//입력4 출력3
	b=torch.zeros(1,requires_grad=True)
	optimizer=optim.SGD([w,b],lr=1)
	for epoch in range(21):


			z=x_train.matmal(w)+b
			cost=F.cross_entropy(z,y_train)
			
			optimizer.zero_grad() //기울기초기화
			cost.backward() //기울기계산
			optimizer.step() //스텝으로 개선(진행)			
			
			print('epoch {:4d} batch{}/{} Cost:{:.6f}'.format(
				epoch,21,batch_idx+1,len(dataloader),cost.item()))

레이어(모델)로 만들땐
	class softmaxClassifierModel(nn.module):
		def __init__(self):
			super().__init__()
			self.liner=nn.Linear(4,3)//입력값이4 출력값이3
			
		def forward(self,x):
			return self.linear(x)

model=softmaxClassifierModel()
하면됨
모델을사용할떄 구현은
	model=softmaxClassifierModel()
	optimizer=optim.SGD(model.parameters,lr=0.1)
	for epoch in range(21):


			prediction=model(x_train)  //xtrain=(m,4) prediction=(m,3)
			
			cost=F.cross_entropy(prediction,y_train)  //y_train=(m,1)
			
			optimizer.zero_grad() //기울기초기화
			cost.backward() //기울기계산
			optimizer.step() //스텝으로 개선(진행)			
			
			print('epoch {:4d} batch{}/{} Cost:{:.6f}'.format(
				epoch,21,batch_idx+1,len(dataloader),cost.item()))


보통 다중분류에선 소프트맥스(마지막층함수),크로스 엔트로피(손실함수) 이진분류에선 바이너리 크로스 엔트로피(손실),시그모이드(마지막층함수)를 사용함


