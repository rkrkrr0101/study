1.파이썬기초
    넘어감
 


 
2.퍼셉트론
	퍼셉트론은 다수의 신호를 받아서 하나의 신호를 출력(그 머시기회로시간에 했던 and게이트 xor게이트 그거)
	신호값(뉴런값)은 안바뀌니까 거기에 가중치가 곱해져서 값을 결정하고,그 값이 임계값을 넘었을때 1,0을 출력
	
	여기서 xor같은경우 퍼셉트론 하나로는 표현할수없고,두개이상으로 구성되어야함
	그래서 다중퍼셉트론임(or nand and사용)




	
3.신경망
	신경망에서 편향은,뉴런들사이에 상수항의 뉴런이 하나 있다고 보면됨
	활성화함수는 입력신호의 총합이 활성화를 일으키는지를 정하는 역할
	
1.활성화함수
	활성화함수는 임계값을 기준으로 출력이 바뀌는데,이런함수를 계단함수라고 함
	그런데 시그모이드같이 계단형태가 아닌 함수들도 있음
	
	시그모이드는 입력을 주면 0~1사이의 출력을 돌려주는 함수임
	사실상 퍼셉트론과 신경망의 차이는 활성화함수가 거의 다임
	
	그래서 퍼셉트론에선 0과 1의 값을 돌려줬지만,시그모이드를 사용한 신경망에선 연속적인 실수(0~1사이의)를 돌려줌
	
	그리고 또 다른 건 계단함수와 시그모이드 둘다 비선형 함수임
	신경망에선 활성화 함수로 비선형 함수를사용해야함,선형함수를 사용하면 안됨
	
	왜냐면 출력값에서 선형함수를 써버리면,신경망을 쓰는 이유가 없어짐(결국 표현을 선형으로밖에 못하기때문에)
	
	1.relu
		relu는 입력이 0을 넘으면 그 입력을 그대로 출력하고,0 이하면 0을 출력하는 함수임
		그냥 max(0,x)라고 보면됨
		
2.출력층
	회귀는 항등함수(값그대로 내보냄)
	이진분류는 시그모이드
	다중분류는 소프트맥스를 씀
	
	소프트맥스는 마지막 각 뉴런마다 전체뉴런 총합을 1로한 값을 나눠줌
	근데 그냥소프트맥스를 쓰면 오버플로 날확률이 높아서 입력신호의 최댓값을 분모분자에 둘다 곱해서 값을 낮춰서 사용함
	사실 근데 가장 큰 값 잡으면 돼서(소프트맥스는 값의 대소관계가 바뀌진않으니까)없어도 보기불편해서그렇지 추론에는 별문제없는듯
	훈련할때는 소프트맥스 쓰고 추론할땐 소프트맥스 제거하고 사용하는듯
		
		
		
		
		
4.신경망학습

1.손실함수
	신경망에서 그레이디언트 추정할때 사용하는 함수,이걸 최소로 하게 방향을 잡고 움직임
	mse 크로스엔트로피
	
	그리고 손실을 계산할때 보통은 미니배치를 전부 더해서 거기서 계산하고(전부더하고 배치갯수로 나눔,즉 평균) 경사하강함
	
	손실함수를 쓰는 이유는 정확도같은건 계단식이라서 미분하기 적절하지않고,값이 크게변하기때문,
	즉 경사하강법처럼 조금조금 가는거엔 잘 영향을 주지못해서 학습이 잘 안됨
	
2.수치미분
	컴퓨터의 한계상 진짜 미분과 다르게 오차가 포함된 미분을 하는데 이걸 수치미분(근사로구한 접선)이라고함
	
	여기서 딥러닝에선 각 항에 편미분(정해진거하나빼고 전부상수취급)을 해서 각 항마다의 값을 배열로 만든후 거기에 현재값을 대입해서 그걸가지고 경사하강법을 함
	
	즉 이렇게 각 항마다의 값을 배열로 만들걸 그레이디언트라고 함
	
3.기울기
	그레이디언트가 최저가 되는 지점을(최소한 현재위치에서 값을 제일 크게줄이는쪽을) 찾으려면 기울기에 -를 붙인거만큼 이동하면 됨
	그냥 이동하면 너무 크게움직이니까,기울기에 학습률을 곱한거만큼 그방향으로 움직이고 다음스텝을 진행함,이게 경사하강법임
	
	즉 그레이디언트(편미분의 배열)을 각 항마다 학습률x배열의 그 항 만큼을 빼주는식으로 이동하고,이동한데서 또 반복
	
	즉 초기값을 랜덤으로 초기화하고,거기서 경사하강법으로 기울기가 0이되는 전역최저점을 향해 가는것
	
	
	1.신경망에서의 기울기
		신경망에서는 w를 각 항으로 보고 모든w에 대해 기울기를 구해서 경사하강법을 진행함
		즉 모든w에 편미분하고 현재값을 대입해서 각 w마다 조금씩 변경시킴
	
	
	
	
	
	
5.오차역전파	
	계산그래프는 국소적계산(값과 노드1개)를 반복해서 최종결과를 얻는 식임
	즉 현재처리하는거말곤 다 무시해도됨
	
	그리고 연쇄법칙에 따라 시작값미분값/현재미분값으로 식이 단순화됨(위아래 서로약분하면서 사라짐)
	
	
1.각 노드별 역전파
	1.덧셈노드의 역전파
		덧셈노드는 그냥 그대로 흘려보냄(전값을 그대로 흘려보냄)
		전값이 3이면 양쪽노드에 전부 3을 흘려보냄
		
	2.곱셈노드의 역전파
		곱셈노드의 역전파는 양쪽을 거꾸로해서 전값을 곱함
			x*y=z일경우
			yz xz로 거꾸로 들고감
			
	즉 이렇게해서 각 시작점에서 값이 바뀌면 얼만큼 민감하게 반응하는지를 알수있게되고,마지막 오차에 얼마나 기여했는지도 알수있게됨
	
2.활성화함수 계층
	1.relu
		relu는 x가 0보다 클때 그대로,0이하일때 0출력
		
		얘는 순전파때 x가 0보다 크면 그대로 흘리고,0보다 작으면 0을보냄
		역전파때는 순전파 값을 기억했다가 0보다 크면 덧셈처럼 그대로 흘리고,0보다작으면 0을보냄(앞에값에 0을곱해서보냄)
		
	2.시그모이드
		시그모이드는 * exp + /순으로 연결된 층의 묶음이라고 생각하면됨
		exp층은 exp(x)를 수행하고 /는 y=1/x를 수행함
		
		+랑 *는 그대로 하면되고
		/는 역전파할때 -y^2(순전파값을 제곱하고 -붙임)을 전값이랑 곱해서 흘려보냄
		
		exp층은 전값에 순전파때의 출력(exp(x))을 곱해서 흘려보냄(만약 순전파값이 -x^2이었으면 exp(-x^2))
		
		이걸 전부 묶으면
			전값* 순전파의 상류값^2*exp(-순전파의 하류값)
		이 됨 
		이걸 또 정리하면
			전값*상류값(1-상류값)
		이 됨
		
		즉 시그모이드의 역전파는 순전파의 출력(상류값)으로만 계산이 가능함

3.affine/softmax층 구현
	1.affine
		기본적으로 뉴런의 가중치합은 x 점곱 w + 편향 임
		여기서 w는 (x의갯수,y의갯수)만큼을 가지고있음
		여기서 행렬곱을 어파인변환이라고 하고,이걸하는걸 어파인층이라고 함(즉,dense임)
		
		행렬도 계산그래프 곱연산과 같은데 행렬이다보니 좀 다른게 있긴함
		역전파시에 전값*wT,전값*xT(전치행렬,(i,j)를 (j,i)로 바꾼거)를 해서 보내면됨
		즉 가중치와 입력값을 거꾸로보내는건 같은데,거꾸로 보낼때 전치행렬을 곱해서 보내야함
		
		덧셈은 똑같이 그대로 보내면됨
		
		여기서 배치를 묶어서 역전파를 할때도 똑같이 (n,x)로 하면됨
		
		그리고 편향을 더할때도 x와w에 두 데이터에 대한 미분을 데이터마다 더해서 구함(같은칸([0][1],[1][1],[2][1])의 모든합을 가지고)
	2.softmax with loss
		소프트맥스는 어짜피 출력층에서밖에 안쓰니까 로스도 묶어서 계산함
		그리고 학습할때 말고는 소프트맥스를 넣을 이유가 없음,값이 크기가 바뀌는거지 순서가 바뀌지않음
		그냥 큰거 고르면됨
		
		소프트맥스의 역전파는,y-t로 나옴,즉 순전파때의 값 두개를 뺀값
		즉 여기서 예측과 실제의 차이만큼 값을 역전파해 나가는거임
	
		
	
	
	
6.학습관련기술들	
1.매개변수 갱신
	1.sgd
			w=w-학습률x손실함수기울기
		sgd는 안장을 거꾸로한 그런 그래프(기울기가 y축으로 크고 x축으로 작은)에서는 효율이 높지않아서,다른걸 사용함
	2.모멘텀
			v=av-학습률x손실함수기울기(a는 하이퍼파라미터로,0.9쯤 설정함,가만히있을때 계속 감소시키는역할,마찰이나 공기저항같은느낌)
			w=w+v
		모멘텀최적화는 만약 x축방향으로 힘이 약하긴하지만 일정하게 계속 힘이 주어지고있으면,그쪽으로 힘을 더 세게줘서(가속도)빠르게 수렴하게 하는방식임
	3.AdaGrad
		얘는 w 배열의 각각의 가중치마다 학습률값을 둬서,전부 다르게 학습하게 하는 방식임
		즉 적응형으로 학습률을 조정함
			h=h+손실함수기울기 점곱 손실함수기울기
			w=w-학습률*(1/루트h)*손실함수기울기
			
		매개변수의 원소중에서 크게 움직인(크게 갱신된)원소는 학습률이 낮아진다는거
		
		근데 이거는 직접사용하진 않음,과거의 기울기를 계속 제곱해서 더하니까 결국은 갱신량이 0이되어서 갱신이 안되게됨
	
	4.Adam
		그래서 나온게 Adam(정확히는 rmsprop지만)
		얘는 rmsprop(얘는 각각의 가중치마다 학습률값을 두긴하는데,이전을 전부더하는게 아닌 최근값만 가지고 판단함,즉 무한대로 늘일이 없음)
		랑 모멘텀최적화를 합친방식임
		보통 이걸 제일 많이씀
		
2.가중치의 초깃값
	1.0초기화
		0초기화는 모든 뉴런이 똑같이 갱신되게해서 뉴런수를 늘리는게 아무의미없게 돼서 안씀
	2.은닉층의 활성화값 분포
		그냥 가중치를 표편1로잡힌 정규분포로 초기화할수도 있지만,그러면 양쪽극단값이 엄청치우쳐지게 나와서 잘안씀
		양극단에 치우쳐져있으면 기울기소실이나 기울기폭주같은 문제가 생기기쉬움
		
		이걸 표편 0.01 정규분포로 초기화하면,이번엔 0.5부근에서 엄청 치우쳐지게 나옴
		이러면 기울기소실같은건 안생기는데,뉴런들이 맨날 똑같은값만 뱉으니까 뉴런1개랑 다를바없어짐 
		
		그래서 쓰는게 xavier초기화(1/루트n으로 초기화,사실상 루트(1/n)과 같음1이니까 뺀거,n은 앞층의 노드수)와 he초기화(relu를 사용할때는 he초기화 사용함)
		즉 보통은 다 relu쓸테니까 기본값인 he초기화쓰면됨
		
		he초기화는 루트 (2/n)인 정규분포를 사용함
		relu는 음의영역이 0이니까 넓게 분포시키기위해 2배의 계수가 필요해서 저렇게됨
		
		즉 relu는 he초기화
		시그모이드 tanh등은 xavier초기화를 사용하면됨
		
		
3.배치정규화
	배치정규화는 배치단위로 값들을 정규화시키는거
	
	배치정규화의 장점은
		학습이 빨리진행됨
		초깃값에 크게의존하지않음
		오버피팅을 억제함
	이 있음
	
	배치정규화는 단순히 미니배치 단에서의 데이터들을 평균0 분산1로 변환하는 일을 함
	그리고 배치정규화 계층마다 이 정규화된 데이터에 확대와 이동을 수행하는데,이건 파라미터라서 학습할수있음
	
	
	
	
4.바른학습을 위해(오버피팅방지)
	1.오버피팅
		오버피팅은 데이터가 적거나,매개변수가 많고 표현력이 높은(층과 뉴런이 많은)모델에서 잘 일어남
	2.가중치 감소
		오버피팅을 억제하기위해 사용하는 방법으로,가중치가 크면 거기에 패널티를 부과하여 움직이는걸 줄여서 오버핏을 막는방법임
		허눈벙봅운 가중치의 제곱노름(l2노름)을 손실함수에 더하면됨
	3.드롭아웃
		이건 아예 뉴런을 랜덤으로 꺼버리는거
		이러면 각 뉴런이 모든 역할을 해야해서 오버핏이 줄어듬
		
		이건 역전파때 relu처럼 순전파때 꺼졌는지 아닌지 기억했다가 꺼버리면됨
		
5.하이퍼파라미터 최적화
	순서
		1.하이퍼파라미터 범위 설정
		2.범위내에서 값 무작위 추출
		3.무작위추출값으로 학습하고 정확도평가
		4.이걸 반복해서 결과보고 범위 좁힘
	
	
		
	
	
	
	
7.cnn	
	완전연결층(dense)는 데이터의 형상이 무시되는 단점이 있음
	그래서 가로세로의 데이터를 같이 먹는 합성곱층이 나옴
1.합성곱층	
	1.합성곱연산
		합성곱 연산은 입력데이터를 슬라이스해서,그것의 크기와 같은크기의 필터를 곱해서 더하는식(합성곱)으로 함
		여기서 필터는 랜덤하게 초기화되고(가중치랑 같음,파라미터로 학습의 주요 대상) 곱해서 더해진 행렬을 리턴함
		
		합성곱에서 편향은,필터계산을 완료한거에 하나의 채널당 하나의값을 모든원소에 브로드캐스팅함
		
	2.패딩
		필터크기때문에 합성곱연산하면 계속 줄어드니까 그거때문에 크기조절하는거
	3.스트라이드
		필터를 겹치지않게(한번에 2칸이상씩)넘어가는거
		
		
	4.입출력크기
		입출력크기의 식은
			((높이+2패딩-필터높이)/스트라이드)+1=출력크기
			((길이+2패딩-필터길이)/스트라이드)+1=출력크기
	
	
	
	
	5.3차원데이터의 합성곱연산(채널)
		얘도 똑같이 계산하는데,각 채널마다 필터가 하나씩 있고,편향이 하나씩 있음,그리고 그걸 전부 더해서 하나로 만들거나 함
		
		만약 채널을 여러개 만들려고하면,그 채널덩어리들을 여러개 만들면됨(출력채널수만큼)
		
		정리하면,하나의 출력을 출력할때 입력채널 갯수당 하나의 필터가 있고,그 모든필터들의 값을 합쳐서 한 채널을 만듬
		여러채널을 만들고싶으면 입력채널갯수만큼의 필터를 출력채널갯수만큼 만들어야함
		그리고 여기서 편향은 출력채널마다 하나씩 있음
	
2.풀링층
	풀링도 합성곱처럼 똑같이 슬라이스하는데,거기서 최대풀링이면 최대값을 뽑고 평균풀링이면 평균값을 뽑는식임
	
	풀링은 학습해야할 매개변수가 없고,채널수가 변하지않음(gap같은건 제외하고)
	그리고 입력의 변화에 영향을 적게받음(최대풀링의경우 근처조금바뀌면 값이 바뀌진않으니까)
	
		
3.cnn시각화
	cnn에서는 앞단에선 가로 세로같은 간단한값들이 뽑히고,뒷단으로 갈수록 좀더 추상화되고 고차원적인 데이터로 묶이게됨
	
4.대표cnn
	1.lenet
		98년도에 나온 처음 합성곱신경망
		이때나 지금이나 크게 바뀐건없고,시그모이드를 사용했다는거정도 이떄는
	2.alexnet
		12년도에 나온 합성곱신경망
		relu를쓰고 드롭아웃을 사용한다는거 정도가 차이점
		
	솔직히 별차이는없는데,데이터가 생기고 gpu를 사용한다는거때문에 엄청 발전한거
		
		
		
		
		
		
		
8.딥러닝		
		
		
1.깊은 신경망
	1.vgg
		제일 베이직한 cnn
		필터크기는 전부 3x3이고 층이 깊어지면서 채널수가 점점 늘어나고 conv 3개쯤뒤에 풀링이 붙는형식
		
	2.깊은층의 장점
		층을 깊게하면,즉 필터를 여러개로 쓰면 같은크기를 압축할때도 매개변수의 숫자가 줄어듬
		5x5필터로 25칸을 한번에 먹으면 매개변수가 25개지만,3x3을 2번반복하면 같은 25칸을 먹어도 매개변수가 18개임
		
		그리고 층을 깊게하면 전에말했던거처럼 앞단에선 간단한거,뒤에선 추상적인거로 패턴을 분해하기가 쉬워짐
		
2.딥러닝의 초기역사
	1.이미지넷
		이미지 100만장을 모아둔 데이터셋
	2.vgg
		합성곱과 풀링으로 구성된 기본적인 cnn임
		16층과 19층 2가지 종류가 있음
		3x3필터를 3번쯤 거치고 풀링한번하고 하는식으로 계속줄여나가다가 완전연결층으로 분류하는식
	3.googlenet
		이건 가로층으로 넓은걸 필터로 합쳐서 부분적 앙상블같은느낌
		여기서 나온 1x1합성곱(채널수를 조정하는거)는 채널수 조정할필요있을때 어디든 잘쓰임
	4.resnet
		잔차연결(스킵연결)을 만든것
		인코더-디코더형식일때 같은크기끼리 뒷단에 더해서(입력층쪽걸 출력층쪽에 더해서)앞단의 압축되어 날아가기 전 정보를 가져오는식으로
		층을 깊게해도 괜찮게 만들어진 신경망

3.딥러닝고속화
	cnn의처리의 95퍼센트정도는 합성곱층의 연산에 사용됨
	즉 cnn의 연산효율을 올릴려면 합성곱층을 건드려야함(합성곱층은 단일곱셈-누산임,필터를 곱하고 편향을더하는거니까)
	
	1.분산학습
		합성곱층의 연산은 gpu가 거의 다 담당하고,단일gpu로도 만히 가속할수있긴하지만
		그래도 여러개쓰는게 더빠르긴한데 신경써야할게 많음,병목,가중치 동기화등
		그냥 이런건 텐서플로같은거한테 하라고하자
		
	2.낮은 연산정밀도
		딥러닝은 그다지 큰정확도의 실수가 별 필요없어서 16비트 정밀도를 해도 출력에 별 영향을 주지않음(강건성)
		
		
4.딥러닝의 활용
	뭐 알고있는건 다스킵하고(객체탐지 뭐이런게잇다그런거)
	
	마지막에 dense층 대신에 map해서 dense대신 합성곱으로 추론을 하는 방식도 있음(FCN full conv network)
	이러면 dense가 안들어가니까 이미지의 입력이 자유로워짐









		
		
		
		