1.파이썬기초
    넘어감
 


 
2.퍼셉트론
	퍼셉트론은 다수의 신호를 받아서 하나의 신호를 출력(그 머시기회로시간에 했던 and게이트 xor게이트 그거)
	신호값(뉴런값)은 안바뀌니까 거기에 가중치가 곱해져서 값을 결정하고,그 값이 임계값을 넘었을때 1,0을 출력
	
	여기서 xor같은경우 퍼셉트론 하나로는 표현할수없고,두개이상으로 구성되어야함
	그래서 다중퍼셉트론임(or nand and사용)




	
3.신경망
	신경망에서 편향은,뉴런들사이에 상수항의 뉴런이 하나 있다고 보면됨
	활성화함수는 입력신호의 총합이 활성화를 일으키는지를 정하는 역할
	
1.활성화함수
	활성화함수는 입계값을 기준으로 출력이 바뀌는데,이런함수를 계단함수라고 함
	그런데 시그모이드같이 계단형태가 아닌 함수들도 있음
	
	시그모이드는 입력을 주면 0~1사이의 출력을 돌려주는 함수임
	사실상 퍼셉트론과 신경망의 차이는 활성화함수가 거의 다임
	
	그래서 퍼셉트론에선 0과 1의 값을 돌려줬지만,시그모이드를 사용한 신경망에선 연속적인 실수(0~1사이의)를 돌려줌
	
	그리고 또 다른 건 계단함수와 시그모이드 둘다 비선형 함수임
	신경망에선 활성화 함수로 비선형 함수를사용해야함,선형함수를 사용하면 안됨
	
	왜냐면 출력값에서 선형함수를 써버리면,신경망을 쓰는 이유가 없어짐(결국 표현을 선형으로밖에 못하기때문에)
	
	1.relu
		relu는 입력이 0을 넘으면 그 입력을 그대로 출력하고,0 이하면 0을 출력하는 함수임
		그냥 max(0,x)라고 보면됨
		
2.출력층
	회귀는 항등함수(값그대로 내보냄)
	이진분류는 시그모이드
	다중분류는 소프트맥스를 씀
	
	소프트맥스는 마지막 각 뉴런마다 전체뉴런 총합을 1로한 값을 나눠줌
	근데 그냥소프트맥스를 쓰면 오버플로 날확률이 높아서 입력신호의 최댓값을 분모분자에 둘다 곱해서 값을 낮춰서 사용함
	사실 근데 가장 큰 값 잡으면 돼서(소프트맥스는 값의 대소관계가 바뀌진않으니까)없어도 보기불편해서그렇지 추론에는 별문제없는듯
	훈련할때는 소프트맥스 쓰고 추론할땐 소프트맥스 제거하고 사용하는듯
		
		
		
		
		
4.신경망학습

1.손실함수
	신경망에서 그레이디언트 추정할때 사용하는 함수,이걸 최소로 하게 방향을 잡고 움직임
	mse 크로스엔트로피
	
	그리고 손실을 계산할때 보통은 미니배치를 전부 더해서 거기서 계산하고(전부더하고 배치갯수로 나눔,즉 평균) 경사하강함
	
	손실함수를 쓰는 이유는 정확도같은건 계단식이라서 미분하기 적절하지않고,값이 크게변하기때문,
	즉 경사하강법처럼 조금조금 가는거엔 잘 영향을 주지못해서 학습이 잘 안됨
	
2.수치미분
	컴퓨터의 한계상 진짜 미분과 다르게 오차가 포함된 미분을 하는데 이걸 수치미분(근사로구한 접선)이라고함
	
	여기서 딥러닝에선 각 항에 편미분(정해진거하나빼고 전부상수취급)을 해서 각 항마다의 값을 배열로 만든후 거기에 현재값을 대입해서 그걸가지고 경사하강법을 함
	
	즉 이렇게 각 항마다의 값을 배열로 만들걸 그레이디언트라고 함
	
3.기울기
	그레이디언트가 최저가 되는 지점을(최소한 현재위치에서 값을 제일 크게줄이는쪽을) 찾으려면 기울기에 -를 붙인거만큼 이동하면 됨
	그냥 이동하면 너무 크게움직이니까,기울기에 학습률을 곱한거만큼 그방향으로 움직이고 다음스텝을 진행함,이게 경사하강법임
	
	즉 그레이디언트(편미분의 배열)을 각 항마다 학습률x배열의 그 항 만큼을 빼주는식으로 이동하고,이동한데서 또 반복
	
	즉 초기값을 랜덤으로 초기화하고,거기서 경사하강법으로 기울기가 0이되는 전역최저점을 향해 가는것
	
	
	1.신경망에서의 기울기
		신경망에서는 w를 각 항으로 보고 모든w에 대해 기울기를 구해서 경사하강법을 진행함
		즉 모든w에 편미분하고 현재값을 대입해서 각 w마다 조금씩 변경시킴
	
	
	
	
	
	
5.오차역전파	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		
		
		
		
		
		
		
		