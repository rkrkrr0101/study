1.파이썬기초
    넘어감
 


 
2.퍼셉트론
	퍼셉트론은 다수의 신호를 받아서 하나의 신호를 출력(그 머시기회로시간에 했던 and게이트 xor게이트 그거)
	신호값(뉴런값)은 안바뀌니까 거기에 가중치가 곱해져서 값을 결정하고,그 값이 임계값을 넘었을때 1,0을 출력
	
	여기서 xor같은경우 퍼셉트론 하나로는 표현할수없고,두개이상으로 구성되어야함
	그래서 다중퍼셉트론임(or nand and사용)




	
3.신경망
	신경망에서 편향은,뉴런들사이에 상수항의 뉴런이 하나 있다고 보면됨
	활성화함수는 입력신호의 총합이 활성화를 일으키는지를 정하는 역할
	
1.활성화함수
	활성화함수는 입계값을 기준으로 출력이 바뀌는데,이런함수를 계단함수라고 함
	그런데 시그모이드같이 계단형태가 아닌 함수들도 있음
	
	시그모이드는 입력을 주면 0~1사이의 출력을 돌려주는 함수임
	사실상 퍼셉트론과 신경망의 차이는 활성화함수가 거의 다임
	
	그래서 퍼셉트론에선 0과 1의 값을 돌려줬지만,시그모이드를 사용한 신경망에선 연속적인 실수(0~1사이의)를 돌려줌
	
	그리고 또 다른 건 계단함수와 시그모이드 둘다 비선형 함수임
	신경망에선 활성화 함수로 비선형 함수를사용해야함,선형함수를 사용하면 안됨
	
	왜냐면 출력값에서 선형함수를 써버리면,신경망을 쓰는 이유가 없어짐(결국 표현을 선형으로밖에 못하기때문에)
	
	1.relu
		relu는 입력이 0을 넘으면 그 입력을 그대로 출력하고,0 이하면 0을 출력하는 함수임
		그냥 max(0,x)라고 보면됨
		
2.출력층
	회귀는 항등함수(값그대로 내보냄)
	이진분류는 시그모이드
	다중분류는 소프트맥스를 씀
	
	소프트맥스는 마지막 각 뉴런마다 전체뉴런 총합을 1로한 값을 나눠줌
	근데 그냥소프트맥스를 쓰면 오버플로 날확률이 높아서 입력신호의 최댓값을 분모분자에 둘다 곱해서 값을 낮춰서 사용함
	사실 근데 가장 큰 값 잡으면 돼서(소프트맥스는 값의 대소관계가 바뀌진않으니까)없어도 보기불편해서그렇지 추론에는 별문제없는듯
	훈련할때는 소프트맥스 쓰고 추론할땐 소프트맥스 제거하고 사용하는듯
		
		
		
		
		
4.신경망학습

1.손실함수
	신경망에서 그레이디언트 추정할때 사용하는 함수,이걸 최소로 하게 방향을 잡고 움직임
	mse 크로스엔트로피
	
	그리고 손실을 계산할때 보통은 미니배치를 전부 더해서 거기서 계산하고(전부더하고 배치갯수로 나눔,즉 평균) 경사하강함
	
	손실함수를 쓰는 이유는 정확도같은건 계단식이라서 미분하기 적절하지않고,값이 크게변하기때문,
	즉 경사하강법처럼 조금조금 가는거엔 잘 영향을 주지못해서 학습이 잘 안됨
	
2.수치미분
	컴퓨터의 한계상 진짜 미분과 다르게 오차가 포함된 미분을 하는데 이걸 수치미분(근사로구한 접선)이라고함
	
	여기서 딥러닝에선 각 항에 편미분(정해진거하나빼고 전부상수취급)을 해서 각 항마다의 값을 배열로 만든후 거기에 현재값을 대입해서 그걸가지고 경사하강법을 함
	
	즉 이렇게 각 항마다의 값을 배열로 만들걸 그레이디언트라고 함
	
3.기울기
	그레이디언트가 최저가 되는 지점을(최소한 현재위치에서 값을 제일 크게줄이는쪽을) 찾으려면 기울기에 -를 붙인거만큼 이동하면 됨
	그냥 이동하면 너무 크게움직이니까,기울기에 학습률을 곱한거만큼 그방향으로 움직이고 다음스텝을 진행함,이게 경사하강법임
	
	즉 그레이디언트(편미분의 배열)을 각 항마다 학습률x배열의 그 항 만큼을 빼주는식으로 이동하고,이동한데서 또 반복
	
	즉 초기값을 랜덤으로 초기화하고,거기서 경사하강법으로 기울기가 0이되는 전역최저점을 향해 가는것
	
	
	1.신경망에서의 기울기
		신경망에서는 w를 각 항으로 보고 모든w에 대해 기울기를 구해서 경사하강법을 진행함
		즉 모든w에 편미분하고 현재값을 대입해서 각 w마다 조금씩 변경시킴
	
	
	
	
	
	
5.오차역전파	
	계산그래프는 국소적계산(값과 노드1개)를 반복해서 최종결과를 얻는 식임
	즉 현재처리하는거말곤 다 무시해도됨
	
	그리고 연쇄법칙에 따라 시작값미분값/현재미분값으로 식이 단순화됨(위아래 서로약분하면서 사라짐)
	
	
1.각 노드별 역전파
	1.덧셈노드의 역전파
		덧셈노드는 그냥 그대로 흘려보냄(전값을 그대로 흘려보냄)
		전값이 3이면 양쪽노드에 전부 3을 흘려보냄
		
	2.곱셈노드의 역전파
		곱셈노드의 역전파는 양쪽을 거꾸로해서 전값을 곱함
			x*y=z일경우
			yz xz로 거꾸로 들고감
			
	즉 이렇게해서 각 시작점에서 값이 바뀌면 얼만큼 민감하게 반응하는지를 알수있게되고,마지막 오차에 얼마나 기여했는지도 알수있게됨
	
2.활성화함수 계층
	1.relu
		relu는 x가 0보다 클때 그대로,0이하일때 0출력
		
		얘는 순전파때 x가 0보다 크면 그대로 흘리고,0보다 작으면 0을보냄
		역전파때는 순전파 값을 기억했다가 0보다 크면 덧셈처럼 그대로 흘리고,0보다작으면 0을보냄(앞에값에 0을곱해서보냄)
		
	2.시그모이드
		시그모이드는 * exp + /순으로 연결된 층의 묶음이라고 생각하면됨
		exp층은 exp(x)를 수행하고 /는 y=1/x를 수행함
		
		+랑 *는 그대로 하면되고
		/는 역전파할때 -y^2(순전파값을 제곱하고 -붙임)을 전값이랑 곱해서 흘려보냄
		
		exp층은 전값에 순전파때의 출력(exp(x))을 곱해서 흘려보냄(만약 순전파값이 -x^2이었으면 exp(-x^2))
		
		이걸 전부 묶으면
			전값* 순전파의 상류값^2*exp(-순전파의 하류값)
		이 됨 
		이걸 또 정리하면
			전값*상류값(1-상류값)
		이 됨
		
		즉 시그모이드의 역전파는 순전파의 출력(상류값)으로만 계산이 가능함

3.affine/softmax층 구현
	1.affine
		기본적으로 뉴런의 가중치합은 x 점곱 w + 편향 임
		여기서 w는 (x의갯수,y의갯수)만큼을 가지고있음
		여기서 행렬곱을 어파인변환이라고 하고,이걸하는걸 어파인층이라고 함(즉,dense임)
		
		행렬도 계산그래프 곱연산과 같은데 행렬이다보니 좀 다른게 있긴함
		역전파시에 전값*wT,전값*xT(전치행렬,(i,j)를 (j,i)로 바꾼거)를 해서 보내면됨
		즉 가중치와 입력값을 거꾸로보내는건 같은데,거꾸로 보낼때 전치행렬을 곱해서 보내야함
		
		덧셈은 똑같이 그대로 보내면됨
		
		여기서 배치를 묶어서 역전파를 할때도 똑같이 (n,x)로 하면됨
		
		그리고 편향을 더할때도 x와w에 두 데이터에 대한 미분을 데이터마다 더해서 구함(같은칸([0][1],[1][1],[2][1])의 모든합을 가지고)
	2.softmax with loss
		소프트맥스는 어짜피 출력층에서밖에 안쓰니까 로스도 묶어서 계산함
		그리고 학습할때 말고는 소프트맥스를 넣을 이유가 없음,값이 크기가 바뀌는거지 순서가 바뀌지않음
		그냥 큰거 고르면됨
		
		소프트맥스의 역전파는,y-t로 나옴,즉 순전파때의 값 두개를 뺀값
		즉 여기서 예측과 실제의 차이만큼 값을 역전파해 나가는거임
	
		
	
	
	
6.학습관련기술들	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		
		
		
		
		
		
		
		