1.신경망복습
	스킵
-2.자연어와 단어의 분산표현
1.시소러스
	시소러스는 유의어사전으로,비슷한 단어들을 묶어서(동의어와 유의어를)서로간의 관계를 알수있게,사람이 수동으로 만든 사전임
	유명한거로 wordnet이 있음,근데 이건 수동으로 레이블링해야하기때문에 너무 비싸고,시대변화에 따라가지못함(단어에 뜻이 추가되는경우)
	
2.통계기반방법
	그래서 말뭉치를 이용해서,통계기반으로 하는 방식이 나왔음
	그 정보검색때 하던 그거임
	
	기본적으로 말뭉치를 사용하려면 전처리를 해야함,보통 lower쓰고 .같은거 지우고 스페이스단위로 떼버리면 됨
	그리고 각 단어를 원핫인코딩으로 바꾸면 일단 전처리끝임
	
	그리고 이 단어들의 주변 n범위 내에(배열의 근처에) 있는 애들을 세서,백터화 함으로써 비슷한 애들을 서로 묶을수있음
	이떄 단순히 갯수만세면,많이나오는애가 유리하니까 tf-idf식으로 하는게 좋음
	
	그리고 각 단어들끼리 유사도를 보고싶으면,이 배열의 코사인유사도를 서로 계산하면 됨(완전히같으면 1,완전히반대면 -1)
	
	
3.통계기반방법 개선
	이 통계기반방법에는 문제점이 있는데,위에서 말한,tf-idf적용문제와 희소행렬문제임
	이 희소행렬 문제를 해결하기위해,차원감소를 사용할수있음(그 머신러닝에서 본 차원축소임,가장 확산 큰 축을 찾고 그거기준으로 짜부시키는거)
	이떄 svd(특이값분해)를 사용함,이건 행렬을 세 행렬의 곱으로 분해하는 방식임
	
	
	
-3.word2vec
1.추론기반기법과 신경망
	워드투벡은 추론기반의 기법임
	기본적으로 통계기반의경우엔,말뭉치가 커지면 연산이 기하급수적으로 늘어나버림(배치학습)
	그런데 추론기반은 데이터의 일부만 사용해서(미니배치) 순차적으로 학습할수 있음
	즉 병렬처리도 가능해지고,순차처리도 됨
	
	기본적인 방법은,통계기반이 특정단어 주변에 있는걸 기록했다면,추론기반은 주변단어를 사용해 그 단어를 예측하는식으로 동작함(지도학습으로)
	이떄 단어를 원핫벡터로 변경하고(단어그대로 먹일수없으니)
	완전연결신경망에 넣어서 추론값을 뽑아낼수있음
	
2.단순한 word2vec
	가장 단순한방법은,양쪽의 단어를 주고,사이의 단어를 답으로 하는 신경망을 굴리는것
	이때 입력층을 두개로 해서 구성할수있음(이경우엔 양쪽을 평균내면됨)
	그리고 출력층에서 소프트맥스로 받아보면됨
	이경우 모델이(입력dense2개,더하고/2하는 단순계산층,은닉층,출력층-소프트맥스-로스층)으로 구성되는데,
	이때 입력층의 가중치만 가져다가 쓰면됨(출력층은 버림)
	
	이떄 학습이 끝난 입력층의 각 단어에 대한 분산표현을 가지고,벡터로 사용하면됨
	
	
-4.word2vec 속도개선
1.임베딩
	단순한 word2vec의 문제는,말뭉치에서의 단어갯수가 많아졌을때 필요한 엄청난 수의 뉴런(단어100만개면 입력에 100만개필요함)임
	이걸 해결하기위해 원핫벡터를 사용하지않고 임베딩을 사용할수있음
	즉 입력dense를 embedding로 바꿔서,원핫이 아닌 벡터로 취급해서 처리할수있음
2.네거티브샘플링
	두번째문제는,단어가 매우많을때의 소프트맥스층의 계산임,다중분류의경우 각 단어 하나마다 뉴런이 하나씩 있어야해서,계산이 매우느려짐
	그래서 사용하는게 네거티브샘플링이고,이건 다중분류를 이진분류로 근사하는것임
	단 이건,목표가 하나일때만 사용할수있음
	
	여기서 문제는,오답일때의 처리를 학습할때,단어의 분포에 따라서 많이 나오는 단어를 샘플링 하는게 좋고,이걸 고를수있는 방법이 네거티브 샘플링임
	즉,많이 나오는 단어와 정답을 섞어서 학습시키는 방식임
	
3.의의
	word2vec은,신경망으로 제대로 학습하기전에,전처리(단어,문장을 벡터화하는)에 사용하기 좋음(고정길이벡터로 변환되니까)
	
	
-5.rnn	
	rnn이 등장하게 된 이유는,단순한 피드포워드신경망에선 맥락을 이해하는게 불가능하기때문임(즉,현재 입력으로 출력만 낼수있고,이전출력을 기억할수없음)
1.rnn이란
	rnn은 순환신경망으로,단순rnn의 경우엔,dense한층을 여러번 반복하면서,입력값을 이전출력값과 현재입력값 두개를 받아서 출력을 하는식으로 구성됨
	즉 for문돌리면서,이전값과 현재값을 합쳐서 다음걸 예측하는식
	그리고 출력은,각 반복횟수마다 하나씩 나오게되고,이걸 다 받아갈지,마지막값만 받아갈진 문제에따라 다름
	
	rnn에서 역전파할땐,마지막출력에서부터 일반적인 역전파를 하면되는데,이렇게하면 너무 큰 시계열데이터에선 메모리 문제가 나타나게됨(다 기억해야하니)
	또한 그레이디언트 소멸도 나타나게됨
	그래서 rnn을 일정주기로 잘라서,그안에서 오차역전파를 수행함(truncated bptt)
	이경우에도,순전파의 연결은 끊을수없고,역전파의 연결만 끊어서 학습해야함
	
	즉,만약 10개단위로 한다고치면,10개를 순전파할때마다 역전파하고,저장된메모리를 초기화하는식
	
-6.게이트가 추가된 rnn	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	