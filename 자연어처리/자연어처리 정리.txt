1.소개
1.타깃인코딩
	타깃인코딩엔 원핫표현,tf-idf표현등이 있음
	원핫은 그 원핫이고
	tf는 문서에 나온수 만큼 문서에서 중요한 단어라고 치고,df는 전체문서에서 계속나오는 단어는 어떤 한 문서의 중요도에서 높지 않다고 보는것(정보검색때 tfidf맞음)
	tf* 1/df 같은느낌임(로그치고 하는거빼고 러프하게보면)

	1.타깃인코딩
		기본적으로 nlp에서는 레이블 수가 작을땐 레이블인코딩(각 단어나 캐릭터를 1,2,3,4로 구별짓는거)를 사용하고,
		좀 커지면 임베딩같은걸 사용
		수치타깃이면(문장을보고 나이예측이라던지) 범주형으로 바꾸고 순서있는분류문제로 다룰수도있음 
		이경우엔 타깃인코딩이 성능에 매우 큰 영향을 주니 잘 봐야함
		
2.파이토치 기초
		torch.Tensor(x차원,y차원)
	하면 차원만큼 랜덤초기화된 값이 나옴
		torch.rand(x,y)
	하면 0,1범위 랜덤초기화값
		torch.randn(x,y)
	하면 평균0 분산1인 정규분포
	
	
		torch.ones(x,y)
	는 1로채우기
		torch.zeros(x,y)
	는 0으로채우기
		텐서.fill_(숫자)
	는 숫자로 텐서채우기 (메서드 마지막에 _있는건  텐서값을 바꾸는 연산을 의미함,즉 새로운텐서를 안만들고 현재값을 변경)
		텐서.normal_()
	은 정규분표변경
		텐서.uniform_()
	은 균등분포로변경
	
	
	파이썬 리스트를 텐서로 바꾸는건
		x=torch.Tensor(리스트)
	
	넘파이를 텐서로 바꾸는건
		torch.from_numpy(npyarray)
		
		
	2.텐서 타입과 크기
		생성자를 사용할때
			torch.floatTensor(리스트)
		로 타입지정해서 생성할수있고
			x.long,x.float
		등으로 타입캐스팅할수있음
		
			x.shape,x.size
		로 텐서차원 확인할수도있음
		
	3.텐서연산
		텐서를 만들고 +-*/등으로 연산할수있음
		
		같은크기의 텐서끼리 하면 그냥 더해지고,스칼라값이면 전체에 브로드캐스팅됨
		
			torch.arange(숫자)
		는 숫자만큼 1차원텐서생성
			x.view(2,3)
		은 2,3만큼을 잘라내서 뷰생성(뷰니까 저거건드리면 원본도바뀜)
			torch.sum(x,dim=0)
		0차원으로 sum
			torch.transpose(x,0,1)
		x를 T
	4.인덱싱 슬라이싱 연결
		인덱싱 슬라이싱도 파이썬리스트랑 똑같이 할수있음
		그리고 뛰엄뛰엄있는거 선택할떈
			torch.index_select(x,dim=선택차원,index=인덱스리스트)
		하면 선택한 차원에서 인덱스 텐서에 있는 인덱스만 뽑아서 리턴함(torch.longTensor([0,2]))
			012
			345
		1차원에서 0,2를 뽑으면 
			0 2
			3 5
		0차원에서 0,0를 뽑으면
			012
			012
		인덱스는 롱텐서여야 함,필수조건임
		
		이거외에도 행렬곱,역행렬,대각합등이 있음
		
	5.텐서와 계산그래프
		텐서 생성시에 requires_grad=True 하면 그레이디언트 기반학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가연산을 활성화함
		
		그러면 파이토치는 그레이디언트 계산에 사용하는 부가정보를 관리
		파이토치가 정방향 계산의 값을 기록
		계산이 끝나면 스칼라값 하나를 사용해 역방향계산을 수행
		역방향계산은 텐서에서 backward메서드를 호출해 시작,역방향계산은 정방향계산에 참여한 텐서객체에 대한 그레이디언트값을 계산함
		그래프에서 기울기는 모델의 파라미터마다 존재하고,오류신호에 대한 파라미터의 기여로 생각할수있음
		.grad로 참조할수있음
		
	6.cuda사용
			torch.cuda.is_available()로 gpu사용확인하고
			device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')로 사용디바이스넣은다음
			
			x=torch.rand(3,3).to(device)
		이런식으로 사용
		
		그리고 쿠다객체와 쿠다가 아닌객체를 다루려면 두 객체가 같은데 있어야하니까,마지막에 계산할땐 최종결과를 cpu로 보내야함(y.to(cpudevice))
		
		
		
2.nlp기술 빠르게 훑어보기
1.말뭉치,토큰,타입
	말뭉치(corpus)는 데이터셋임
	말뭉치는 원시텍스트(일반적인 텍스트)와 메타데이터를 포함함
	원시 텍스트는 문자 시퀸스지만,일반적으로 문자를 토큰이라는 단위(스페이스바같은)걸로 묶어서 사용함
	메타데이터는 샘플 또는 데이터포인트라고 부름
	
	토큰화는 텍스트를 토큰으로 나누는 과정인데,
	보통 토큰화를 공백만 가지고 하면 부족하고 좀 더 빡세게 바꿔야하는데 라이브러리있으니까 그거쓰던가,아니면 임베딩쓰자
	
	타입은 말뭉치에 등장하는 고유한 토큰,말뭉치에 있는 모든 타입의 집합이 어휘사전,또는 어휘임
	단어는 내용어와 불용어로 구분됨,불용어는 대부분 내용어를 보충하는 문법적 용도로 사용됨
2.유니그램,바이그램,트라이그램,n그램
	그 정보검색때 했던 한글자씩 떼는거
	정보검색이면
	정보 보검 검색 이런식으로 나누는거
	
	영어권에선  단어단위로 자르나봄
	
3.표제어와 어간
	표제어는 단어의 기본형
	fly가 표제어면 flow flew flies 등등이 어미가 바뀌면서 변형된 단어
	
	토큰을 표제어로 바꾸면 벡터표현의 차원이 줄어들어서 도움이 됨
	이런걸 표제어 추출이라고 함
	
	
4.단어분류하기:품사태깅
	문서에 레이블을 할당하는 개념을 단어나 토큰으로 확장할수있음
	단어 분류의 예로는 품사태깅이 있음(단어마다 이게 무슨품사인지를 태깅하는것)
	
5.청크나누기와 개체명인식
	종종 여러 토큰으로 구분되는 텍스트구에 레이블을 할당해야함
	이럴땐 연속된 토큰끼리의 부분영역이 있음
	이걸 나누는걸 청크나누기,혹은 부분구문분석이라고 함
	이거도 라이브러리로 하면됨
	
6.문장구조
	구 단위를 식별하는 부분구문분석과 달리 구 사이의 관계를 파악하는 작업을 구문분석(파싱)이라고 함
	문장안의 문법 구조가 계층적으로 어떻게 관련되는지를 보여주는 트리를 구분분석트리라고 함
	
7.단어의미와 의미론
	단어에는 의미가 하나 이상 있음
	이걸 쓰고싶을떈 영어권은 wordnet
	
	단어의미는 문맥으로 결정될수도있음
	텍스트에서 단어 의미를 자동으로 찾는 일은 실제로 nlp에 적용된 첫 준지도학습임
	
	


3.신경망의 기본구성요소
	그냥 기본임 활성화함수 손실함수 그런거
	파이토치로 뭐 만들일생기면 좀 자세히보자,그거아니면 그냥 다 알던내용임
	
	
1.퍼셉트론
	퍼셉트론은 그 입력출력하나있는 신경망 ,dense라보면됨
	파이토치에선 torch.nn.Linear(입력크기,출력갯수)
	
2.활성화함수
	시그모이드는 1이나 0 사이에서 값이 나오는 함수,매우 쉽게 1이나 0이되기때문에(중간값보다)이진분류에 적합함(중간엔 안쓰고 출력층에만 사용함)
	
	하이퍼볼릭 탄젠트는 시그모이드함수의 변종,얘는 -1~1범위로 바꿈
	
	relu(렐루)는 0이하는 0으로 1이상은 그대로 두는 활성화함수,얘는 보통 그대로 쓰긴하는데,
	좀 커지면 뉴런이 사망하는경우가있어서 리키렐루,prelu같은거 사용도 함(신경망이큰경우)
	
	소프트맥스는 다중분류(예측클래스가 10개라든가)에 사용하는 활성화함수,이거도 출력층에서만 쓰고,소프트맥스의 총합은 1이됨
	얘는 카테고리컬 크로스엔트로피랑 같이사용함 보통
	
3.손실함수
	mse는 회귀에 사용하는 손실함수,rmse는 루트씌운거
	출력과 레이블을 제곱해서 빼는느낌이었음
	알아보기힘들다고 r2쓰는경우도 꽤 있는듯(얘는 최대치100 최소치0인가 그랬음)
	
	카테고리컬 크로스엔트로피는 다중분류에 사용하는 손실함수
	얘는 레이블이 1에가깝고 나머지는 0에 가깝다고 생각하고 거기서 벗어나면 때림
	
	바이너리 크로스엔트로피는 이진분류에 사용하는 손실함수
	얘는 답이 1이고 아니면 0이라고 생각하고 거기서 벗어나면 때림
	
4.지도학습훈련 알아보기
	옵티마이저는 보통 adam쓰고 학습률기본은 0.001임
	
5.부가적인 훈련개념
	평가지표는 손실함수랑 비슷한데 좀 제약이 약한거,보통 자주쓰는건 정확도(accuracy)
	
	모델을 올바르게 측정하려면 test val train으로 나눠야한다 어쩌구저쩌구
	
	모델에포크 수 정하기 힘들면 조기종료를 쓰자
	
	하이퍼파라미터수를 좀 조정해서 최적의값을 찾아보자
	
	규제는 오버핏 좀 막아내는것,보통 드롭아웃같은거 사용
	l2규체는 그래프를 좀 부드럽게만들고(뾰족뾰족해서 오버핏나는게아니라)l1은 좀 더 희소한 솔루션을 만들게됨(대부분 파라미터값이 0이됨)
	
6.레스토랑리뷰 감성분류
	데이터를 토큰화하고 그걸 정수로 매핑(레이블인코딩)그리고 문자열을 수치벡터로 변환하고 그걸 미니배치로 모음
	
	그리고나서 모델을 만들고 훈련을 시키면됨(이진분류니까 바이너리 크로스엔트로피에 시그모이드)
	
	
	
	
	
4.자연어처리를 위한 피드포워드 신경망	
	그냥 완전연결신경망 몇개 보는거
	그 퍼셉트론이 두개이상이면 선 여러개로 제대로나눌수있다 그런거(xor문제같은거)
	즉 1퍼셉트론이면 x>5 이런식의 표현밖에 안되는데 두개이상이면 3<x<5 이런식의 표현이 된다는거
	
	그리고 클래스가 불균형하면 좀 맞춰주는게 좋고(많은거 제거하던가 그렇게)
	
	그리고 합성곱신경망(conv1d)도 있음
	필터크기랑 스트라이드랑 패딩이랑.. 그런거설명
	
	다일레이션은 필터가 어케 들어생겼는지를 정함
	다일레이션을 2로하고 필터가 2,2면 가운데 +모양이 있고 모서리를 차지하는식으로 스캔을 함
	이건 파라미터 갯수를 안늘리고 넓은 공간을 요약하는데 유용함
	
	그리고 합성곱+완전연결층쓸땐 합성곱 마지막을 1로 맞추거나,flatten하거나,gap하거나 3가지방법이있음
	
	그리고 데이터크기는 인풋데이터중에서 제일 큰걸 찾아서 그걸기준으로 맞추는게좋음
	
	그리고 1x1연결은 커널사이즈가 그대로인걸 써서 채널수를 조절하는 트릭
	
	잔차연결은 그 전반부값이랑 후반부값을 연결해서(크기가같은) 둘을 더해서 사용하는방식,이러면 사라진정보(위치값같은거)도 어느정도 보충할수있음
	
	
	
5.단어와 타입임베딩	

	임베딩은 단어를 여러차원의 벡터로 바꾸는것(good를 100차원의 벡터로 바꾸는것)
	그래서 king+woman=queen이런느낌이 되게 함
		 queen-king하면 그 차이벡터가 woman인 느낌
	두가지 단어를 더하고 빼고해서 그위치에서 가장 가까운값을 찾으면 그게 목표물이 되는식
	그래서
	cat:kitten:dog 3개를 주면 cat-kitten한 벡터를 dog에 더해서 puppy를 뽑아낼수있음
	
	이렇게하면 단어집합도 데이터량에 따라 학습을 시킬수 있음(tfidf같은 카운터형 표현은 학습이안되니까)
	그리고 차원이 덜들어가고,차원수를 조정할수있음
	
	그리고 임베딩은 개인이 훈련시키긴 너무빡세니까(최소 테라단위 텍스트여야함)있는거 블랙박스처럼 쓰자
	훈련시키는건 같은 윈도우내의 값을 예측하는식으로 훈련시킴
	
	기본적인 전처리는 텍스트 소문자변환후 쉼표,마침표,느낌표,물음표 주위에 공백을 추가하고 그외의 기호는 제거하는식으로 전처리
	그리고 학습검증테스트로 나누고 클래스 레이블별로 포인트집계후 분포 동일하게 유지할수있게 할당
	그리고 단어를 정수로 매핑하고 문장을 경계토큰으로 감싸고 벡터크기 같게만들기위해 남은자리를 0으로 패딩
	
	
	
6.자연어처리를 위한 시퀀스 모델링-초급
	기본적인 rnn소개임 건너뛰어도됨
	rnn은 입력과 전타임스텝출력값의 합이다 뭐 이런거
	
7.자연어처리를 위한 시퀀스 모델링-중급
	이거도  그냥 게이트있는 lstm이나 gru소개
	건너뛰어도됨
	그냥 전입력이 타임스탭 거치다보면 휘발되니까,그걸 새로운입력의 비율을 조절해서 휘발을 막아보겠다 이런거,
	그리고 그 입력조절을 게이트라고 하고 이런 가중치라서 모델이 조정할수있음

8.자연어처리를 위한 시퀀스 모델링-고급	
	s2s모델에서 전통적인 방식대로 rnn사용하면,1대1대응이 아니기때문에(출력벡터와 입력벡터의 길이가 다를수있으니) 결국 출력벡터는 벡터하나일수밖에없음
	그래서 문자열이 길어질수록 과거의 기록들이 사라져서(벡터크기한계때문에) 문제가 있어서 어텐션을 사용함

1.양방향 순환모델
	rnn을 왼쪽에서 오른쪽으로도 돌리고 오른쪽에서 왼쪽으로도 돌리는 rnn2개를 만들어서 그 두개를 합침
	이러면 단방향보다 훨씬 긴 길이의 입력을 처리할수 있게 됨(그래도 너무길면 가운데있는게 처리안되긴함,예전건 왼쪽에있는건 싸그리날아갔는데 그거보단 낫다는거지)
	
2.어텐션
	기본 rnn모델은 전체문장을 하나의 백터에 밀어넣고 그걸 디코더로 분석하는식이기 때문에,하나의 벡터에 밀어넣는다는 문제가 있음
	그리고 긴 문장은 시간을 거슬러 역전파할때 기울기가 소실되어 훈련이 어려움
	
	그래서 어텐션을 사용함
	
	어텐션은 현재 값을(쿼리) 모든 키-밸류조합에서 유사도를 계산한뒤에 그 유사도만큼의 밸류를(30x0.3이런느낌,유사도30퍼일때)전부 더하는식임
	즉 임베딩에서 킹+여자하면 퀸이 나온다는거랑 비슷한느낌으로,유사도만큼을 전부 더하면 괜찮은 추론값이 나올거라는 소리(유사도는 총합 1)
	
	즉 쿼리와 키를 입력으로 받아서 값벡터를 선택하는 일련의 가중치들을 계산
	또한 디코더의 은닉상태를 쿼리백터로 사용하고 인코더상태벡터를 키와 값벡터로 사용함
	디코더의 은닉상태와 인코더상태벡터의 점곱은 인코딩된 시퀸스에 있는 아이템마다 스칼라값 하나를 만듬
	이걸 소프트맥스로 확률분포로 변경한뒤(이게 가중치)전부 더해서 배치아이템마다 벡터 하나를 만듬(유사도의 총합)
	
	여기서 전역의 모든값을 사용하면 전역어텐션,현재타임스탭의(현재쿼리의) 주변값만 사용하면 지역어텐션
	
	그리고 이런 어텐션을 여러개 사용하면 멀티헤드어텐션임
	그리고 입력의 어떤 영역이 다른 영역에 영향을 미치는지 학습하는게 셀프어텐션
	멀티헤드어텐션을 통해 셀프어텐션이 대중화됐음 
	
	그리고 이미지와 음성처럼 입력의 형태가 다양하면 멀티모달 어텐션을 사용할수있음
	
3.시퀸스모델 평가
	시퀸스모델의 평가에는 n그램 중복기반 지표를 주로 사용함
	참조와 출력이 얼마나 가까운지를 n그램중복통계로 점수를 계산함(대표적으로 bleu)
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		