1.소개
1.타깃인코딩
	타깃인코딩엔 원핫표현,tf-idf표현등이 있음
	원핫은 그 원핫이고
	tf는 문서에 나온수 만큼 문서에서 중요한 단어라고 치고,df는 전체문서에서 계속나오는 단어는 어떤 한 문서의 중요도에서 높지 않다고 보는것(정보검색때 tfidf맞음)
	tf* 1/df 같은느낌임(로그치고 하는거빼고 러프하게보면)

	1.타깃인코딩
		기본적으로 nlp에서는 레이블 수가 작을땐 레이블인코딩(각 단어나 캐릭터를 1,2,3,4로 구별짓는거)를 사용하고,
		좀 커지면 임베딩같은걸 사용
		수치타깃이면(문장을보고 나이예측이라던지) 범주형으로 바꾸고 순서있는분류문제로 다룰수도있음 
		이경우엔 타깃인코딩이 성능에 매우 큰 영향을 주니 잘 봐야함
		
2.파이토치 기초
		torch.Tensor(x차원,y차원)
	하면 차원만큼 랜덤초기화된 값이 나옴
		torch.rand(x,y)
	하면 0,1범위 랜덤초기화값
		torch.randn(x,y)
	하면 평균0 분산1인 정규분포
	
	
		torch.ones(x,y)
	는 1로채우기
		torch.zeros(x,y)
	는 0으로채우기
		텐서.fill_(숫자)
	는 숫자로 텐서채우기 (메서드 마지막에 _있는건  텐서값을 바꾸는 연산을 의미함,즉 새로운텐서를 안만들고 현재값을 변경)
		텐서.normal_()
	은 정규분표변경
		텐서.uniform_()
	은 균등분포로변경
	
	
	파이썬 리스트를 텐서로 바꾸는건
		x=torch.Tensor(리스트)
	
	넘파이를 텐서로 바꾸는건
		torch.from_numpy(npyarray)
		
		
	2.텐서 타입과 크기
		생성자를 사용할때
			torch.floatTensor(리스트)
		로 타입지정해서 생성할수있고
			x.long,x.float
		등으로 타입캐스팅할수있음
		
			x.shape,x.size
		로 텐서차원 확인할수도있음
		
	3.텐서연산
		텐서를 만들고 +-*/등으로 연산할수있음
		
		같은크기의 텐서끼리 하면 그냥 더해지고,스칼라값이면 전체에 브로드캐스팅됨
		
			torch.arange(숫자)
		는 숫자만큼 1차원텐서생성
			x.view(2,3)
		은 2,3만큼을 잘라내서 뷰생성(뷰니까 저거건드리면 원본도바뀜)
			torch.sum(x,dim=0)
		0차원으로 sum
			torch.transpose(x,0,1)
		x를 T
	4.인덱싱 슬라이싱 연결
		인덱싱 슬라이싱도 파이썬리스트랑 똑같이 할수있음
		그리고 뛰엄뛰엄있는거 선택할떈
			torch.index_select(x,dim=선택차원,index=인덱스리스트)
		하면 선택한 차원에서 인덱스 텐서에 있는 인덱스만 뽑아서 리턴함(torch.longTensor([0,2]))
			012
			345
		1차원에서 0,2를 뽑으면 
			0 2
			3 5
		0차원에서 0,0를 뽑으면
			012
			012
		인덱스는 롱텐서여야 함,필수조건임
		
		이거외에도 행렬곱,역행렬,대각합등이 있음
		
	5.텐서와 계산그래프
		텐서 생성시에 requires_grad=True 하면 그레이디언트 기반학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가연산을 활성화함
		
		그러면 파이토치는 그레이디언트 계산에 사용하는 부가정보를 관리
		파이토치가 정방향 계산의 값을 기록
		계산이 끝나면 스칼라값 하나를 사용해 역방향계산을 수행
		역방향계산은 텐서에서 backward메서드를 호출해 시작,역방향계산은 정방향계산에 참여한 텐서객체에 대한 그레이디언트값을 계산함
		그래프에서 기울기는 모델의 파라미터마다 존재하고,오류신호에 대한 파라미터의 기여로 생각할수있음
		.grad로 참조할수있음
		
	6.cuda사용
			torch.cuda.is_available()로 gpu사용확인하고
			device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')로 사용디바이스넣은다음
			
			x=torch.rand(3,3).to(device)
		이런식으로 사용
		
		그리고 쿠다객체와 쿠다가 아닌객체를 다루려면 두 객체가 같은데 있어야하니까,마지막에 계산할땐 최종결과를 cpu로 보내야함(y.to(cpudevice))
		
		
		
		
	
	
	
	
	
	
	
		