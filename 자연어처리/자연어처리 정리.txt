1.소개
1.타깃인코딩
	타깃인코딩엔 원핫표현,tf-idf표현등이 있음
	원핫은 그 원핫이고
	tf는 문서에 나온수 만큼 문서에서 중요한 단어라고 치고,df는 전체문서에서 계속나오는 단어는 어떤 한 문서의 중요도에서 높지 않다고 보는것(정보검색때 tfidf맞음)
	tf* 1/df 같은느낌임(로그치고 하는거빼고 러프하게보면)

	1.타깃인코딩
		기본적으로 nlp에서는 레이블 수가 작을땐 레이블인코딩(각 단어나 캐릭터를 1,2,3,4로 구별짓는거)를 사용하고,
		좀 커지면 임베딩같은걸 사용
		수치타깃이면(문장을보고 나이예측이라던지) 범주형으로 바꾸고 순서있는분류문제로 다룰수도있음 
		이경우엔 타깃인코딩이 성능에 매우 큰 영향을 주니 잘 봐야함
		
2.파이토치 기초
		torch.Tensor(x차원,y차원)
	하면 차원만큼 랜덤초기화된 값이 나옴
		torch.rand(x,y)
	하면 0,1범위 랜덤초기화값
		torch.randn(x,y)
	하면 평균0 분산1인 정규분포
	
	
		torch.ones(x,y)
	는 1로채우기
		torch.zeros(x,y)
	는 0으로채우기
		텐서.fill_(숫자)
	는 숫자로 텐서채우기 (메서드 마지막에 _있는건  텐서값을 바꾸는 연산을 의미함,즉 새로운텐서를 안만들고 현재값을 변경)
		텐서.normal_()
	은 정규분표변경
		텐서.uniform_()
	은 균등분포로변경
	
	
	파이썬 리스트를 텐서로 바꾸는건
		x=torch.Tensor(리스트)
	
	넘파이를 텐서로 바꾸는건
		torch.from_numpy(npyarray)
		
		
	2.텐서 타입과 크기
		생성자를 사용할때
			torch.floatTensor(리스트)
		로 타입지정해서 생성할수있고
			x.long,x.float
		등으로 타입캐스팅할수있음
		
			x.shape,x.size
		로 텐서차원 확인할수도있음
		
	3.텐서연산
		텐서를 만들고 +-*/등으로 연산할수있음
		
		같은크기의 텐서끼리 하면 그냥 더해지고,스칼라값이면 전체에 브로드캐스팅됨
		
			torch.arange(숫자)
		는 숫자만큼 1차원텐서생성
			x.view(2,3)
		은 2,3만큼을 잘라내서 뷰생성(뷰니까 저거건드리면 원본도바뀜)
			torch.sum(x,dim=0)
		0차원으로 sum
			torch.transpose(x,0,1)
		x를 T
	4.인덱싱 슬라이싱 연결
		인덱싱 슬라이싱도 파이썬리스트랑 똑같이 할수있음
		그리고 뛰엄뛰엄있는거 선택할떈
			torch.index_select(x,dim=선택차원,index=인덱스리스트)
		하면 선택한 차원에서 인덱스 텐서에 있는 인덱스만 뽑아서 리턴함(torch.longTensor([0,2]))
			012
			345
		1차원에서 0,2를 뽑으면 
			0 2
			3 5
		0차원에서 0,0를 뽑으면
			012
			012
		인덱스는 롱텐서여야 함,필수조건임
		
		이거외에도 행렬곱,역행렬,대각합등이 있음
		
	5.텐서와 계산그래프
		텐서 생성시에 requires_grad=True 하면 그레이디언트 기반학습에 필요한 손실함수와 텐서의 그레이디언트를 기록하는 부가연산을 활성화함
		
		그러면 파이토치는 그레이디언트 계산에 사용하는 부가정보를 관리
		파이토치가 정방향 계산의 값을 기록
		계산이 끝나면 스칼라값 하나를 사용해 역방향계산을 수행
		역방향계산은 텐서에서 backward메서드를 호출해 시작,역방향계산은 정방향계산에 참여한 텐서객체에 대한 그레이디언트값을 계산함
		그래프에서 기울기는 모델의 파라미터마다 존재하고,오류신호에 대한 파라미터의 기여로 생각할수있음
		.grad로 참조할수있음
		
	6.cuda사용
			torch.cuda.is_available()로 gpu사용확인하고
			device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')로 사용디바이스넣은다음
			
			x=torch.rand(3,3).to(device)
		이런식으로 사용
		
		그리고 쿠다객체와 쿠다가 아닌객체를 다루려면 두 객체가 같은데 있어야하니까,마지막에 계산할땐 최종결과를 cpu로 보내야함(y.to(cpudevice))
		
		
		
2.nlp기술 빠르게 훑어보기
1.말뭉치,토큰,타입
	말뭉치(corpus)는 데이터셋임
	말뭉치는 원시텍스트(일반적인 텍스트)와 메타데이터를 포함함
	원시 텍스트는 문자 시퀸스지만,일반적으로 문자를 토큰이라는 단위(스페이스바같은)걸로 묶어서 사용함
	메타데이터는 샘플 또는 데이터포인트라고 부름
	
	토큰화는 텍스트를 토큰으로 나누는 과정인데,
	보통 토큰화를 공백만 가지고 하면 부족하고 좀 더 빡세게 바꿔야하는데 라이브러리있으니까 그거쓰던가,아니면 임베딩쓰자
	
	타입은 말뭉치에 등장하는 고유한 토큰,말뭉치에 있는 모든 타입의 집합이 어휘사전,또는 어휘임
	단어는 내용어와 불용어로 구분됨,불용어는 대부분 내용어를 보충하는 문법적 용도로 사용됨
2.유니그램,바이그램,트라이그램,n그램
	그 정보검색때 했던 한글자씩 떼는거
	정보검색이면
	정보 보검 검색 이런식으로 나누는거
	
	영어권에선  단어단위로 자르나봄
	
3.표제어와 어간
	표제어는 단어의 기본형
	fly가 표제어면 flow flew flies 등등이 어미가 바뀌면서 변형된 단어
	
	토큰을 표제어로 바꾸면 벡터표현의 차원이 줄어들어서 도움이 됨
	이런걸 표제어 추출이라고 함
	
	
4.단어분류하기:품사태깅
	문서에 레이블을 할당하는 개념을 단어나 토큰으로 확장할수있음
	단어 분류의 예로는 품사태깅이 있음(단어마다 이게 무슨품사인지를 태깅하는것)
	
5.청크나누기와 개체명인식
	종종 여러 토큰으로 구분되는 텍스트구에 레이블을 할당해야함
	이럴땐 연속된 토큰끼리의 부분영역이 있음
	이걸 나누는걸 청크나누기,혹은 부분구문분석이라고 함
	이거도 라이브러리로 하면됨
	
6.문장구조
	구 단위를 식별하는 부분구문분석과 달리 구 사이의 관계를 파악하는 작업을 구문분석(파싱)이라고 함
	문장안의 문법 구조가 계층적으로 어떻게 관련되는지를 보여주는 트리를 구분분석트리라고 함
	
7.단어의미와 의미론
	단어에는 의미가 하나 이상 있음
	이걸 쓰고싶을떈 영어권은 wordnet
	
	단어의미는 문맥으로 결정될수도있음
	텍스트에서 단어 의미를 자동으로 찾는 일은 실제로 nlp에 적용된 첫 준지도학습임
	
	


3.신경망의 기본구성요소
	그냥 기본임 활성화함수 손실함수 그런거
	파이토치로 뭐 만들일생기면 좀 자세히보자,그거아니면 그냥 다 알던내용임
	
	
1.퍼셉트론
	퍼셉트론은 그 입력출력하나있는 신경망 ,dense라보면됨
	파이토치에선 torch.nn.Linear(입력크기,출력갯수)
	
2.활성화함수
	시그모이드는 1이나 0 사이에서 값이 나오는 함수,매우 쉽게 1이나 0이되기때문에(중간값보다)이진분류에 적합함(중간엔 안쓰고 출력층에만 사용함)
	
	하이퍼볼릭 탄젠트는 시그모이드함수의 변종,얘는 -1~1범위로 바꿈
	
	relu(렐루)는 0이하는 0으로 1이상은 그대로 두는 활성화함수,얘는 보통 그대로 쓰긴하는데,
	좀 커지면 뉴런이 사망하는경우가있어서 리키렐루,prelu같은거 사용도 함(신경망이큰경우)
	
	소프트맥스는 다중분류(예측클래스가 10개라든가)에 사용하는 활성화함수,이거도 출력층에서만 쓰고,소프트맥스의 총합은 1이됨
	얘는 카테고리컬 크로스엔트로피랑 같이사용함 보통
	
3.손실함수
	mse는 회귀에 사용하는 손실함수,rmse는 루트씌운거
	출력과 레이블을 제곱해서 빼는느낌이었음
	알아보기힘들다고 r2쓰는경우도 꽤 있는듯(얘는 최대치100 최소치0인가 그랬음)
	
	카테고리컬 크로스엔트로피는 다중분류에 사용하는 손실함수
	얘는 레이블이 1에가깝고 나머지는 0에 가깝다고 생각하고 거기서 벗어나면 때림
	
	바이너리 크로스엔트로피는 이진분류에 사용하는 손실함수
	얘는 답이 1이고 아니면 0이라고 생각하고 거기서 벗어나면 때림
	
4.지도학습훈련 알아보기
	옵티마이저는 보통 adam쓰고 학습률기본은 0.001임
	
5.부가적인 훈련개념
	평가지표는 손실함수랑 비슷한데 좀 제약이 약한거,보통 자주쓰는건 정확도(accuracy)
	
	모델을 올바르게 측정하려면 test val train으로 나눠야한다 어쩌구저쩌구
	
	모델에포크 수 정하기 힘들면 조기종료를 쓰자
	
	하이퍼파라미터수를 좀 조정해서 최적의값을 찾아보자
	
	규제는 오버핏 좀 막아내는것,보통 드롭아웃같은거 사용
	l2규체는 그래프를 좀 부드럽게만들고(뾰족뾰족해서 오버핏나는게아니라)l1은 좀 더 희소한 솔루션을 만들게됨(대부분 파라미터값이 0이됨)
	
6.레스토랑리뷰 감성분류
	데이터를 토큰화하고 그걸 정수로 매핑(레이블인코딩)그리고 문자열을 수치벡터로 변환하고 그걸 미니배치로 모음
	
	그리고나서 모델을 만들고 훈련을 시키면됨(이진분류니까 바이너리 크로스엔트로피에 시그모이드)
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		