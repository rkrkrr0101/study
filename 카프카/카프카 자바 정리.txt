1.서문
	카프카는 데이터 서빙의 중앙에서 모든 데이터처리를 중앙집중해서 파편화를 없애는 툴임
	즉 모든곳에서의 데이터스트림을 카프카로 통합하고,소비자들도 여기서 소비할수있게 만드는툴
	이렇게되면 데이터를 생성하는애도 그냥 카프카에 밀어넣는책임까지만 가지게되고,소비자를 신경쓰지않아도됨
	
	카프카에서 데이터를 보내는쪽을 프로듀서라고 부르고,
	소비하는쪽을 컨슈머라고 부름(생산자-소비자 패턴)
	카프카로 전달할수있는 데이터포맷의 제한은 사실상없고(직렬화 역직렬화를 사용하기때문에),
	기본적으로는 ByteArray, ByteBuffer, Double, Long, String 타입에 대응한 직렬화클래스를 기본제공함
	커스텀도 Serialzer<T>,Deserialzer<T>를 상속받아서 만들면 됨
	
	상용환경에서 카프카는 최소 3대이상의 서버에서 분산운영해야함
	이래야 한서버가 장애가 발생해도 지속적으로 데이터를 복제하기때문에 안전하게 운영할수있음
	그리고 데이터를 묶음단위로 배치전송하기때문에,낮은지연과 높은데이터처리량도 가지게됨
	
	
	카프카는 데이터레이크로 사용됨,일단 모든데이터를 다 때려박고(정제되지않은),여기서 나중에 필요한걸 알아서 찾으면됨
	이떄 e2e방식으로는 넣으면 한두군데서넣긴 할만한데,많아지면 파편화되고 복잡도가 올라감,그래서 하나로 통일된 솔루션이 필요한데,그게 카프카임
	카프카의 장점은
		배치전송해서 처리량이 높음
		확장이 쉬움(스케일아웃이 쉬움)
		영속성을 가지면서 느리지않음(캐시를 사용해서)
		고가용성(3대이상으로 구성해야하는이유)
	이 있음
	
	카프카는 모든 데이터를 스트림데이터로 취급해서 처리함
	데이터는 배치데이터와 스트림데이터로 나뉘는데,
		배치데이터는 한정된기간단위(140130일 데이터 이런식)의 데이터고,즉 끝이 정해져있고 일괄처리가 쉬움
		스트림은 한정되지않은 데이터고,끝이 정해지지않은(주식정보,클릭로그등)데이터임
	배치데이터를 스트림으로 처리하는건,모든데이터에 타임스탬프를 붙여서 배치데이터로 구성된 스트림으로 취급하는거임(스트림<배치> 이런느낌)
	이렇게 처리하려면 변환기록이 일정기간 삭제되면 안되고,지속적으로 추가되어야함
	그리고 해당레이어는 spof가 될수있으므로 반드시 내결함성과 장애허용특징을 지녀야함
	즉 이런식으로 카프카는 데이터를 처리함
	
2.카프카 빠르게 시작해보기
	aws설치부분은 넘어감 필요하면보자
	
	카프카 커맨드라인툴은 카프카를 사용할때 가장 많이 사용하는 툴임
	이거로 토픽을 생성하고,토픽리스트를 확인하고,프로듀서 컨슈머를 할수있음
	
	
	
	kafka-topics.sh는 이걸 사용해 토픽과 관련된 명령을 실행할수있음
	토픽은 카프카에서 데이터를 구분하는 가장 기본적인 개념임(rdb의 테이블이라고 생각하면됨)
	즉 토픽은 한 카프카 클러스터내에서 여러개 존재할수있고,토픽을 파티셔닝해서 각 클러스터마다 분산배치할수있음
	파티션은 토픽에서 매우 중요한데,이걸사용해서 한번에 처리할수있는 데이터양을 늘릴수있고,토픽내부에서도 파티션을 통해 데이터의 종류를 나눠처리할수있음
	
	토픽을 생성하는방법은 2가지가있는데
		컨슈머나 프로듀서가 생성되지않는 토픽에 대해 데이터를 요청할때(묵시적)
		커맨드라인툴로 명시적으로 토픽을 생성할때
	가급적이면 명시적으로 토픽생성하는게 좋음,토픽마다 처리되어야하는 데이터특성이 다르기때문
	토픽을 생성할때는 데이터특성에 따라 옵션을 다르게 설정할수있음
	만약 동시데이터처리량이 많아야하면 파티션을 100개로 설정하던가,단기데이터처리만 필요하면 데이터보관기간을 짦게 잡던가 할수있음
	
	토픽생성은
		bin/kafka-topics.sh --create --bootstrap-server 카프카1ip:포트 카프카2ip:포트 --topics 토픽명
	으로 생성할수있고,bootstrap-server뒤에 적힌 카프카 클러스터들에 동시에 세팅할수있음
	토픽명은 내부데이터가 뭐들어있는지 유추가 가능할정도로 자세하게적는게 나중에 알아보기좋음
	그리고 추가적으로 설정추가하려면(엔터넣으려면 \ 추가)
		bin/kafka-topics.sh --create 
			--bootstrap-server 카프카1ip:포트 카프카2ip:포트 
			--partitions 3 //파티션수,기본값1
			--replication-factor 1 //파티션복제갯수,기본값은 브로커설정따라감,복제1마다 저장하는 브로커갯수가 늘어남
			--config retention.ms=172800000 //토픽데이터 유지기간,ms단위
			--topics 토픽명
	이렇게 할수있음
	
	토픽리스트조회는
		bin/kafka-topics.sh --bootstrap-server 카프카1ip:포트 --list
	로 목록조회할수있고,자세히는
		bin/kafka-topics.sh --bootstrap-server 카프카1ip:포트 --describe --topic 토픽명
	으로 상세조회할수있음
	상세조회시에는 해당토픽설정,파티션과,리더,레플리카등이 나타남
	이때 토픽들의 리더가 일부 브로커에 몰려있는경우,부하가 특정브로커들로 몰릴수있어서,네트워크 대역이슈시 이런식으로 확인후 분산배치해야할수있음
	
	토픽의 옵션을 수정할땐,kafka-topics.sh나 kafka-configs.sh 두개를 사용해서 수정할수있음
	파티션갯수수정을 할때는 kafka-topics.sh를 사용하고,토픽삭제정책인 리탠션기간을 수정할땐 kafka-configs.sh를 사용해야함
	파티션갯수수정은
		bin/kafka-topics.sh 
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명
			--alter
			--partitions 바꿀파티션갯수(증가만 가능)
	리탠션수정은
		bin/kafka-configs.sh
			--bootstrap-server 카프카1ip:포트
			--entity-type topics
			--entity-name 토픽명
			--alter
			-- all-config retention.ms=바꿀삭제시간
	이렇게 하면됨
	이떄 파티션은,늘릴수는있지만 줄일수는 없으니 잘생각해야함
	
	
	
	토픽에 데이터를 넣을땐,kafka-console-producer.sh를 사용할수있음
	토픽에 넣는 데이터는 레코드라고 부르며,메세지키와 메세지값으로 이루어져있음
	메시지 키 없이 보낼수도있는데 이땐
		bin/kafka-console-producer.sh 
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명
	하면 텍스트를 입력할수있게됨
	그뒤로 입력하고 엔터치면 데이터 들어가는거
	이떄 주의할점은,이거로 던지면 무조건 스트링타입으로 던져지니까 주의해야함
	메시지키를 가진걸 보낼땐
		bin/kafka-console-producer.sh 
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명	
			--property "parse.key=true"
			--property "key.separator=:" //기본값 \t(탭문자)
	이렇게 키사용 true로 놓고,키 구분자를 넣어주면됨
	그리고 텍스트를
		abc1:ppp123
	이런식으로 넣으면 abc1이 키고 ppp123이 값이됨
	만약 이렇게했는데 키를 넣지않으면 익셉션터지면서 종료됨
	
	메세지키가 없이 전송한 레코드는 라운드로빈으로 하나씩 던지고(키가 null),
	메세지키가 있는 레코드는,키의 해시값으로 던져서 키가 같은 모든 데이터는 한 파티션에 존재하게됨
	만약 파티션갯수를 바꿀경우엔 이게 보장되지않으니,이땐 커스텀파티셔너를 만들어야함
	카프카는 레코드를 오프셋,키,값으로 저장함
	오프셋은 컨슈머가 어디까지 데이터를 처리했는지같은걸 처리할때 사용하는 타임스탬프임
	
	
	
	이렇게 전송한 데이터를 받아볼땐 kafka-console-consumer.sh로 받아볼수있음
	이때 --from-beginning옵션을 주면 토픽에 저장된 가장 처음데이터부터 출력하고,없으면 지금부터받는걸 출력함
	기본적으론 키를 출력하지않고 값만 출력함
		bin/kafka-console-consumer.sh
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명
			--from-beginning
	키도 보고싶으면
		bin/kafka-console-consumer.sh
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명
			--property print.key=true
			--property key.separator="-"
			--group 그룹명
			--from-beginning	
	이렇게 할수있음
	이때 그룹은 컨슈머그룹으로,만약 해당이름의 그룹이 없었다면 새 그룹을 생성함
	그룹은 1개이상의 컨슈머로 이루어져있고,이 그룹을 통해 가져간 컨슈머는 토픽의 메시지에 대해 커밋(해당오프셋까지 처리완료했다고 브로커에 저장하는것)함
	이때 카프카프로듀서로 전송할때와 순서가 다를수있는데,이건 파티션때문에그럼
	컨슈머는 모든 파티션으로부터 동일한 중요도로 데이터를 가져가기때문에 순서를 보장하지않음
	만약 순서를 보장해야한다면,파티션을 1개로만 구성하면 순서를 보장함
	
	
	컨슈머그룹의 목록을 확인할땐 kafka-consumer-groups.sh를 사용할수있음
	생성은 그냥 따로 할필요없고,컨슈머를 킬때 그룸명을 지정하면 새로 생성됨
	목록확인은
		bin/kafka-console-consumer-groups.sh
			--bootstrap-server 카프카1ip:포트
			--list
	로 확인할수있음
	이거도 describe로 자세히보기할수있음
		bin/kafka-console-consumer-groups.sh
			--bootstrap-server 카프카1ip:포트
			--group 그룹명
			--describe
	이러면 
		그룹이 마지막으로 커밋한 토픽과 파티션
		그룹의 최신오프셋
		처리한 마지막 오프셋
		둘간의 차이(지연)
		내부id(컨슈머id+uuid)
		컨슈머 호스트명 or ip
		컨슈머id
	를 확인할수있음
	이걸로 그룹중복확인과 랙이 얼마인지를 확인하여서 컨슈머상태최적화를 할수있음
	
	
	그냥 간단히 네트워크통신테스트할땐
		kafka-verifiable-producer.sh
		kafka-verifiable-consumer.sh
	둘을 사용해서 할수있음
	핑테스트같은느낌
	이건
		bin/kafka-verifiable-producer.sh 
			--bootstrap-server 카프카1ip:포트
			--max-messages 10 //보낼메시지수,-1이면 종료될때까지 계속보냄
			--topic 토픽명
	로 보내고
	이때 
		최초실행시점
		메시지별 보낸시간,키,값,토픽,저장된파티션,저장된오프셋번호
		max도달후 통계값
	이 표시됨
	
	받을땐
		bin/kafka-verifiable-consumer.sh
			--bootstrap-server 카프카1ip:포트
			--topic 토픽명
			--group-id 그룹명
	으로 받아볼수있음
	이때
		최초실행시점
		할당된파티션
		받아온레코드들(파티션명,갯수,최소오프셋,최대오프셋)
		커밋메시지
	가 나타남
	컨슈머는 레코드를 한꺼번에 리스트로 받기때문에,한번에 n개씩 받음
	
	
	카프카에서 레코드를 삭제할땐 kafka-delete-records.sh를 사용할수있음
	이건 이미 적재된 데이터중 가장 오래된데이터(가장 낮은숫자의 오프셋)부터 특정시점까지의 오프셋까지를 삭제할수있음
	즉,0번부터 100번까지 있을때 0번부터 30번까지 지우는건 되지만 30번만 지우는건 불가능하다는거에 유의해야함
	
	이건 삭제할 토픽을 json으로 만들고 sh를 실행해야함
	json은
		{"partitions":[{"topic":"토픽명","partition":파티션명,"offset":삭제할오프셋}],"version":1}
	그리고나서
		bin/kafka-delete-records.sh
			--bootstrap-server 카프카1ip:포트
			--offset-json-file 삭제json.json
	하면 저 json에 있는 오프셋까지를 삭제함
	
	
3.카프카 기본개념	
	
	
	
	
	
	
	
	
	
	