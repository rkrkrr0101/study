1.경진대회란
	1장은 뭐없으니 스킵

2.평가지표
1.문제종류
	보통 문제는 회귀,분류,추천이 있는데 
	회귀는 mse를 쓰고
	분류는 
		이진분류는 바이너리 크로스엔트로피
		다중클래스분류는 멀티 클래스 크로스엔트로피,mse,카테고리컬 크로스엔트로피를 사용
	
	추천은 분류의 하위라고 보면됨(이진분류로 품,각 사용자가 a,b,c상품을 구매할지 말지를 분류하는식으로)
	
2.데이터셋
	1.정형데이터
		정형데이터는 csv같은 행과 열이 있는 형식의 데이터
	2.외부데이터
		외부데이터는 캐글에서 주는데이터가 아닌 외부에서 가져온데이터,대회마다 사용가능,불가능한게 있고 사용할땐 공개해야함
	3.시계열데이터
		시간의 흐름에 따라 관측된 데이터,즉 데이터의 순서에 인과관계가 있을수있음(음식점의 일별 손님수,주식그래프등)
	4.기타데이터
		이미지,동영상,음성등의 데이터,얘들은 딥러닝을 많이 사용함(합성곱)
		
		
		
		
		
		
3.평가지표
	평가지표는 모델의 성능이나 예측결과의 좋고 나쁨을 평가하는 지표임
	그래서 목적함수랑 겹치는 부분이 많음
	1.회귀의 평가지표
		1.rmse
			mse에 루트씌운거 
			각 데이터에 예측값과 실제값의 차이를 제곱하고 평균을 내 루트씌움
			특성상 회귀문제에 사용편함 목적함수로
		2.mae
			mse에 절대값씌운거,이상치영향을 좀 덜받는대신,좀 쓰기힘듬(2차미분0,미분 불연속)
		3.결정계수
			회귀분석의 적합성을 나타내는데,이지표의 최대화는 rmse의 최소화와 같음
		
	2.이진분류의 평가지표
		이진분류의 평가지표는 크게 두개로 나눔
		결과값만 중시하는것(혼동행렬기반)
		확률값대로 계산하는것(크로스엔트로피기반)
		1.혼동행렬
			tp,tn,fp,fn으로 나누는 그거
			이쪽이 기반이되는 평가지표는 임계값을 올렸다내렸다 한다는걸 미리 바닥에 깔아두고있음
			
			1.정확도
				정확하게 예측한 비율,(tp+tn)/전체행수
				이건 이거자체로는 쓰기힘들어서(불균형한 데이터일때 전부 p나 n으로 밀면 정확도높게나옴)잘안씀
			2.정밀도와 재현률 
				양성으로 예측한값중 실제값도 양성일 확률은 정밀도
				실제값이 양성인거중 예측값이 양성일 확률은 재현율
				
				즉 정밀도는 진짜 확실한거말고는 다 네거티브에 던져서 사기칠수있고
				재현율은 다 포지티브에 던지면 사기칠수있어서
				둘을 섞은 f1스코어를 주로 사용함
			3.f1스코어
				정밀도와 재현율을 섞은것
				
		2.로그손실
			1.바이너리 크로스 엔트로피
				기본적인 바이너리 크로스엔트로피
				확률이 크게 틀리면 값을 많이더하고(머리를 세게치고)확률이 거의 맞으면(양성인데 0.95라던가)값을 조금만 더하고(살살 톡건드리고)
			
			
			2.auc
				roc곡선의 아래면적을 auc라고 부르나봄
				roc곡선은 임계값을 움직일떄의 거짓양성비율(진짜거짓인걸 양성으로 잘못예측한비율(fp/(fp+tn))과 
				참 양성비율(tp/(tp+fn))을 그래프의 xy축으로 나타낸것
				
				
				

	3.다중클래스 분류의 평가지표
		1.멀티 클래스 액츄얼리
			이진분류의 정확도를 다중클래스로 확장한것
			예측이 정답인 행 데이터 수를 모든 행 데이터 수로 나눈 결과
			
		2.멀티 클래스 로그손실(크로스엔트로피)
			이진분류의 바이너리 크로스엔트로피를 멀티클래스로 확장한것
			각 클래스의 예측확률을 내고 행 데이터의 예측확률에 로그를써서 부호를 반전시킨값이 점수임
			
		3.meanf1,macrof1,microf1
			말그대로 다중레이블에서의 f1평균값들
			mean은 전체평균낸거
			macro는 각 클래스별f1평균값,얘는 각 클래스에서 이진분류를 하고 그거의 f1스코어를 평균하는것과 같아서,
			다중레이블에서는 각 클래스별로 독립적으로 임계값을 최적화 가능(즉 a,b,c 세레이블이 있으면 각기 f1스코어를 하나씩 가지는거)
			micro는 행데이터x클래스의 각 쌍에대해 어디인지(tp,tn,fp,fn)카운트해서 그 혼동행렬에 근거해서 f1을 계산함
			
		4.qwk
			이건 다중클래스 분류에서 클래스간의 순서관계가 있을때 사용(영화점수 1~5라던가)
			각 행 데이터의 예측값이 어느클래스에 속하는지 제출함
			즉 이건 순서가 멀리 떨어질수록 패널티를 세게줌(5점이 레이블인데 2점예측했다던가)
			얘는 완전한예측이면 1,랜덤일땐 0 랜덤보다 나쁘면 -값을 줌
			
	4.추천의 평가지표
		1.MAP@k
			얘는 각 행 데이터가 하나 또는 여러 클래스에 속할떄,포함될 가능성이 높을것으로 예측한 순서대로 k개를 예측값으로 삼음
			단 k번째 예측값이 정답일경우에만 값을 취하고,그 외에는 0이됨
			계산방식은 1번부터 k번까지 가면서 분모는 계속+1을 하고 분자가 정답일때만 +1을 해서 k번까지 가는 식
				1번 정답 1/1
				2번 오답 --1/2 총합에는빠짐
				3번 정답 2/3
				...
			이런식
			저기에 답의 갯수로 총합을 나누면 map@k
				(5/3)/정답수
			
			
4.평가지표와 목적함수
	1.평가지표와 목적함수의 차이점
		기본적으로 평가지표는 목적함수로 사용할수있음(같은함수를 사용하는경우가 많음)
		그렇지만 평가지표가 좀 더 제약이 덜함(목적함수보다)
		목적함수는 미분을 할수 있어야함(그레이디언트 최소화)
		그리고 보통 회귀는 mse,분류는 크로스엔트로피를 자주 사용함
		
		만약 목적함수와 평가지표를 통일시킬수있다면 통일시키는게 좋음
		
	2.사용자 정의 평가지표와 목적함수
		실제로 뭐 이상한 목적함수를 요구하면 만들어쓸수도 있음(머신러닝책참고)
		
		
5.평가지표의 최적화
	평가지표랑 목적함수를 통일시킬수있으면 통일시키는게 좋음
	학습데이터를 전처리해서 평가지표를 통일시킬수있음(레이블에 로그를써서 변환하고 학습시킨뒤 로그를 푼다던가)
	다른 평가지표를 사용하고 후처리
	사용자 정의 목적함수의 사용
	다른 평가지표를 사용하고 조기종료
	
	1.임계값 최적화
		임계값을 0.5가 아닌 최적의 임계값을 찾기
		단 트레인데이터에 하면 오버핏날수있으니까 분할해서 검증(oof)
		
		하는건 0.01부터 0.99까지 다 돌려보거나,scipy.optimize사용
		
	2.예측확률과 조정
		평가지표를 최적화하려면 타당한 예측확률이 필요함
		보통 신경망등은 크로스엔트로피를 목적함수로 사용하니까 대충 타당한 예측확률이라고 할수있는데,
		이게 특정원인에 따라 왜곡될수있는데 이때 확률을 조정하면 점수가 올라갈수있음
		
		1.예측확률의 왜곡
			1.데이터가 충분하지 않을때
				데이터가 적으면 0이나 1에 가까운 확률을 뱉기힘듬
			2.모델 학습 알고리즘상 타당한 확률을 예측하도록 최적화 되지 않은경우
				랜덤포레스트같은데 크로스엔트로피를 쓴다던가 그러면 확률이 꼬임(이진트리랑 안맞으니까)
			
		
		2.예측확률의 조정
			1.예측값을 n제곱
				마지막에 예측값을 0.9~1.1을 곱해서 보정할수있음
			2.0이나 1에 가까운확률 제외
				너무 큰 패널티를 피하는것(0.1~99.9까지만 받는다던지)
			3.스태킹
				확률예측모델을 사용(신경망쓰면 됨)
			4.calibratedcalssifierCV사용
				예측값을 보정하는 방법인데,시그모이드나 등위회귀등을 선택할수있음
				얘는 0이나 1에 한없이 가까운 확률을 보정함
				
				
6.데이터 정보 누출
	1.예측에 유용한 정보 누출
		막 테스트데이터같은게 유출되는것
		중반이후에 갑자기 말도안되는점수 올라오면 이거찾아다녀야함
	2.검증방법이 잘못된 누출 
		oof에서 막 id값같은걸 사용해서 학습했는데,테스트데이터는 그런게 없을수있음
		이건 5장에서 다시보자




3.특징생성
1.모델과특징
	1.모델과 특징의 관계
		모델에 따라 특징을 먹는 방식이 있음
		gbdt같은경우
			수치의 크기(범위)는 의미가 없고 크고작은관계만 영향이 있음(트리니까)
			결측값이 있어도 그대로 처리가능
			결정트리 내부 반복작업에따라(안에서 분기갈리는거에 따라) 변수간 상호작용을 반영함
			
		즉 수치의 대소관계가 영향을 안주니 정규화를 안해도되고,결측값을 꼭 채워야하는것도아니고(에러가안뜸)
		범주형을 원핫인코딩이 아닌 레이블인코딩(0,1,2,3으로 인코딩하는거)해도 됨
		그래서 gbdt를 간단하게쓸떄 자주씀
		
		신경망은
			값의 범위에 영향을 받음
			결측값을 채워야함
			앞층의 출력을 결합하여 하는 계산으로 변수간 상호작용을 반영함
		
		즉 수치의 대소관계에 영향을 받아서 정규화해야하고,결측값은 꼭 채워야하고(안채우면에러)
		레이블인코딩한 값이 그대로 쓰이니까(숫자크기에 영향을 받으니까(1과4보단 1과2가 가깝다고 생각하는것))레이블인코딩보단 원핫인코딩이 더 좋음
		
	2.베이스라인이 되는 특징
		gbdt는 그냥 id만 지우고 범주형을 레이블인코딩하면 바로 베이스라인으로 쓸수있음(결측치도 안매워도되니까)
		신경망은 범주형을 원핫인코딩하고 결측값을 채우고 표준화를 한걸 기본베이스라인으로 쓸수있음
		
	3.결정트리의 사고방식으로 생각하기
		특징이 유효할지 생각할때 결정트리처럼 생각하면됨
		기본적으로 데이터를 입력하면 변수간 상호작용이나 비선형관계성도 정확히 반응하여 예측하지만(분기를 조합하여 상호작용이나 비선형의 관계성이 표현가능하니까),
		그러나 기본적으로 존재하지않는 정보를 입력정보로 반영할수는 없음,그리고 상호작용을 직접 표현한(두특징을 합쳐서 컬럼에넣었다던가,명확한특성)특징이 있으면
		그걸 반영하는게 더 쉬움
		
		그러니까 우리가 해야할건 입력으로 읽어낼수 없거나(아예 다른데이터셋에 있거나),읽기 어려운(찾아내기 좀 빡세보이는 특징)을 뽑아내서 생성해줘야함
		즉 평균단가같은게 레이블결정에 크게 영향을 주는데,이걸 그냥 판매금액과 판매갯수로 들어가있어도 어느정도 반영이 되긴하지만,
		둘을 나눈 컬럼을 추가하면 더 정확하게 100%반영이됨
		
		
		
2.결측값 처리
	결측값은 몇가지 이유로 만들어지는데
		값이 존재하지 않거나(개인과 법인데이터가 섞여있을때 법인의 나이)
		특정의도가 있는경우(측정을 하지않음(데이터가없음))
		값을 얻는데 실패한경우(오류로 입력이안됨)
	gbdt는 결측치무시하고 쓰면되는데 신경망은 채워줘야함
	
	여기서 몇가지 선택사항이 있음
	1.결측값인채 사용
		gbdt에서는 사용가능한 방식
		만약 결측값이 에러뜨는데 결정트리식모델이면 결측값을 -9999처럼 안쓰는수로 바꿔주면됨
	2.대푯값으로 채우기
		제일 많은수나,평균값으로 채우기
		근데 결측값이 랜덤하게 발생하는게 아니면 추천할만한 방법은 아님(결측이 발생하는 이유가있을때)
		또 다른방법은 다른 컬럼의 범주형값으로 그룹을 만들어서 그 그룹의 평균을 넣던가할수도 있음
	3.다른변수로 결측값 예측하기
		결측값을 예상하는 모델을 만들어서 그거로 넣을수도있음,당연하지만 여기서도 레이블은 뺴야함 테스트데이터엔 레이블이 없으니까
		
		na인걸 전부 테스트데이터로 사용하고 값있는거만 남기고 그거가지고 모델돌리고 na값에 예측해서 넣고 그걸 사용하는식
		
	4.결측값으로 새로운 특징 만들기
		결측값 자체가 특징이라고 할수도 있으니까,새 필드를 만들어서 거기다 이게 특정컬럼이 결측이었냐 아니냐를 0,1로 나타내게 할수있음
		그리고 결측값이 여러개 나타날때 그 조합을 패턴으로 분류할수있으면,그 패턴을 원핫인코딩할수있음
	
	5.데이터의 결측값 인식
		결측값이 공백이나 na가 아니라 -1이나 9999같이 입력되어있을수도 있으니까 히스토그램등으로 확인해야함
		그리고 어떤 컬럼에선 -1이 결측값인데 어떤건 -1이 유효한값이면 해당컬럼을 읽어서 치환시키면됨(b['ggg'].replace(-1,np.nan))
		
		
3.수치형 변수 변환
	수치도 변환을 하긴해야함(정규화같은거나 그룹핑하는거)
	그리고 여기서 중요한게 캐글에서는 학습데이터랑 테스트데이터를 결합해서 평균분산을 계산하고 다시 떼는게 좋음(일이편함)
	그냥 만들때는 예측대상 데이터가 없을테니까 학습데이터기준으로 해야겠지만
	1.표준화
		말그대로 정규화하는거
		모델밖에서 할떈 사이킷런의 standardscaler쓰면되고 모델안에선 어짜피 배치노말라이제이션쓰겟지
	2.최소최대스케일링
		이건 최소값을 전체에 뺀후에 max-min으로 나눈거
		최대는 1이되고 최소는 0이됨
		
		이건 보통 숫자형에는 잘 안쓰고 이미지나 소리같은데서 사용함(픽셀은 0~255가 정해져잇으니까)
		숫자는 표준화사용함
	3.비선형변환
		앞에거는 그냥 숫자만 줄지 그래프의 형태는 바뀌지않음(분포는 안바뀜)
		이건 데이터가 불균형상태일때 데이터의 형태를 바꾸는거(값이 커지면 많이깎는다던가)
		
		1.로그변환
			이건 log(x+1)을 씌우는거
			+1은 log 0들어갈까봐 해둔거
			만약 -값이 있으면 절대값씌우고 로그하고 다시 부호붙이면됨
			
		2.박스칵스변환,여존슨변환
			이건 로그변환을 일반화한거(박스칵스)와 그걸 음수에도 사용할수있게한거(여존슨변환)
			사이킷런에 있으니까 필요하면검색
		
		
		이런거들을 사용하고나면 그래프가 보통 표준분포에 가까워짐(중앙이 높은 산모양)
		
	4.클리핑
		이건 상한 하한을 잘라내서(상한 하한값으로 치환) 이상치를 없애는거
		간단하게는 1%와 99%를 잘라낼수있음
		이러면 그래프가 압축되어서 표시잘될수있음
		
	5.구간분할
		구간별로 범주형변수로 만들어서 사용하는거
		데이터에 대한 사전지식이 있고 나눠야하는 이유가 있을때 효과적임
		이걸 범주형의 중간값으로 치환할수도있고,원핫인코딩할수도있고 그럼
		
	6.순위로 변환
		수치형 변수를 대소관계에 따른 순위(1~9이런식으로)바꿀수도있음
		수치의 크기나 간격을 버리고 대소관계만을 얻는 방법
		
	7.rankgauss
		이건 순위로 변환한후 순위를 유지한채로 반강제적으로 정규분포로 바꾸는것
		신경망에서 일반적인 표준화보다 성능이 좋다고함
		
		
4.범주형 변수 변환
	범주형 변수 변환에선 원핫인코딩,레이블인코딩,임베딩등 여러방법이 있음
	그리고 수치형변수라고 해도 값의 크기나 순서에 의미가 없으면 범주형변수로 인코딩해야함(1~9로 종류나눠둔거)
	
	그리고 학습데이터엔 없고 테스트데이터에만 존재하는 범주가 있을땐
		대응하지 않아도 될경우
			특별히 영향안주는 하꼬데이터면 무시함
		최빈값이나 예측으로 보완
			결측값으로 간주해서 모델만들어서 예측하거나 자주나오는값으로 매꾸기
		해당 변환의 평균으로 입력
			학습데이터 전체의 최빈값으로 채움
			
			
	1.원핫인코딩
		가장 대표적인 처리방법
		범주의 각 레벨(카테고리)에 대해 맞는지 아닌지를 0,1로 표현함
		이건 문제가 범주형변수의 레벨이 많은경우 특징갯수가 엄청나게 늘어남
		이런경우
			다른인코딩방법을 찾던가
			규칙을 찾아서 그륩화해서 레벨을 줄이던가
			빈도낮은걸 모아서 기타로 넣던가
		
	2.레이블 인코딩
		각 레벨을 단순하게 정수로 변환(aaa,bbb,aaa->1,2,1)
		이러면 결정트리는 상관없는데,신경망같은건(사실상 결정트리뺴고 전부) 숫자크기에 영향을 받음(1과2가 1과4보다 가깝다고생각하는등)
		타깃정도만 레이블인코딩 할수있음(0,1 이진이라던지 이럴떄)
		
	3.특징 해싱
		원핫인코딩에서 특징수가 정해져있는경우(a3815,b2232)계수정렬처럼 하나하나 떼서 원핫인코딩하는거(a,3,8,1,5 b,2,2,3,2 이런식)
		캐글에선 잘 안쓰긴하는듯
		
	4.프리퀀시 인코딩
		각 범주의 출현 횟수나 빈도로 변수를 대체함
		출현빈도와 목적변수와의 관련성이 있을때 사용
		근데 이렇게쓰면 동률이 나올수도 있으니 주의해야함
		
	5.타깃인코딩
		목적변수(레이블)을 사용하여 범주형을 수치형으로 변환하는것
		이건 잘못쓰면 데이터누출확률 올라감
		
		특정 범주형을 전부 카운트한다음에 그거의 목적변수의 평균을 내서 그걸 대입해버림
		그러니까 잘못하면 데이터누출나서 오버핏남
		
		그래서 이거 사용할때는 데이터를 나눈후 자신을 제외한 나머지폴드들의 평균을 넣는식으로 좀 막을수있음
		테스트데이터에선 트레인데이터 전체를 사용해서 넣고
		
		1.누출
			만약 어떤 레벨에 속하는 데이터가 1개일때 해당레벨의 타깃인코딩값은 목적변수 그 자체가 되어버림
			극단적으로 프라이머리키로 돌리면 당연히 목적변수가 쓰이니까 정답을 먹여주는꼴이됨
			
		2.폴드시 문제
			폴드수가 너무 많아져도 문젠데 그럼 나누기의 값을 보고(0.5,0.25) 값을 추측해서 목적변수 누출이 일어날수있음
			그래서 4~10개만 폴드만드는게 좋음
			
		3.기타 누출막는법
			노이즈 추가하거나 데이터 수가적은레벨은 데이터전체 평균값과 가중치 부여하는식으로 막을수있음
			
			
	6.임베딩
		범주나 자연어등을 실수벡터로 변환하는거
		임베딩 자체도 하나의 모델로 볼수있음
		
		임베딩쓸땐 이미 처리 끝난 모델들 있으니까 그거가져다쓰면됨

	7.순서변수의 취급
		순위처럼 순서관계에만 의미가 있고 간격은 의미없는애들(등수)
		보통 수치형으로 치환할수도있고(a,b,c->1,2,3)범주형으로 치환할수도있음
		
	8.범주형 변수값의 의미 추출
		만약 레벨이 무의미한게 아닌 의미가 있다면(abc-00123에서 abc가 로뜨넘버라든가)abc랑 숫자를 분리해서 사용할수도 있음
		그냥 임베딩돌리면 의미가 사라져버림
		
		
		
5.날짜및 시간변수 변환
	날짜와 시간은 시계열데이터니까 주의할점이 있음
	만약 같은 날짜를 테스트와 트레인으로 짤랐다면 날짜시간특징을 트레인으로 훈련하면 테스트에도 같은 영향을 주지만,
	미래정보를 테스트로 주고 그 전 데이터가 트레인이면 같은 영향을 준다는 보장이 없음(일단 연도부터가 다름)
	
	이럴땐 연도정보를 빼버리거나 ,최신데이터만 사용하거나(테스트와 가까운) 하는게 오히려 더 좋은 모델이 나올 가능성이 있음
	
	그리고 주기성이 있는 데이터가 있을때,만약 주기가 2번이상 돌아가지 않을경우(2년이하일경우) 예측이 떨어질수있음(이게 주기적으로 이러는건지 확신을 하지못함)
	
	그리고 12월1월은 너무비슷하니까 한 9월을 월말로 잡는방법도있음
	
	1.주기성 변수다루기
		월같은 주기성변수를 단순원핫인코딩해버리면 서로간의 관계가 날아가버려서 정보가 손실되고,수치로 다뤄도 12~1의경우 반영이 제대로안됨
		그래서 두개의 변수로(xy좌표로) 원형으로 나타내는방법도 있는데 이경우도 3월9월을 같은레벨에 있다고 취급할 위험성이 생김
		
	2.날짜변수와 시간변수로 만드는 특징
		1.연도
			연도데이터가 예측에 유효하게 작용할지 아닐지는 데이터 분할방식에 따라 다름(주가는 연도에 영향을 받는데 그건 테스트트레인이 같이잘려야함)
			이때 추가하는 방법은
				단순추가
				연도정보를 특징에 추가하되 테스트데이터에만 존재하는 연도를 학습데이터의 최신연도로 치환
				연도를 사용하지않음
				연도와 월을 사용해서 학습데이터의 사용기간을 제한
				
		2.월
			월 정보를 넣으면 1년간의 계절성을 파악할수 있음
			단 2년미만일땐 빼는게나음
			
		3.일
			일을 수치로 넣으면 월에 주기적인 경향이 있을때 일정보를 토대로 파악할수있음(월급날,월초,월말)
			이걸 전부 원핫인코딩하면 변수가 너무 많아지니까 특징있는날에만(월말 월급날 월중간 월초)해당일인지 아닌지 바이너리로 변수만드는게 좋음
			
		4.요일,공휴일,휴일
			요일은 원핫인코딩도 쉽고(값이 7개뿐임)혹은 공휴일을 포함해 휴일인가 아닌가로 원핫인코딩할수도있음
			
			또 설날이나 크리스마스같은걸로 사용할수도있음
			
		5.시분초
			시간을 사용하면 하루중 주기적인 움직임을 반영할수 있음(24개변수)
			보통 분초는 안쓰고 시간도 3시간단위 이렇게쓸수도있음(8개변수)
			
		6.시간차
			건물지은지 얼마됐는지 이런걸로 값 2개빼서 시간의 차이를 사용할수도 있음
			즉 데이터 별로 다른 시간을 기준으로 삼아 시간차를 뽑아냄
			

6.변수의 조합
	변수를 조합해서 변수간 상호작용(방갯수랑 집전체크기를 나눠서 방크기를 만든다던지)을 표현하는 특징을 만들수있음
	
	수치형변수x범주형변수
		범주의 레벨별로 수치형변수의 평균이나 분산을 뽑아 새 특징으로 삼을수있음
	수치x수치
		수치 두개이상을 사칙연산해서 새 특징을 만들수있음(방크기같은거)
		여기서 모델은 덧셈뺄셈보다 곱셈나눗셈의 상관관계에 취약하니까 곱셈나눗셈으로 만들수있는 특성을 찾는게 좋음
	범주x범주
		범주 여러개를 조합해서 새로운 범주로 만들수있음
		결합한뒤의 최대레벨수는 두 범주의 레벨곱만큼 되니까,원핫인코딩은 사용하지못함
		그래서 범주형끼리 조합할때는 타깃인코딩을 씀,목적변수의 평균을 계산하는 그룹이 세분화되니까 더 특징적인 경향이 나올 가능성이 높아짐
		물론 세분화될수록 과적합에 위험이 올라가긴함
		
	행의 통계량
		행의 데이터별로 여러 변수의 통계량을 구할수도 있음,
		모든 변수대상으로 해도 되지만 특정 중요한 변수 몇개로 통계내는게(평균,분산등과 결측값 제로 마이너스값의수) 더 효과적임

7.다른 정형데이터와 결합
	대회에 따라 테스트 트레인 말고 다른 데이터가(상품데이터,로그데이터)주어질수도 있음
	이걸 사용하려면 트레인데이터와 결합시켜야하는데,여기서 일대일이면 그냥 매칭시키면 되지만 일대다일경우는 좀 생각좀해야함
	
	일대다일경우 로그데이터일경우 사용자id별로 통계를 내서(새로고침몇번 로그인몇번 조회몇번 구매몇번)붙이는식도 있고,
	가장 가까운(마지막의) 행동만 붙일수도있고
	간격정보를 붙일수도 있고,
	순서(조회후 구매라던가 몇번조회했을때 구매한번했다던가),체류시간등으로 뽑을수도있음
	즉 중요한건 일대다일경우 정보 전체를 활용하진못하고 특징을 직접 뽑아내야함(데이터에 대한 이해가 필요함)
	
	그리고 특정 행동이나 아침같은 시간으로 잘라서 통계낼수도 있고,id가 아닌 소속그룹단위로 범주형처럼 집계할수도 있음(같은지역,성별,직업등)
	
	또 거꾸로 상품이나 이벤트 중심으로 집계할수도 있음(누가 많이사갔는지)



8.시계열 데이터 처리
	시계열데이터는 시간의 추이에 따른 순차적인 데이터
	시계열데이터는 3가지 포인트가있는데
		1.시간정보를 가지는 변수가 있는지(년월일)
		2.학습데이터와 테스트데이터가 같이 나눠져있는지,테스트데이터가 미래데이터가 아닌지
		3.사용자나 매장 등 계열별로 시계열의 목적변수가 있는지(lag특징을 취할수있는 형식이 있는지,즉 과거의 데이터를 활용할수있는지)
		
	1은 시간정보를 이용하여 주기성특징을 만들수있고
	2는 학습데이터를 그냥 그대로 써도 되는지,미래정보를 부적절하게 사용하는지를 주의해야하고
	3은 과거의 목적변수가 미래예측에 중요한 정보(전날의 매출은 다음날매출을 예측하는데 중요지표임)가 되니까 lag특징을 만들수잇음
	
	1만 있는 데이터는 그냥 개꿀로 특징뽑아쓸수있고
	1,2가 같이있는 데이터는 미래정보 사용안하게 주의해야함(만약 다음달에 해지할지가 레이블인데 다음달로그가 없으면 해지했다고 미래정보사용할수있음)
	1,2,3이 같이 있는데이터는 데이터에 과거목적변수를 추가할수있는데,주의할건 맨앞에는 목적변수를 추가할수 없으니 따로 다른모델쓰던가 해야함(전날데이터가 없는 첫날)
	
	1.누출에 대비한 제약사항
		목적변수가 과거의 목적변수를 포함할경우
			미래에 방문횟수가 증가하면 과거에도 증가할가능성이 높음
		목적변수 이외의 데이터가 과거의 목적변수 정보를 포함할경우
			다음달의 행동로그가 없으면 그 이전에 해지했을 가능성이 높음(레이블을 포함함)
			어떤상품의 프로모션증가는 이미 많이 팔려서 할수도있음(판매건수라는 레이블을 포함)
		
		그래서 목적변수 이외의 데이터도 눈여겨봐야하고,예측시점보다 과거정보만 사용해야함
		그리고 검증할때 학습데이터에 검증데이터보다 미래의 행 데이터를 포함시키면 안됨
		
		
		근데 미래정보가 목적변수랑 크게 연관되지않을땐(시계열적인 특징이 약해서)사용해서 만들기도 하는데,테스트데이터에 없는 정보를 사용하지 않도록 유의해야함
		시계열특성이 약하거나,특징이 목적변수랑 크게 연관이 없거나,데이터가 없어서 그런거 가릴때가 아닐경우정도만 미래정보 사용함
		
	2.와이드포맷과 롱포맷
		와이드포맷은 날짜와 이용자(프라이머리키)를 행열로하고 데이터를 레이블로 하는 포맷이고
		롱포맷은 날짜와 이용자를 모두 열로 하고 다른변수들도 포함시킬수 있는 형식을 롱포맷이라고 함,롱포맷은 키가되는변수와 사용자별로 나머지변수를 유지할수있음
		
		와이드포맷의 사용처는 해당 변수의 시계열적 변화를 알아보기 쉽고 lag특징을 구할때 다루기 쉬움
		그렇지만 훈련할떄는 롱포맷을 써야함
		
		그러니까 롱포맷을 와이드포맷으로 바꿔서 특징을 만들고 그걸 롱포맷에 붙여서 트레인을 돌리는 식이 자주 나옴
		이거 돌릴때는 판다스 stack랑 pivot 사용하면됨 
		
	3.lag특징
		lag특징은 과거시점의 값을 그대로 특징으로 삼는 시차특징
		shift로 미는식으로 lag특징을 만들수있음 
		여기서 밀린쪽의 맨앞은 na되니까 그걸 채워주든 따로 다른모델롤리든 해야함
		
		그리고 단순 이동말고 이동평균(일주일간의 평균으로 요일주기를 지운다던가)도 자주쓰임
		팬더스 rolling(window=7),mean으로 하면됨
		
		1.과거데이터의 집계허용범위
			과거데이터가 요즘주기랑 달라졌으면 빼버리는게 낫고(노이즈) 큰차이없는데이터면 넣어서 하는게 나음
			그리고 최신데이터에 가중치를 주는 가중이동평균같은걸 사용할수도있음
		2.다양한 조건으로 lag생성
			단순 과거매출뿐 아니라 그 매장지역의 매출평균처럼 그륩화해서 집계한 결과로 lag특징을 만들수도 있음
		3.목적변수이외의 lag생성
			다른 변수(날씨등)으로 전날날씨같은거로 lag를 만들수도있음
			
		4.lead특징
			이건 과거가 아니라 미래정보임
			이건 미래의 목적변수는 사용할수있으니까 못쓰고,하루뒤의 날씨(일기예보)등을 사용할수있음(맨마지막은 따로돌려야겠지만)
			
		5.시점관련 특징 생성
			시점과 연계된 특징을 만들고 해당 시점을 키로 삼아서 데이터와 결합하는 방식도 있음
			예를들어서 그 시점에서 특정 이벤트의 개최수라던지를 삼을수있음(세일진행을 바이너리로 하고 세일누적수를 누적합으로 표기하는식으로)
			여기에 더해 특정 id가 몇번 세일을 봤는지(가입일로부터 세일진행수)도 쓸수있음
			
		6.데이터의 기간
			앞에서 말했던거처럼 테스트의 맨 앞같이 lag를못쓰는건 따로모델을 돌리던가 다른값으로 채워야함
			
	
9.차원축소와 비지도학습의 특징
	1.주성분분석(pca)
		이건 그 맨날보던 프레스로 찍을때 가장 분산높게 찍는거
	2.음수미포함 행렬분해(NMF)
		음수가 들어가지 않은 행렬 데이터를,음수를 포함하지 않은 행렬들의 곱의 형태로 만드는것
		이건 음수가 들어가면 안되지만,pca와 달리 벡터의 합 형태로 나타낼수있음
		
	3.잠재디리클레 할당(LDA)
		얘는 자연어처리에서 문서분류에 쓰이는 기법임
		각문서를 행으로 각 단어를 열로 해당 문서에 해당 단어가 몇번 등장했는지 보여주는 행렬(tf라고 봐도될듯)을 만들어두고
		분류할 토픽의 수를 지정해둠
		
		여기서 베이즈추론으로 각 문서를 확률적으로 토픽으로 분류하고(각문서를 요소수가 토픽의수와 같은 벡터로 변환)이 벡터의 요소는 각 토픽에 속하는 확률을 나타냄
		
		즉 lda를 사용하면 tfidf에서 문서가 각 토픽에 소속될 확률행렬과 토픽의 단어분포를 나타내는행렬을 작성할수있음(문서의 주제를 찾을수있음)
		lda는 빈도수기반(백오브워드)고 순서는 토픽에 기여하지않음(정보날림)
		
		즉 lda는 토픽갯수만큼  군집을 시켜버림(각 토픽은 단어의 무리라고 정의하고 비슷한단어군이 모이면 토픽이라고 해버림)
		
	4.선형판별분석(LDA)
		지도학습의 분류문제에서 차원축소를 실시하는 방법 
		학습데이터를 잘 분류할수있는 저차원의 특징공간을 찾고(분리가 잘된),원래특징을 거기에 투영해서 차원을 줄임
		즉 가장 분리가 잘된 -1차원등을 찾아서 거기로 프레스로 눌러버림
		즉 분류용 pca
		
	5.기타
		tsne나 umap같은게 있긴하대
	6.오토인코더
		오토인코더는 입력공간보다 작은 중간층으로 입력을 그대로 재현하는신경망
		즉 인코더와 디코더를 떼서 쓰는식으로 차원축소를 할수있음

	7.군집화
		클러스터링, 데이터를 특정 식으로(kmean등을써서) 분리하고 범주형으로 삼음


10.기타기법
	1.배경메커니즘의 이해
		특징을 생성하는 방법은 다양하게 있음,이떄 분석대상데이터에 관한 배경지식을 사용해서 유효한 특징부터 만들어나가는게 효율적
		
		1.사용자행동에 주목
			사용자 행동과 목적변수간의 관계가 강한경우가 많음
				사용자 성격,행동특징,행동사이클을 표현하는 특징생성
				이용목적의 클러스터로 나눠생각
				특정 상품에 대한 선호도가 있을지 생각
				같은 물건을 이미 구매한경우 행동저해요소가 없을지 생각
				사용자가 웹사이트에서 어떤식으로 화면을 이동하여 구매했는지 살펴보기
			등이 있음
		2.서비스 제공측의 동향 주목
			서비스를 제공하는측에서 이유가 생기는 경우도 있음
				판매수가0인데,재고가 없었을경우
				휴가,유지보수작업중이었을때 영향
				앱,웹서비스로 검색해서 맨 위에 표시될지 여부와 상관관계 있을특징 생성
				검색이나 리스트박스의 선택사항 고려
		3.업계에서 사용하는 분석방법
			rfm(최신구매일 구매빈도 구매금액)기법으로 사용자 분류와 특징 생성
			신용리스크를 심사할때 어떤 항목이 대상이 될수있을지 관련단어로 조사
			질병진단기준과 관련해 어떤식의 점수책정방법이나 조건분기규칙으로 진단되는지,어떤 특징과의 조합이 고려되는지 조사
		
		4.여러변수를 조합하여 지수작성
			신장+체중으로 bmi를 구하거나 기온+습도로 불쾌지수를 구하는거처럼 여러변수를 조합한 지수를 작성할수도 있음
			
		5.자연현상의 메커니즘
			날씨나 강우량같은건 해당분야의 배경지식으로 특징을 뽑을때가 많음
		6.대상 서비스 이용
			웹사이트 구매량같은건 직접 그사이트가서 동선을 밟아보는게(옥션에서 구매해보는게) 특징뽑을때 도움이 됨
			
		
		즉 배경지식을 토대로 뽑아내면됨(데이터기반으로 배경지식을 쌓아서)
	
	
	2.행 데이터의 관계성에 주목
		각 행 데이터가 독립적일수도 있지만,서로 관계성이 강할수도 있음(같은사용자의 주문내역)
		그러면 그거에 주목해서 특징을 만들수도있음(그 사용자가 제일 많이 산거라든가)


4.모델 구축		
	기본적으로 사용하는 모델은 gbdt,신경망,선형모델정도가 있음
	책기준으로는(2019년)gbdt가 제일 성능 좋았던거같지만 2021년기준으로는 무지성신경망쓰면됨 성능차이 너무나서 결정트리계열 안씀
	
1.모델구축
	모델구축의 흐름은
		모델종류를 선택하고 하이퍼파라미터를 지정(베이스로 대충)
		학습데이터와 레이블을 제공해서 학습진행
		테스트데이터를 제공하여 예측
	순으로 진행됨
	여기서 성능올릴때는
		특징추가및 변경
		하이퍼파라미터 변경
		모델종류 변경
	을 한번예측할때마다 하면됨
	근데 딥러닝의경우엔 특성공학이 별필요없어서 제외하고,파라미터랑 모델종류만 건드리면됨(합성곱이나 층을 추가한다거나)
	여기서 
		학습데이터는 학습데이터의 행수x학습데이터의 특징수
		레이블은 학습데이터의 행수
		테스트데이터는 테스트데이터의 행수x학습데이터의 특징수
		예측값은 테스트데이터의 행수
	
	
	2.검증
		모델 학습데이터를 전부때려넣으면 평가하기 곤란하니까 모델을 조금떼거나,아니면 폴드로 나눠서 검증을 해야함
		홀드아웃(일부를 떼는거)이 쓰기쉽긴한데 학습용데이터가 그만큼 사라지는거니까 아까우면 폴드로 나눠서 교차검증하면됨
		
	3.조기종료
		신경망의경우 조기종료를 넣어주는게좋음
		
	4.배깅
		모델 여러개를 조합해서(병렬로)그거의 최빈값이나 평균으로 예측,같은모델에 난수시드만 바꿔서 날먹치기도함
	5.부스팅
		얘는 직렬로 조합해서 보정하면서 하나씩 모델학습시킴
		
2.GBDT
	얘는 그레이디언트 부스팅 결정트리
	과거에 잘쓰였던 트리방식 선택 머신러닝임
	이진선택을 계속 반복해서 나누고 마지막에 확률값이 있는형식
	
	범주형을 원핫인코딩 안해도된다는게 제일 큰 장점
	
	쓰고싶으면 xgboost쓰면되는데 쓸일이 있을까
	
	쓸일생기면 찾아서 보자 파라미터
	

3.신경망
	보통 완전연결층으로 이루어진 3~4층정도의 은닉층을 가진 구조를 사용함(정형데이터의경우,이미지는 원래쓰던거나 허브에서 가져다쓰고)
	
	얘는 뭐 다 알고있는내용임 
	입력층은 특성을받고
	relu기반 활성화함수 자주쓰이고
	출력층에서 원하는대로 시그모이드나 소프트맥스로 출력치면 알아서 나오는(회귀는 그냥 아무것도안붙이고)
	
	옵티마이저는 adam
	로스는 회귀 mse 이진분류 binary_crossentropy 다중분류 categorical_crossentropy
	1.신경망의 특징
		변수값은 수치로 표현
		결측값을 다룰수없음
		비선형성과 변수간의 상호작용을 반영(특성공학을 안하거나 덜해도됨)
		변수값을 표준화등으로 스케일링(값의 크기가 비슷해야 잘돌아감)
		하이퍼파라미터조정이 전부(특성공학할시간에 층갯수나 파라미터조정해야함)
		다중클래스분류에 강점
		gpu사용가능
		
	2.기타정보
		드롭아웃은 넣으면 과적합방지되니까 좋음
		조기종료는 사용하는게좋음(에포크수 정하기 어려우니까 걍 조기종료에 맞김)
		임베딩층은 첫층으로 입력때만 쓸수있음(원핫인코딩 쓰기뭐하거나 자연어일때 사용하면됨)
		배치정규화는 보통 막 넣어도됨 특수상황말곤(gan생성자)
	
		
4.선형모델
	그냥 뭐 간단히만들수있다는거 말곤 장점이 없어서,특수상황에 사용하는 서브모델로 쓰임
	
5.기타모델
	뭐 알아둘만한거 없어보임 다 본거
	k최근접알고리즘정도는 군집할때 쓰긴했던듯
	
6.기타팁
	특성이 너무 많으면 학습이 안끝나거나 메모리오버로 팅기니까 적당히 가지치기할수있는건 자르자
	원핫인코딩도 좀 심하다싶으면 다른거찾아보고
	
	데이터가 모자라면 유사레이블링으로 테스트데이터를 써서 모델강화하는 금단의방법같은거도 있음
	모델로 테스트데이터 예측하고 그거를 학습데이터에 더해서 다시 학습해서 모델생성하고 테스트데이터 예측
	당연히 테스트데이터에 오버핏확률오르니까 일반적인 방법은 아닌거같고
	
	여기서 확률 50퍼로하면 좀 이상치많이들어가니까 85퍼센트이상인 테스트데이터만 학습데이터에 더하는식으로 모델좀 견고하게 하는게좋대
		



5.모델평가



















