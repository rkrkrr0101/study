kubectl get pods
kubectl get pods --namespace=네임스페이스명
kubectl get pods --all-namespace
kubectl get pods |wc -l 몇줄인지 라인세기,맨위도 포함이니까 -1해줘야함
kubectl get pods | grep 찾을거  grep된거만 표시해줌
kubectl get pods -l 레이블키=레이블밸류  레이블로 검색
kubectl get all
kubectl get pods --selector 키=벨류
kubectl get pods --show-labels 레이블표시
kubectl get pods --no-headers | wc -l  갯수만 표시
kubectl describe pod123
kubectl get event
kubectl top node  리소스사용량보기
kubectl logs -f 파드명 컨테이너명(컨테이너가 2개이상일떄,이거안붙이면 무조건 첫번째컨테이너 출력)
kubectl -n 네임스페이스명  logs -f 파드명 컨테이너명(로그는 파드내 /log/컨테이너명.log에 저장되고,그걸불러옴)

kubectl explain pods --recursive //파드 아래에 속한 모든 필드 설명 보기

kubectl exec -it 파드명 -c 컨테이너명(컨테이너2개이상일때) -- bash (여기에 리눅스명령어 넣으면됨 cat /log/app.log넣으면 저파일 읽기)



kubectl run 파드명 --image=nginx   (파드는 런임 크리에이트안됨)
kubectl run 파드명 --image=nginx --labels=tiers=db 이런식으로 레이블은 넣으면됨
kubectl expose deployment 디플로이먼트명 --name=서비스 이름   기타등등(서비스 특정디플로이먼트에 생성할때)
,포드도 가능,이러면 셀렉터에 포드나 디플로이먼트가 가지고있던 레이블 전부가 들어감
kubectl run 파드명 --image=nginx --port=80 --expose 이런식으로 바로 서비스생성도 가능

당연하지만 yaml에서 위에메타데이터레이블은 디플로이먼트레이블이고 밑에 템플릿밑에 레이블은 파드레이블임

kubectl describe 머시기 | grep -i image나 뭐 찾을거

리눅스 파일실행 ./파일명
쉘스크립트 sh 파일명 으로 실행하면 한번실행되고 맘



kubectl run nginx --image=nginx
kubectl run nginx --image=nginx --restart=never --dry-run=client -o yaml  리스타트네버 넣어야함
kubectl create namespace dev
kubectl create ResourceQuota
kubectl create deployment --image=nginx nginx
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml
kubectl run 파드명 --image=이미지 --dry-run -o yaml > bee.yaml

kubectl run  --image=busybox sbox --dry-run -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml    스태틱파드생성

kubectl create configmap 컨피그맵이름 --from-literal=설정이름=설정값 --from-literal=설정이름2=설정값2
kubectl create secret generic 시크릿이름 --from-literal=설정이름=설정값 or --from-file=파일경로  둘중 선택해서,혹은 둘다 넣을수있음

kubectl apply -f .   현재폴더내 전부 크리에이트


데몬셋은 크리에이트안됨,디플로이먼트로 야믈뽑고 수정해야함

kubectl scale deployment --replicas=5 디플로이먼트명

servicename.namespace.svc.cluster.local

kubectl config set-context $(kubectl config current context) --namespace=dev

노드접근시 curl ip or dns(노드 ssh접근):포트(그노드의 포트접근)

서비스도 셀렉터넣어서 선택

kubectl taint nodes 노드명 key=value:tainteffect(noSchedule(노스케쥴),preferNoSchedule(가능하면),NoExecute(톨러없이실행x))
spec:
	tolerations:
	- key:테인트키값
	  operator: equal(연산자넣는거)
	  value:테인트밸류값
	  effect:테인트이펙트값

테인트 삭제시엔,테인트넣는거 똑같이넣고 마지막에 -붙여서 한번더 날리면됨
그리고 특정노드 배치하는거할땐,톨러레이션은 일반노드에도 배정될수있기때문에 노드어피니티와 같이써야함(노드어피니티만 쓰면 다른애들이 여기들어올수있음)

kubectl label nodes 노드명 키=밸류    노드에 레이블추가

노드어피니티에서 키만 일치시키려면 오퍼레이터에 Exists줘야함

파드 변경시엔,이미지와 액티브데드라인세컨드,톨러레이션말곤 수정할수없어서,yaml로 뽑아서 원본을 삭제시키고 다시넣어야됨
디플로이먼트야 바꾸면 바로 자기가 다삭제하고 다시만듬

메모리 리소스 초과시엔 describe로 보면 last state-reason에 나옴,이벤트에 표시안됨

스태틱파드는 워커노드의 /etc/kubernetes/mainifests 에 yaml을 두면 알아서 읽고 만듬,이건 삭제되도 yaml이 폴더에 남아있으면,계속 유지시킴
이건 기본값이고,kubelet.service에서 pod-manifest-path를 바꾸면 경로바꿀수있음
이거파일 찾을때는 ps -aux | grep kubelet로 config.yaml 경로찾고,거기접속해서(vi)경로보면됨(staticpodpath)
만약 다른노드면 ssh ip 로 거기 접속해서하면됨
그리고 스태틱파드볼땐,api서버가 없으면 docker ps로 볼수있음
api서버가 있으면 get할수는있지만 수정할순없음(파일이 노드에있기때문)
그리고 파드뒤에 노드이름붙어있으면 스태틱파드

커스텀스케줄러할때 스케줄러파드 배포할떈,원본(디폴트)복사해다가 이름만바꾸면됨,그리고 leader-elect=true한애가 리더임(얘가 우선)
즉,스케줄러 추가할때
커맨드밑에
	- --leader-elect=false
	- --scheduler-name=스케줄러이름
	이거넣고 파드 이름 바꿔주면됨
그리고 스태틱파드폴더에 넣으면됨
그리고 파드 스펙밑에(컨테이너랑 같은라인에) schedulerName:스케줄러파드이름 으로 자기가 스케줄링당할 스케줄러를 고를수있음
그리고 k get events 로 어떤스케줄러가 어떤파드를 어떻게했는지 볼수있음
그리고 이전기록은 k logs 스케줄러이름 --name-space=네임스페이스이름  으로 볼수있음  
커스텀스케줄러 디렉토리는 /etc/kubernetes/manifests/(스케줄러는 스태틱파드로 실행됨 )커스텀스케줄러는 자신없으니까 한번더보자

메트릭서버 모니터링은,시험범위 내에선 저장을 못함(프로메테우스같은거있어야함),그래서 k top node,pod같은거로 현재값검색밖에 못함
메트릭서버 설치할땐   git clone 깃주소     하면됨


kubectl rollout status deployment/디플이름  으로 롤아웃상태 볼수있음(롤링업데이트할때 현재상태)
kubectl rollout history deployment/디플이름  으로 디플로이먼트 전단계 볼수있음(롤백할떄쓸거)
kubectl rollout undo deployment/디플이름 히스토리숫자   로 롤백할수있음,히스토리숫자없으면 전거로 롤백함

kubectl set image deployment/디플이름 이미지명=새이미지:태그  으로 명령형이미지변경가능

kubectl drain --ignore-daemonsets 노드명 파드 밖으로 다뿌리고 스케줄러차단
kubectl cordon 노드명 스케줄러만 차단 
kubectl uncordon 노드명 차단해제





describe로 디플로이먼트 배포전략을 볼수있는데(strategyType),
리크리에이트는 전버전을 0으로 줄이고 새걸 그숫자만큼올림
롤링업데이트는 1씩 줄이고 올림(이건 설정할수있음)


cmd는 항상 전체를 바꾸는데(매개변수에 뭘넣어도 전부 바뀜)
entrypoint는 엔트리 포인트에 더해서 뒤에 매개변수가 적힘

기본값을 두고 값만 받아오게하려면
	엔트리포인트 명령어
	cmd 값
을 하면 매개변수없으면 cmd값,있으면 cmd를 대체함

엔트리포인트를 오버라이드하고싶으면,
	docker run --entrypoint 대체명령어 이미지 매개변수
이렇게 하면됨


쿠버네티스에서 매개변수를 줄땐
	spec:
		containers:
		-  name: 이름
		   image: 이미지
		   args:['매개변수값']
을 하면됨
엔트리포인트를 오버라이드할땐
	spec:
		containers:
		-  name: 이름
		   image: 이미지
		   command: ['명령어']
		   args:['매개변수값']
하면됨
만약 도커파일에 엔트리포인트가 있는데,커맨드쓰면 다 덮어씌워지니까 주의

컨테이너속 앱에 매개변수줄땐
	spec:
		containers:
		-  name: 이름
		   image: 이미지
		   env:
		   -  name: 변수이름
		      value: 변수값
넣으면됨
그러면 앱에서 이걸 가져다가 쓸수있음,그리고 직접 값을 하드코딩하는대신 valueFrom으로 값을 어디서 가져올수도 있음(컨피그맵이나 시크릿)
컨피그맵을 파드에 넣고,그걸 컨테이너에 삽입하고,이름찾으면 거기서 찾는식
	spec:
		containers:
		-  name: 이름
		   image: 이미지
		   envFrom:  (여기에 -가 컨피그맵은 있고 시크릿은 없었음)
		   -   configMapRef:   //시크릿은 secretRef 넣으면됨 똑같이씀
				 name:컨피그맵이름(미리만들어둔거)
or
		   env:
		   -  name: 변수이름
		      valueFrom:
				configMapRef:
					name:컨피그맵이름
					키:컨피그맵에있는키
or
		volumes:
		-  name:볼륨명(지금붙인거)
		   configMap:
			  namle: 컨피그맵이름



시크릿에서 사용할땐 base64로 저장되니까,이걸 해독해줘야함
리눅스상에선
	echo -n 'gdfgd==' | base64 --decode

그리고 시크릿 desc로 봤을때 데이터밑에 있는게 각각 하나의 default-token임
왠진모르겠는데 시크릿은 이미지랑 이름 밑에 생성해야지,컨테이너 맨위에 생성하면 에러뜸(아 이미지에 속해있어야되나봄 이미지부터 행렬시작이니까)

initContainers는 그밑에 여러 컨테이너를 가질수있고(-배열로 이름,이미지를 하나씩 포함한),그러면 위부터 순서대로 실행하고,실패하면 파드를 다시시작함
그리고 실행된후에 종료되니까,컨테이너갯수에 포함안됨(get으로보는거)

노드아웃됐을때 타임아웃 세팅은 kube-controller-manager --pod-eviction-timeout=3m0s 로 설정가능 (기본값5분)
저 타임아웃시간내에 다시 가동가능하면, 그안에 업그레이드나 재부팅을 해도됨,근데 확신못하면 미리 다 뿌려두고 여기배치못하게 막고 업뎃하는게 안정적
kubectl drain 노드명 --ignore-daemonsets 으로 현재그노드 파드 전부 다 밖으로 뿌리고 스케줄러차단
드레인은 레플리카셋이 아닌 개별로 생성된 파드는 지우지못함(지우려면 뒤에 --force 붙이면되는데,이러면 파드소실됨)

노드 업그레이드는 kubeadm upgrade apply v1.13.4 이렇게 하면됨(클러스터는 이렇게 안하고 자체적으로 지원함)
현재 최신버전보려면 kubeadm upgrade plan
업그레이드는 한번에 한버전씩 하는게 좋음(10에서 13으로 건너뛰지말고 11,12,13이렇게)
다운타임 있어도되면 한번에업ㄱ글,없어야하면 하나씩업글,클러스터면 그냥 새로 노드추가하고(새버전으로)구버전노드 삭제
kubelet 업그레이드는 apt-get upgrade  kubelet=1.12.9.00 하면 됨(마스터노드)
그리고 systemctl restart kubelet 하면 재시작되고 업글됨

워커노드는 마스터에서 그노드 드레인하고,그노드 접속해서 똑같이 업글하면됨(ssh),업글끝나면 드레인 해제(uncordon)

kubeadm 마스터노드 업데이브방법
	apt update
	apt install kubeadm=1.20.0-00
	kubeadm upgrade apply v1.20.0
	apt install kubelet=1.20.0-00
	systemctl restart kubelet

워커노드 업데이트
	드레인하고
	ssh접속
	apt update
	apt install kubeadm=1.20.0-00
	kubeadm upgrade node
	apt install kubelet=1.20.0-00
	systemctl restart kubelet
	exit

즉 adm을 업데이트하고 그거로 노드를 업데이트하고 쿠버렛을 업데이트하면됨



kubectl get all --all-namespace -o yaml >백업.yaml 로 현재실행중인것들을 백업할수있음
물론 상용에선 백업도구를 사용함(velero같은)
etcd는 etcd.service에 있는 data-dir=/var/lib/etcd 에 저장됨
etcdctl snapshot save 파일명.db 로 etcd 백업할수있음
etcdctl snapshot status 파일명.db 로 상태볼수있고
//친건 시험에선 안해도됨  클러스터메인터넌스는 한번더해보자 잘못하겠음
//////////////service kube-apiserver stop 로 api서버 멈추고
etcdctl snapshot restore 파일명.db --data-dir etcd경로     로 리스토어할수있음(/var/lib/etcd-from-backup)
겹치는거 방지위해서,새로 경로를 지정해야함,겹쳐도되긴함 원래자리랑 혹시 겹쳐서 파일날아갈까봐 이렇게해둔거
그리고 경로바뀌었으면, 
/etc/kubernetes/manifests 이런 스태틱파드자리가서 볼륨(volumes,맨밑에있을거임)의 etcd위치(etcd-data)를 바꾼거로 바꿔줘야함

////////////systemctl daemon-reload   로 서비스데몬 재시작하고
//////////////service etcd restart    로 etcd 리스타트하고
//////////////service kube-apiserver start 로 api서버 재시작하면됨

그리고 인증서파일을 쓰면,
snapshot save 할때
	--endpoints=https://ip:포트(127.0.0.1:2379),etcd는 마스터노드에서 실행되고(로컬호스트) 포트기본값은 2379임 listen-client-urls
	--cacert=ca.crt파일경로,tls보안인증서  ,trusted-ca-file
	--cert=etcd-server.crt 경로,tls보안클라이언트식별  cert-file
	--key=etcd-server.key 경로,보안클라이언트 식별 키  key-file
지정해주면됨

etcd파드에서 cert-file는 서버인증서,peer은 클라이언트인증서
etcd ca 인증서는 trusted-ca-file

이렇게 백업방식은 두가지가있음 
etcdctl을 쓰기 전에
export ETCDCTL_API=3 을 날려서 etcd버전을 3으로 설정해줘야함(백업및 복원은 3에있음)

시험칠땐 하고나서 describe로 다시 확인하래


static password방식은 kube-apiserver.service의 --basic-auth-file=파일경로 로 파일이 저장되어있고,
거기에 패스워드 이름 아이디 그룹(옵션)이 저장됨(추가할땐 execstart밑에 더하면됨,그리고 서버재시작)
그리고 파드 커맨드에 kube-apiserver 밑에 -- --basic-auth-file=파일경로 를 넣으면 거기들어간애만 파드가 받아들임

그리고 암호대신 토큰을 가지면,static token방식
이떈 --token-auth-file=경로  를 대신넣으면됨
그러면 토큰을 받아서 확인하고 맞으면 들여보냄

보통 이건 잘 안쓰긴하는데,기본이래(요즘쿠버네티스에선 아예 사용불가래)

쿠버네티스엔 
	api서버용 인증서(공개키),비밀키쌍(서버)
	api서버->etcd용 인증서(공개키),비밀키쌍(클라이언트)
	etcd용 인증서(공개키),비밀키쌍
	워커노드의 kubelet용 인증서(공개키),비밀키쌍
	api서버->kubelet용 인증서(공개키),비밀키쌍
	스케줄러의 인증서(공개키),비밀키쌍
	컨트롤러매니저의 인증서(공개키),비밀키쌍
	쿠베프록시의 인증서(공개키),비밀키쌍
	ca기관의 인증서(공개키)(비밀키는 안가지고있음 우리가)
이 있음
여기서 스케줄러는,성질상 사용자와 같은취급임

나누면,
etcd용 인증서,api서버용 인증서,워커노드의 kubelet용 인증서가 서버역할이고,
나머지는 클라이언트역할임

그리고 etcd서버와 통신하는 etcd용 인증서,api서버->etcd용 인증서는 자체서명된 인증서를 사용함

여기서 워커노드,스케줄러,컨트롤러매니저,쿠베프록시,각 롤(접속한사람)은 전부 api서버와 통신하고,etcd는 유일하게 api서버와만 통신함
이떄 etcd가 서버역할이라서,클라이언트인 api서버는 인증이 필요함

그리고 각 롤마다 인증서(공개키),비밀키쌍 이 있음

ssl로 키를 만들려면
	openssl genrsa -out ca.key 2048
이렇게 만들수있음
그리고 그 키를 사용해서 csr(서명없는 인증서)를 만들수있음
	openssl req -new -key ca.key -subj '/CN=KUBERNETES-CA' -out ca.scr
그리고 그 파일과 키를 사용해서 (crt)서명된 인증서를 만들수있음
	openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
	
클라이언트에서 인증서를 만들때도 비슷하게 하면됨
	openssl genrsa -out admin.key 2048
	openssl req -new -key admin.key -subj '/CN=kube-admin/O=system:masters' -out admin.scr
	//cn은이름에대한메모,막적어도됨,로그에서 이게표시됨 O는 무슨그룹에 속했는지
	openssl x509 -req -in admin.csr -Ca ca.crt -CAkey ca.key -out admin.crt
여기서 ca.crt와 ca.key를 사용해서 어드민crt를 생성하는거만 다름(tls랑 같이 어드민에서 공개키비밀키를 두고 그걸 암호화해서 보내는거)

인증서는 id,비밀키는 패스워드라고 생각하면 됨
그리고 인증서 서명에,그룹 세부정보를 입력할수있고 이걸기반으로 사용자를 그룹별로 나눌수있음

이걸 모든 오브젝트에 다 해줘야함(스케줄러,컨트롤러매니저,쿠브프록시)

인증서를 사용하는방법은
	curl https:ip:포트  --key admin.key --cert admin.crt --cacert ca.crt
하면됨
이게 귀찮으니까
kubeconfig에 이걸 기록해두고,모든 명령에 자동으로 적히게 할수도있음
얘는 kind가 Config이고
api서버 엔드포인트,세부정보,인증서들을 지정할수있음

그리고 모든 오브젝트엔 ca.crt가 복사되어서 들어가 있음

서버도(etcd서버) 똑같이 만들면됨,이때 고가용성을위해 복사노드들이 있으면,서로 통신하기위한 인증서가 또 필요함
이건 메인을 서버로,나머지를 클라이언트로 하는 인증서임
이건 etcd.yaml에 있음(peer-cert-file,peer-key-file)
그리고 거기엔 key-file,cert-file도 있고,trusted-ca-file도 있음

api서버용 인증서도 비슷하게 만드는데
	openssl genrsa -out apiserver.key 2048
	openssl req -new -key apiserver.key -subj '/CN=kube-apiserver' -out apiserver.scr-config openssl.cnf
	
	openssl x509 -req -in apiserver.csr -Ca ca.crt -CAkey ca.key -out apiserver.crt

openssl.cnf는 api서버의 dns이름과 모든 ip를 적어둔 파일,인증서만들땐 모든 dns를 포함해야하니까 넣어준거

이 파일의 위치는 kubeapiserver파일의(/etc/systemd/system/kube-apiserver.service,kubeadm은 스태틱포드의 apiserver.yaml) 
	client-ca-file에 ca파일이 있고
	tls-cert-file에 apiserver crt파일
	tls-private-key-file에 apiserver.key파일

	etcd-cafile에 ca파일
	etcd-certfile에 api클라이언트-etcd crt
	etcd-keyfile에 api클라이언트-etcd key

	kubelet-certificate-authority에 ca파일
	kubelet-client-certificate=에 api클라이언트-kubelet crt파일
	kubelet-client-key=에 api클라이언트-kubelet key파일
이 있음

kubelet(kubectl)은 모든 노드에(워커마스터포함) 각각 자신의 kubelet마다 자신의 인증서를 가지고있음(자기노드이름을 붙인,system:node:노드명)
kubelet-config.yaml파일의
	clientCAFile을 보면 ca파일위치
	tlsCertFile을 보면 crt위치
	tlsPrivateKeyFile을 보면 key위치
를 알수있고,바꿀수있음(파일위치를)
kubelet의 인증서엔,그룹을 O=SYSTEM:NODES 해야함

kubeadm이 만든거의 인증서 확인 위치
	apiserver /etc/kubernetes/manifests/kube-apiserver.yaml 에서 
	ca,etcd,api-etcdclient,api-kubeletclient,apiserver 확인가능

인증서를 확인할땐
	openssl x509 -in crt파일 -text -noout
하면 됨
	subject는 인증서이름
	x509v3 subject alternative name은 dns명이나 자신의 다른ip
	validity의 after은 인증서만료날짜
	issuer은 인증서 발급자

가장 중요한건,올바른 발급자가 발급하고,인증서가 만료되지 않았는지를 확인하면됨(특히 인증서만료로 문제생기는게 젤많음)

직접 구성하면
	journalctl -u etcd.service -i
에서 crt파일을 찾아야함

그리고
	kubectl logs etcd-master
로 cert,key 위치와 뭐땜에 문제생겼는지를 볼수있음(모든 파드에서 다 됨)

api서버나 etcd가 다운되면
도커에서
	docker ps -a 로 프로세스id확인하고
	docker logs 프로세스id
로 로그확인할수있음

그리고 알수없는 서명이라고 뜨고 클러스터다운되면,보통 etcd가 api서버와 다른 ca를 사용하는데,이게 api서버쪽에서 ca가 잘못설정된거니까
etcd.yaml에 있는 ca 가져와서 api서버의 etcdca에 붙여넣으면됨


기본적으로 로컬일경우,마스터노드가 ca서버역할을 같이함
그리고 상용일경우,인증서가 만료되기전에 자동으로 인증서를 교체하는 자동화가 되어야함
이때 사용하는게 certificates api

인증서 갱신 요청을 날리면 마스터노드는 CertificateSigningRequest 오브젝트를 생성하고
이거 관리자가 보고 승인할수있음(kubectl로)

이때 순서는
	openssl genrsa -out user.key 2048
	openssl req -new -key  user.key -subj '/CN=user' -out  user.scr
	CertificateSigningRequest yaml 생성
		apiVersion: certificates.k8s.io/v1
		kind: CertificateSigningRequest
		metadata:
			name: 이름1
		spec:
			signerName: kubernetes.io/kube-apiserver-client
			groups:
			-  system:authenticated  //무슨그룹들어갈건지
			usages:
			- client auth  //요청문자열,뭘원하는지 무슨그룹들어갈건지
			request:
				베이스64포멧한 자신의 csr파일(base64 파일명.csr하면 됨)
				
	kubectl get csr로 확인하고
	kubectl certificate approve 이름1 로 승인 or  kubectl certificate deny 이름1 로 거절
	kubectl get csr 이름1 -o yaml 로 추출해서 certificate항목을 주면됨(이때 base64로 암호화됐으니까 base64 --decode로 해독해야함)
	echo '내용' |base64 --decode
이 모든 인증서작업은 컨트롤러매니저가 함
그래서 컨트롤러매니저의 yaml을 보면 ca인증서와 키가 있음(인증서 제대로못주면 얘보면됨)


기본적으로 클라이언트는 모든요청에 전부 키,자기crt,ca.crt를 보내야함
	kubectl get pods --server ip:포트 /
	--client-key 클라.key --client-certificate 클라.crt --certificate-authority ca.crt

근데 머리아프니까 이걸 kubeConfig에 넣어둠(#HOME(환경변수 HOME)/.kube/config(얘가파일임))
export로 리눅스에서 환경변수 확인가능
	--kubeconfig config
	
kubeconfig파일은 클러스터,컨텍스트,유저 섹션으로 나눠져있음
클러스터는 말그대로 클러스터임,개발클러스터 프로덕션클러스터 이런식으로 아예 분리를 해두는거
유저는 그 유저의 권한이라고 보면됨
컨텍스트는 어떤 클러스터에 그 유저가 권한이 있는지를 나타냄

저 위의 서버ip:포트는 클러스터섹션,crt랑 키는 user섹션에 적어짐
그리고 컨텍스트에서 그거 두개를 바인딩한걸 들고있음(키밸류로)
얘도 yaml형식으로 apiVersion은 v1이고 kind는 Config임



당연히 클러스터 컨텍스트 유저는 여러개될수있음
그리고 커런트컨텍스트가 기본값이고,저걸 바꿔서 다른컨텍스트로 넘어갈수있음

현재 kubeconfig를 보려면
	kubectl config view 
특정파일을 열려면
	kubectl config view --kubeconfig=파일경로
현재컨텍스트 바꾸려면
	kubectl config use-context 컨텍스트명
	ex) kubectl config --kubeconfig=/root/my-kube-config use-context research
하면됨
그리고 certificate-authority-data필드를 쓰면 파일경로말고 base64된 파일데이터를 그대로넣을수도 있긴함



kubectl을 쓰지않고 똑같이 하려면
	curl https://kube-master:6443/version
	curl https://kube-master:6443/api/v1/pods  //k get pod
하면됨
/api밑에 있는게 코어고(파드 노드 네임스페이스 서비스등)
/apis 밑에있는게 아직 코어로(메이저로) 안들어온애들이나 논리적으로 존재하는애들 (certificates.k8s.io,디플로이먼트,네트워크정책 등)
공식페이지의 pod v1 core를 검색하면 상세한게나옴,그리고 v1/core는 v1임
이렇게할땐 컨피그파일의 값을 못가져오기때문에 key랑 cert랑 cacert를 직접넣어줘야함 --ca 이런걸로

그리고 각 리소스들에는 동사라고 하는 작업집합이 있음(get list delete 이런거)

kubeproxy와 kubectlproxy는 같지않음
kubectlproxy는 kubectl이 액세스하려고 생성된 프록시(얘가 명령받아서 저 kube-master어쩌구로 바꿔줌)
kubeproxy는 쿠버네티스자체에서 외부에서 받을때 쓰는 프록시

권한부여할땐 네임스페이스로 나눠서 걔가 쓸거만 최소한으로 주는게좋음
보통 rbac를 쓰는데 권한을 만들고 그걸 사람(그사람의 인증서)에 바인딩하는거
그리고 보통은 외부도구를 사용해서 권한을 관리함

권한선택은 kube-admin에 --authorization-mode에 적혀있고 수정할수있음
rbac말고 AlwaysAllow와 AlwaysDeny가 있는데 이건 항상승인과 항상거부
두개이상 쓸수도있는데 --authorization-mode=RBAC,webhook,Node 이렇게 하면됨,그러면 앞에서부터 순서대로 인식함

rbac의 롤도 yaml로 만들면됨
	apiVersion: rbac.authorization.k8s.io/v1
	kind:Role
	metadata:
		name: 롤이름
		namespace: 지정네임스페이스
	rules:
	-  apiGroups: [""]  //비워두면 코어그룹임   그냥 ''할땐 -붙여서 배열표시해주고 아니면 ['']할땐 안붙여야함
	   resources: ['pods']
	   verbs: ['get','list','delete','update']
	   resourceNames: ['blue'] //옵션,파드중에서 blue만 접근가능하게
	-  apiGroups: 
	   -  ""  //비워두면 코어그룹임
	   resources: ['ConfigMap']
	   verbs: ['get','create','delete','update']
	-  apiGroups: ["apps",'extensions']  //비워두면 코어그룹임
	   resources: ['deployments']
	   verbs: ['get','create','delete','update']	   
	   

이걸 롤바인딩으로 바인딩하면됨
	apiVersion: rbac.authorization.k8s.io/v1
	kind:RoleBinding
	metadata:
		name: 롤바인딩이름
		namespace: 지정네임스페이스
	subject:
	- kind: User
	  name: 유저이름
	  apiGroup: rbac.authorization.k8s.io/v1
	roleRef:
		kind: Role
		name: 롤이름(지금생성되어있는)
		apiGroup: rbac.authorization.k8s.io/v1


내가 그 권한이 있는지 알려면
	k auth can-i 명령(get pods)
하면 yes no 로 리턴옴
그리고 걔가 권한있는질 알려면 
	k auth can-i 명령(get pods) --as 유저이름
하면 걔의 권한으로 가능한지 알수있음
	k auth can-i 명령(get pods) --as 유저이름 --namespace 네임스페이스명
으로 네임스페이스까지 확인가능

그리고 네임스페이스 여러개써야할땐 한롤로 여러개하는거보다 롤 여러개만드는게 더 편함,아니면 클러스터롤쓰던가

노드는 클러스터롤에서만 사용할수있음,특정네임스페이스에 포함되지않음
클러스터롤에서 포함되는건
	노드,pv,네임스페이스들,certificatesigningrequests등

클러스터롤은 롤이랑 똑같이 만들면되고 kind랑 리소스만 다름(node나 pv같은거 넣을테니까)
클러스터롤바인딩도 똑같음
그리고 클러스터롤에 pod같은거 넣으면,모든네임스페이스의 pod에 접근가능함

바인드된 유저/그룹을 찾으려면 롤바인딩에서 kind보면됨




유저어카운트는 사용자가 쓰는거,서비스어카운트는 컴터가 쓰는거
프로메테우스 젠킨스등 얘들은 서비스어카운트를 사용함

서비스어카운트를 생성하면 서비스어카운트 토큰도 같이 자동으로 생성됨(시크릿으로 생성됨)

즉
	서비스계정생성
	시크릿객체생성
	서비스계정에 시크릿객체 연결
	rbac 서비스계정에 연결
그리고 이 토큰을 서로 통신할때 사용함
즉 앱에서 쿠버네티스랑 통신을 해야할땐,이 시크릿의 토큰을 가져가서 curl의 
	curl https:ip포트/api -insecure --header "Authorization:  Bearer 토큰내용"
으로 통신하면됨(즉 토큰이 저사람이라는걸 확인함)
토큰은 서비스어카운트를 desc하면 밑에 토큰이름나오는데,그 시크릿을 다시 desc하면 나옴


그리고 쿠버네티스 내에서 파드나 디플로이먼트로 만들경우엔,그냥 시크릿을 볼륨으로 마운트해서 쓰면됨(이거도 자동으로된대)

각 네임스페이스엔 자동으로 생성되는 서비스어카운트가 있고,
쿠버네티스를 설치하면 기본으로 default가 있고 이건 모든 네임스페이스에 접근할수있음

클러스터내에서 파드를 생성할경우엔,자동으로 현재사용자의 토큰볼륨을 자동으로 마운트함(기본값으로 디폴트의 서비스어카운트를 넣음)
그리고 그 적힌곳을 직접가면 ca.crt랑 토큰이랑 네임스페이스가 있음

만약 파드가 다른 서비스어카운트를 쓰고싶으면 컨테이너랑 같은라인에
	serviceAccountName: 서비스어카운트이름
을 쓸수있음
이경우에 파드의경우 새로 생성해야함(apply로 덮어씌우기안됨),디플로이먼트는 상관없지만
그리고
	automountServiceAccountToken:false
로 서비스어카운트토큰자동마운트를 막을수도 있음


기본적으로 이미지앞에 /가 없으면 도커허브(library)로 인식함 nginx=docker.io/library/nginx  (레지스트리/유저네임/이미지이름)
내이미지를 쓰려면 도커허브에 올리거나 사설레지스트리를 만들어야함
도커허브말고도
	gcr.io  구글
등이있음

얘들은 다 퍼블릭인데,기본적으로 클라우드를 쓰면 비공개레지스트리를 제공함
그리고 도커허브도 비공개 레지스트리가 있는데,여기 접근하려면
	--docker-server=
	--docker-username=
	--docker-password=
	--docker-email=
	
	kubectl create secret docker-registry private-reg-cred  //이거가 공식페이지에 있으니까 이거쓰자그냥,즐찾해둠
	--docker-username=dock_user 
	--docker-password=dock_password 
	--docker-server=myprivateregistry.com:5000 
	--docker-email=dock_user@myprivateregistry.com
	
yaml생성시	
타입은 type: "kubernetes.io/dockerconfigjson",데이터랑 같은라인
이 있는 시크릿을 파드에서 제공해야함,컨테이너랑 같은위치에 imagePullSecrets를 넣으면됨
	imagePullSecrets:
	-  name: 시크릿명
이거하면 디플로이먼트도 지웠다가 다시해야함 apply안됨


kubectl create secret -help 를 보면 사용가능한 레지스트리 보안 방식이 나옴(도커레지스트리,generic,tls등)




쿠버네티스에선 컨테이너수준보안과 포드수준 보안을 할수있는데,파드수준으로 하면 파드내 모든 컨테이너에 적용됨,둘다있으면 파드설정이 덮어씌워짐(컨테이너가 이김)
파드단위로 설정하려면 스펙밑 컨테이너랑 같은라인에 securityContext:를 넣으면됨(파드레벨)
	securityContext:
		runAsUser:1000
컨테이너단위로 하려면 컨테이너 밑에 네임이랑 같은라인에 적으면됨
	- name: nginx
	  image: nginx
	  securityContext:
		runAsUser:1000  //루트면 걍 지우면됨
		capabilities:     //이건 컨테이너단위만 적용됨
			add: ["MAC_ADMIN"] 


network policy는,기본적으로 쿠버네티스는 모든노드가 서로서로 통신이 가능한데,이걸 특정 파드만 나한테 통신을 걸수있게 차단하는것(포트로 차단함)
이건 레이블과 셀렉터로 차단할수있음,그리고 수신,송신 둘다 차단하거나 허용하거나 하나만 하거나 할수있음


	apiVersion: networking.k8s.io/v1
	kind: NetworkPolicy
	metadata:
		name: 정책이름
	spec:
		podSelector:
			matchLabels:
				레이블키: 레이블밸류  //이거 씌울 레이블,즉 보호받을애
		policyTypes:
		-  Ingress  //ingress랑 egress ,수신과 송신 두개잇음 
		-  Egress  //둘중 하나나,둘다넣어도됨
		ingress:
		- from:
		  -  podSelector:
			    matchLabels:
				  레이블키2:레이블밸류2  //이 레이블이 있는
			 namespaceSelector:                //이거빼면 클러스터 전체에서 접근가능하고,넣으면 네임스페이스안에서만 접근됨
				matchLabels:
					레이블키3:레이블밸류3 
		  -  ipBlock:
				cidr: 192.168.5.10/32         //얘는 클러스터 외부에서 접근할때 사용함,저ip범위는 허용하는거
		  ports:
		  - protocol: TCP
		    port: 3306      //나 자신의(보호받는애의) 이 포트만 허용됨
		  - protocol: TCP
		    port: 8080 
		egress:
		- to :
			...  //위랑 똑같이 적으면됨
		  ports:
		  - protocol: TCP
		    port: 80      //저 포트로 던지는거만 허용됨,즉 80포트로밖에 못던짐

	
	그리고 to나 프롬이 여러개와도됨,그게 보기쉬우면 그렇게해도됨(저렇게 or로 안하고 그냥 따로표기하는거임,결과는같겠지만)
	즉 
	 -to
	  - 조건들
	  ports
	 -to
	  - 조건들
	  ports	 
	이렇게되도됨
	  
		

그리고 인그레스를 허용하면,거기에 대한 리턴은 이그레스가 아닌,인그레스만으로 됨(tcp는 서로 세션열어서 하는거라서 그거까진허용됨)
즉,처음에 어떤방향으로 갈지(수동적으로 받을지 능동적으로 내가 먼저 걸지)를 가지고 인그레스 이그레스를 정하면됨

네임스페이스를 쓰려면 당연히 네임스페이스에 레이블이 있어야함
그리고 파드셀렉터가 없고,네임스페이스셀렉터만 있으면,그안에 있는 모든 파드가 거기 접근가능함(물론 포트는 맞춰야함),외부에선 접근불가

그리고 보면 알겠지만,파드셀렉터랑 네임스페이스 셀렉터는 같은배열에 들어가있으니까 둘다만족해야하고
ipblock는 따로있으니까 위에서 튕겨내면 얘가 받아서 확인하고 ok주는거임,즉 위랑 아래는 or임
저기서 네임스페이스셀렉터도 분리할수있는데 -로,그러면 파드에서 보고 아니면 네임스페이스에서 보고 아니면 블록에서 보는거,전체 or됨
즉 포함되면 and 아니면 or

이건 kube-router,calico,romana,weave-net은 지원하는데 flannel은 지원하지않음
그리고 flannel에서도 만들어지고 오류메세지도 안뜨는데,적용은안됨

기본적으로 도커에선 컨테이너 스토리지에 저장된 정보들은 컨테이너가 사라지면 같이사라짐
그리고 컨테이너는 이미지레이어와 컨테이너 레이어로 나눠지고,(c,d디스크라생각하면됨)이미지레이어는 리드온리,컨테이너는 rw둘다허용
만약 이미지레이어에있는거 바꿔야하면,컨테이너레이어로 복사해서 그걸바꿔서 쓰는건 허용됨
영구볼륨이 필요하면 docker volume create 볼륨명으로 만들고 docker run -v 볼륨명:/경로 로 컨테이너에 삽입가능
이거 뭐 다 본건데 할필요없겠지,그 경로지정해서 그땅 먹을수도있고,볼륨지정되고 그거 겹치는거 다사라지게할거냐 그런거잇던거
볼륨으로하면 내부데이터가 덮어쓰기안되면 살아있고,바인드마운트하면(경로) 내부 다사라지고 초기화됨
그리고 docker run 할때 --volume-driver 드라이버명 으로 드라이버를 선택할수도있음(클라우드같은거쓸때,근데 뭐 쿠버네티스는 자동처리하겠지)

쿠버네티스단에선,도커든 뭐든 외부로 나가서 cri를 거쳐서 작동하기때문에 신경안써도됨 로우레벨은
그냥 cri쪽에 명령던지면 드라이버가 알아서함 볼륨 삭제든 생성이든

볼륨만들땐(pv말고 그냥볼륨) 스펙밑 컨테이너랑 같은라인에
	volumes:
	-  name: 볼륨명1
	hostPath:
		path: /data  //실제 데이터위치 경로
		type: Directory
로 생성하고
컨테이너에
	volumeMounts:
	-  mountPath: /opt  //컨테이너내에서 사용할 경로(바로가기라고보면됨)
	   name:볼륨명1       //사용할 볼륨명

이렇게하면,단일노드에선 상관없는데 다중노드에선 서로 다른데를 가리키고,어떻게 파드가 겹칠지 예상할수없기떄문에 사용못함
그래서 외부에있는 스토리지를 사용하는데(아마존 스토리지같은)보통 클라우드에 다 있음

	volumes:
	-  name: 볼륨명1
	   awsElasticBlockStore:
	     volumeID:볼륨id(스토리지id)
		 fsType:ext4  //저장타입


이렇게 주먹구구식으로 안하고,볼륨을 잔뜩만들어두고 제일 적당한걸 가져가는식으로 하는게 pv/pvc임
pv yaml은
	apiVersion: v1
	kind: PersistentVolume
	metadata:
		name: pv이름
	spec:
		accessModes:
		-  ReadWriteOnce    //ReadOnlyMany,ReadWriteMany도 있음,말그대로 읽고쓰기 동시에할수있는 사람 수
		capacity:
			storage: 1GI
		hostPath:          //이건 클라우드상황에선 사용되지않음,위에썻던 아이디값으로 대신함 awsElasticBlockStore이거
			path: /tmp/data
		persistentVolumeReclaimPolicy:Retain//delete나 retain이나 그런거 선택

pvc가 생성되면 가장 요구사항(용량,액세스모드)이 비슷한 pv에 바인딩되고,파드는 그걸 가져다가 씀
그리고 pvc에서 원하는 pv에 바인딩을 하고싶으면,레이블셀렉터로 바인딩을 직접 할순있음
그리고 용량은,pvc가 적어둔거보다 크게줄순있는데 작게는 안줌

pvc와 pv는 1대1관계이므로,다른클레임에선 그걸 사용할수없음
만약 사용가능한 pv가 없거나,용량이 작으면,생길때까지 pending함

pvc yaml은
	apiVersion: v1
	kind: PersistentVolumeClaim
	metadata:
		name: 클레임명
	spec:
		accessModes:         //ReadOnlyMany,ReadWriteMany도 있음,말그대로 읽고쓰기 동시에할수있는 사람 수
		-  ReadWriteOnce
		resources:
		  requests:
		    storage: 500Mi
			
pv랑 pvc는,액세스모드는 무조건 일치해야함
그리고 pvc가 삭제되면,pv에 어떤작업을 할지(포멧할지 그냥둘지) 정할수있는데
	persistentVolumeReclaimPolicy: 
		Retain(그대로둠,수동으로 삭제해야하고 다른pvc가 사용할수없음)
		Delete(자동삭제)
		Recycle(그대로두는데,다른pvc가 사용할수도있음)

파드에서 pvc쓸때는(pvc만 쓰면됨 바인딩되어있으니까)
컨테이너밑에
	volumeMounts:
	-  mountPath: /opt  //컨테이너내에서 사용할 경로(바로가기라고보면됨)
	   name:볼륨명1       //사용할 볼륨명
하고
컨테이너랑 같은라인에(스펙밑에)
	volumes:
	-  name: 볼륨명1
	persistentVolumeClaim:
		claimName:클레임이름
으로 쓰면됨,클레임이름으로 로컬볼륨명1에 바운딩하고,그거가져다쓰면됨

pvc는 apply덮어쓰기안되고 지우고다시만들어야함
그리고 파드에서 사용중일경우엔 삭제되지않음(터미네이팅무한대기)
retain시에 pvc 삭제하면 pv상태는 released되는데 이게 현재 데이터남아있고 다시배정안받는다는거

클라우드쓸때 
pv생성전에 클라우드에서 먼저 디스크를 생성하는걸 정적 프로비저닝 볼륨이라고 함
그리고 포드(앱)에서 요구할때 자동으로 볼륨이 프로비저닝되는걸 동적프로비저닝볼륨이라고 함

동적 프로비저닝을 할땐 storageClass를 써야함
yaml은
	apiVersion: storage.k8s.io/v1
	kind:StorageClass
	metadata:
	  name:스토리지클래스명1
	provisioner: kubernetes.io/gce-pd //구글
	volumeBindingMode:WaitForFirstConsumer //WaitForFirstConsumer 는 파드가요청하면,immediate이면 바로연결
	parameters: //각 회사에서 특정 조건을가진걸 배당해달라고 요구할수있음,이건 회사따라 다름,적힌건 구글
		type: pd-standard        //pd-standard,pd-ssd 등있음
		replication-type: none  //none,regional-pd 등있음,볼륨의 위치 지정,리저널이면 비쌈
이렇게되면 pv는 필요없고,pvc에서 
	spec: 
		storageClassName:스토리지클래스명1

해주면,pvc가 파드에 연결될때 자동으로 볼륨이생성됨,그러면 딱 요구한사이즈만큼 생성하고 바인딩함
정확히는 pv를 만들필요가 없다뿐이지,스토리지클래스가 자동으로 pv를 생성하는것
이런 스토리지 클래스를 여러개만들어서,각기 다르게 배급하는거도 가능해짐 파라미터덕분에(속도필요한애는 ssd주고 그런식)

프로비저너가 kubernetes.io/no-provisioner 이면 동적프로비전을 제공하지않음(도큐참고)
그리고 정적 프로비저닝할때도 스토리지클래스로 생성했으면 그쪽으로 연결시켜줘야함(같은 스토리지클래스끼리만 만날수있음)
그리고 스토리지클래스의 volumebindingmode가 waitforfirstconsumer이면 파드생기기전까진 pending이고,immediate이면 바로 연결함










yaml로 생성시 api버전
	pod:v1
	service:v1
	replicaset:apps/v1
	deployment:apps/v1
	
	
apiVersion: v1
kind: Pod
metadata:
	name: pod123
	namespace: dev
	labels:
		app: myapp
spec:
	containers:          //initContainers는 그냥 멀티컨테이너처럼 쓰는데,컨테이너앞에 init붙이고 그대로쓰면됨 C대문자에 주의 
		- name: con123
		  image: nginx   //여기까지 기본
		  ports:
			- containersPort: 8080
		resources:                  //파드에서 직접쓸땐,그 네임스페이스에 LimitRange를 만들어둬야 쓸수있음
			requests:
			  memory: '1gi'
			  cpu: 0.5
			limits:                   //cpu는 초과시 쓰로틀,메모리는 초과시 재시작
			  memory: '3gi'          
			  cpu: 1.5

apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: repl123
	labels:
		app: repl
spec:
	replicas: 3
	selector:
		matchLabels:
			app: podre1
	template:
		metadata:
			name: pod123
			labels:
				apps: podre1
		spec:
			containers:
				- name: pod123
				  image: nginx

apiVersion: v1
kind: Service
metadata:
	name: pod123
	namespace: dev
	labels:
		app: myapp
spec:
	type:LoadBalancer
	ports:
		- targetPort: 80
		  port: 80
		  nodePort: 30008		  

apiVersion: v1
kind: ConfigMap          //시크릿도 카인드만바꾸면 똑같음
metadata:
	name: conapp

data:
	변수명1:변수값1
	변수명2:변수값2






	apiVersion: v1   
	kind: Config       //cubeconfig,#HOME(환경변수 HOME)/.kube/config 에 위치
	
	current-context:컨텍스트명1
	
	clusters:
	-  name: 클러스터명1
	   cluster:
	     certificate-authority:ca.crt  //ca는 여기
		 server: https://경로:포트
	contexts:
	- name: 컨텍스트명1
	  context:
		cluster: 클러스터명1
		user: 유저명1
		namespace: 네임스페이스명1  //옵션,클러스터1의 네임스페이스1로 들어감
	users:
	-  name: 유저명1
	   user:
	     client-certificate: 유저.crt
		 client-key: 유저.key



		  
For Windows: Ctrl+Insert to copy and Shift+Insert to