1.소개
	스킵
2.설치
	스킵

3.쿠버네티스로 컨테이너 실행	
1.kubectl
	kubectl의 기본 구조는
		kubectl 커맨드 타입or이름 (플래그)  플래그는 옵션
	임
	만약 kubectl get pod 를 하면 포드 전체를 겟(표시)하고
	   kubectl get abcd를 하면 abcd라는 이름을 가진걸 겟함
	   
	   
	여기서 커맨드에는
		get:명시된 데이터를 받아옴
		run:명시된 이름으로 명시된 이미지의 파드를 생성함
		apply:명시된 위치의 yaml파일을 기반으로 선언적으로 디플로이먼트를 생성해서 파드를 생성함
		scale:파드의 갯수를 늘이거나 줄임
		expose:명시된이름으로 명시된 타입의 서비스를 생성함
		port-forward:명시된 이름의 객체를 뒤에 포트(8080:25500)의 외부포트로 매핑함
		logs -f:명시된이름의 객체의 로그를 수집함
		exec:명시된 이름의 객체에 뒤에 나올 명령을 실행하라고 함
		  (kubectl (-n default) (exec) ((my-pod) (-c my-container)) (-- ls /) )
		          디폴트 네임스페이스에서 실행해라 마이포드를    마이컨테이너에 있는걸    쿠버네티스관련옵션을 전부 종료시키는
			마이컨테이너에 있는 마이포드의 쿠버네티스관련옵션을 전부 종료시키는 명령을 실행해라
		api-resources:사용할수 있는 자원들을 표시함	
			
	2.kubeconfig환경변수
		kubectl의 환경변수는 home/.kube/config에 있음
		여기서 클러스터에서 사용할수 있는 자원을 확인하는건 kubectl api-resources로 확인할수있음
		
		도커 데스크톱으로 쿠버네티스를 쓰면 자동으로 kubeconfig가 생성되고,
		--kubeconfig옵션으로 다른 설정파일을 지정할수 있음
		다중 클러스터에 다른 인증정보로 접근할때 사용함
		
	3.다양한 사용 예
		단순히 명령실행말고,파이프라인으로 현재값의 출력을 다음명령의 입력으로 넣을수도 있고 그런식으로 스크립트식 사용도 가능함
		
			
			
			
			
2.디플로이먼트를 이용해 컨테이너 실행
	앞에서 run으로 생성한건 직접 파드를 하나 추가한거고,보통은 apply로 디플로이먼트를 생성함
	이렇게해야 선언적으로 생성할수있어서 관리하기가 편함
			
3.클러스터 외부에서 클러스터 안 앱에 접근하기
	쿠버네티스 외부에서 쿠버네티스 내부에 접근하려면,직접 접근하면 안되고 쿠버네티스의 서비스를 통해서 접근해야함
	그래서 서비스를 생성해서 그쪽을 통해서 접근해야함
		kubectl expose deployment 이름 --type=NodePort
	하면 노드포트타입의 디플로이먼트를 생성하고(노드포트는 모든 노드의 포트를 할당함,즉 전체를 다받음 )
		kubectl get service로 포트를 확인하고 그 포트로 접속하면됨
		좀더 자세히 보려면
			kubectl describe service 이름
		으로 상세하게 볼수있음
			
			
			
			
			
			
4.쿠버네티스 아키텍쳐
1.쿠버네티스 클러스터 전체 구조
	쿠버네티스 클러스터는 클러스터를 관리하는 마스터와 실제컨테이너를 실행시키는 노드로 구성됨
	마스터에는 etcd(모든 설정등 적는건 여기다들어있음),apiserver(모든건 여기통해서 입출력을 함) 등이 들어있음
	
	노드는 kubelet,kube-porxy,docker등 컴포넌트가 실행되고,실제 사용하는 컨테이너의 대부분은 노드에서 실행됨
	
	구조는
		쿠버네티스에 명령을 주면 리버스프록시로 마스터api에 명령을 전달하고 걔가 노드랑 etc등으로 명령을 전달하고 실행시킴
	
	
	쿠버네티스의 관리용 컴포넌트들도 다 컨테이너로 실행됨
	
2.쿠버네티스의 주요 컴포넌트
	쿠버네티스는 기본적으로 클러스터를 관리함
	클러스터는 단일컴퓨터뿐만 아니라 여러대컴퓨터를 묶음으로 다루는걸 뜻하므로 여러가지의 컴포넌트를 포함함
	
	쿠버네티스의 컴포넌트는 관리에 필수인 마스터컴포넌트,노드컴포넌트와 추가로 붙인 애드온컴포넌트로 나눠짐
	
	1.마스터컴포넌트
		etcd:etcd는 키밸류 저장소임
			분산시스템에서 노드 사이의 상태를 공유하는,데이터베이스 역할을 함
			etcd는 서버 하나당 프로세스 하나만(즉 전체에서 하나밖에없음,클러스터링등으로 같은걸 복사할순있지만)존재함
			
		kube-apiserver:얘는 쿠버네티스의 api를 사용할수 있도록하는 컴포넌트임
						얘는 클러스터로 온 명령이 유효한지 검증하고(문법과 권한)그걸 실행해서 돌려줌
						얘는 수평적으로 확장 가능하니까(어짜피 api서버라서 유일성같은거없음)서버여러대에 여러개설치가능
		
		kube-scheduler:얘는 클러스터 안에 자원 할당이 가능한 노드중 알맞은 노드를 선택해서 파드를 생성하는 컨포넌트
						파드는 여러 요구조건을 받을수있으며,거기에 맞는 노드를 선택해서 생성함
		
		kube-controller-manager:얘는 파드들을 관리하는 컨트롤러를 실행하는 컴포넌트임
								클러스터에서 새로운 컨트롤러를 생성하고 실행할때 컨트롤러 매니저의 큐에 넣어서 실행하는식으로 동작함
		
		cloud-controller-manager:얘는 쿠버네티스의 컨트롤러를 클라우드와 연결해서 관리하는 컴포넌트임,필요해지면보자
		
	2.노드용 컴포넌트
		kubelet:얘는 클러스터 안의 모든 노드에서 실행하는 컴포넌트,파드컨테이너들의 실행을 직접 관리함,파드스펙이라는 조건설정을 전달받아서 실행하고
				컨테이너가 정상적으로 실행되는지 헬스체크를 진행함,단 노드안에 있는 컨테이너라도 쿠버네티스가 안만들었으면 관리하지않음
				(컨테이너안 파드 지웠을때 바로재시작거는게 얘인듯)
				
		kube-proxy:클러스터안에 별도의 가상 네트워크를 설정하고 관리하는 컴포넌트
		
		컨테이너 런타임:실제로 컨테이너를 실행시키는 컴포넌트,대표적으로 도커가 있음
		
	3.애드온
		네트워킹 애드온:클러스터 안에 가상네트워크를 구성해 사용할떄 프록시이외에 네트워킹 애드온을 사용함,얘가 직접 서버구성할때 가장 까다로움
		
		dns애드온:클러스터 안에서 동작하는 dns서버,쿠버네티스 서비스에 dns레코드를 제공함,쿠버네티스 안에 실행된 컨테이너들은 자동으로 dns서버에 등록됨
				주로 coreDNS를 사용함 
				
		대시보드 애드온:kubectl로 명령 주지만,gui로 볼때 대시보드애드온으로 사용할수있음
		
		컨테이너 자원 모니터링:컨테이너들의 자원사용량등을 시계열형식으로 저장해서 볼수있음
		
		클러스터 로깅:클러스터 안 개별 컨테이너의 로그와 구성요소의 로그를 모아서 보는 애드온
		
			
3.오브젝트와 컨트롤러			
	쿠버네티스는 오브젝트와 오브젝트를 관리하는 컨트롤러로 나눠짐
	사용자는 템플릿등으로 쿠버네티스에 자원의 바라는 상태를 정의하고,컨트롤러는 바라는상태와 현재상태가 일치하도록 오브젝트를 생성/삭제함
	오브젝트에는 파드,서비스,볼륨,네임스페이스 등이 있고 컨트롤러에는 레플리카셋,디플로이먼트,스테이트풀셋,데몬셋,잡등이 있음
	
	1.네임스페이스
		네임스페이스는 클러스터 하나를 논리적인 단위로 나눠서 실행하는것,
		이해하자면,컴퓨터 하나에 특정 폴더에 프로그램 바로가기로 싹 몰아두고 이름붙이는느낌임,그래서 그폴더를 전부 실행하거나 실행에 제한걸거나 하는식
		
		네임스페이스를 지정할때는 
			--namespace=붙일이름 
		으로 하나하나붙여도되는데
		
		그냥 디폴트값을 바꿀수도있음
			kubectl config current-context
		로 현재 컨텍스트 이름를 확인하고
			kubectl config set-context 컨텍스트이름 --namespace=붙일이름
		으로 디폴트값을 바꾸면 새로생성한값의 네임스페이스가 바뀜
		
		네임스페이스 전체검색하려면
			kubectl get pod --all-namespaces
		하면됨
		
		바꾼값 다시 디폴트로 바꾸려면 
			--namespace=""
		하면됨
		
	2.템플릿
		쿠버네티스 클러스터의 오브젝트나 컨트롤러가 어떤 상태여야 하는지 적용할떈 yaml형식의 템플릿을 적용함
		템플릿은 들여쓰기로 구조가 바뀌고(파이썬처럼),
		scalars(스트링,넘버),sequences(어레이,리스트),mappings(해시,딕셔너리) 3기초요소로 표현됨
		
		템플릿의 기본형식은
			apiversion:v1
			kind:Pod(생성종류)
			metadata:
			spec:
		로 구성됨
		apiversion은 api버전이고(쿠버네티스버전과 관련된)
		kind는 어떤 오브젝트나 컨트롤러의 작업인지를 명시하고
		metadata는 해당 오브젝트의 이름이나 레이블등을 설정하고
		spec는 파드가 어떤 컨테이너를 가지고 실행하며,실행할때 어떻게 동작해야할지를 명시함
		
		kubectl explain 생성종류  로 현재 생성할거에 무슨 하위필드가 있는지 출력해서 볼수있음
		
		하위필드를 포함해 특정필드를 커맨드라인에서 지정할때는 .metadata.anno이런식으로 .으로 이어가면됨,맨앞에도 .붙이는거에 주의
		필드설명없이 그 아래에 속한 모든필드를 보려면 --recursive옵션을 쓰면됨
		
		
			
			
			
			
5.파드
1.파드개념
	쿠버네티스는 파드라는 단위로 컨테이너를 묶어서 관리하므로,파드는 안에 컨테이너가 여러개로 구성되긴함
	그리고 파드 하나는 ip를 공유하고,내부의 컨테이너들끼리 포트로 구분해가며 데이터를 받음
	그리고 컨테이너 하나에 프로세스를 여러개 실행시킬수도 있지만,그러면 개빡세니까 보통은 컨테이너 하나에 프로세스 하나씩 해서 돌리는거같음
	
2.파드사용하기
	파드를 설정할때는
	yaml에
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
	
	이런식으로 설정함
	저기서 라벨은 오브젝트를 식별하는 레이블
	그리고 상위 바로 밑에 -를 붙이는건 여기서부터 하위필드를 배열로 묶겠다는 뜻임


3.파드 생명 주기
	파드는 생성부터 삭제까지의 과정에 생명주기가 있음
	
		pending:파드를 생성하는 중일때 나옴,이미지를 다운로드한후에 컨테이너를 실행해야하니까 시간 좀 걸림
		running:파드 안 모든 컨테이너가 실행중인 상태,1개이상의 컨테이너가 실행중이거나 시작,재시작상태일수 있음
		succeeded:성공적으로 모든 컨테이너가 실행 종료된상태,정상종료니까 재시작되지않음
		failed:파드 안 모든 컨테이너중 정상적인 실행 종료가 되지 않은 컨테이너가 있는 상태
		unknown:파드의 상태를 확인할수 없는 상태,파드가 있는 노드와 통신할수 없을때 주로 나옴
		
	
	현재 파드 생명주기는 
		kubectl describe pods 파드이름
	을 실행하고 status를 보면 나옴
	거기서 밑에 type가 있는데
	각 타입별 정보는
		initialized:모든 초기화컨테이너가 성공적으로 시작 완료됨
		Ready:파드는 요청을 실행할수 있고 모든 연결된 서비스의 로드밸런싱 풀에 추가되어야 한다는 뜻
		containersready:파드안 모든 컨테이너가 준비상태라는 뜻
		podScheduled:파드가 하나의 노드로 스케줄을 완료했다는 뜻
		unschedulable:스케줄러가 자원의 부족이나 다른 제약등으로 당장 파드를 스케줄 할수 없다는 뜻
	
4.kubelet로 컨테이너진단
	컨테이너가 실행 된 후에는 kubelet가 주기적으로 컨테이너를 진단함,이때 필요한 프로브는 2개가있음
		livenessprobe:컨테이너가 실행됐는지 확인함,이게 실패하면 kubelet는 컨테이너를 종료시키고 재시작 정책에 따라 컨테이너를 재시작함
		readinessprobe:컨테이너가 실행된 후 실제로 서비스 요청에 응답할수 있는지 진단함
					   이 진단이 실패하면 엔드포인트 컨트롤러는 해당 파드에 연결된 모든 서비스를 대상으로 엔드포인트 정보를 제거함
					   즉 서비스 연결이 안될거같으면 거기 연결 못하게 다지워버림
					   
					   
	
	컨테이너 진단할땐 컨테이너가 구현한 핸들러를 kubelet가 호출해서 실행함
	핸들러에는 3종류가 있음
		execaction:컨테이너 안에 지정된 명령을 실행하고 종료코드가 0일때 성공
		tcpsocketaction:컨테이너 안에 지정된 ip,포트로 tcp상태를 확인하고 열려있으면 성공
		httpgetaction:컨테이너 안에 지정된 ip,포트,경로로 http get명령을 보내서 응답코드가 200~400사이면 성공
		
	결과는 success,failure,unknown3개가 있음,언노운은 진단이 실패하서 컨테이너 상태를 알수없을때 나옴
		
	
5.초기화 컨테이너
	초기화 컨테이너는 앱 컨테이너가 실행되기 전 파드를 초기화함
	보안상이유로 앱 컨테이너이미지와 같이 두면 안되는 앱의 소스코드를 별도로 관리할때 유용
	초기화 컨테이너의 특징은
		초기화 컨테이너는 여러개를 구성할수 있음,초기화컨테이너가 여러개있으면 파드템플릿에 명시한 순서되로 초기화컨테이너가 실행됨
		
		초기화 컨테이너의 실행이 실패하면 성공할때까지 재시작함,그래서 무조건 순서대로 실행되니까 절차적으로 실행시켜서,
		쿠버네티스의 선언적이라는 특징에서 벗어날수있음
		
		초기화 컨테이너가 모두 실행 된 후 앱 컨테이너 실행이 시작됨
	
	
	그래서 이런특징을 이용해 파드를 실행할때 앱 컨테이너가 외부의 조건을 만족할때까지 기다렸다가 실행하게 만들수있음
	그리고 초기화 컨테이너는 readinessprobe를 지원하지 않음,파드가 준비되기 전 실행하고 사라지는 컨테이너이기때문
	
	초기화 컨테이너 yaml은
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			init컨테이너들
				-이름:이름
				이미지:이미지
				커맨드:실행하고 할 커맨드를 입력함(몇초 기다려라 이런거)
				
6.파드인프라컨테이너
	쿠버네티스에는 모든 파드에서 항상 실행되는 pause라는 컨테이너가 있음,이걸 파드 인프라 컨테이너 라고 함
	얘는 파드 안 기본 네트워크로 실행되고,프로세스 식별자가 1로 설정되므로 컨테이너의 부모 컨테이너 역할을 함
	파드 안 다른 컨테이너는 pause가 제공하는 네트워크를 공유해서 사용함,그래서 다른컨테이너가 재시작돼도 ip가 유지되지만,
	얘가 재시작되면 ip가 바뀌고 파드 안의 다른 모든 컨테이너도 전부 재시작함
	
	이 인프라를 바꿀수도 있는데 --pod-infra-container-image옵션으로 다른 컨테이너를 인트라컨테이너로 지정할수 있음
	

7.스태틱 파드
	얘들은 api서버를 거치지 않고 kubelet가 직접 실행하는 파드들임
	여기에 추가하고싶으면 --pod-mainfest-path라는 옵션에 지정한 디렉토리에 스태틱파드에 추가할 파드들을 넣어두면 kubelet가 감지해서 실행함
	
	얘들은 kubelet가 직접 관리하면서 이상이 생기면 재시작함
	그리고 kubelet가 실행중인 노드에서만 실행되고,다른노드에서는 실행되지 않음
	그리고 apiserver로 파드를 조회할수는있지만,거기에 명령을 내릴순 없음
	
	보통 스태틱파드는 apiserver나 etcd같은 시스템파드를 실행하는 용도로 많이 사용함 


8.파드에 cpu와 메모리 자원 할당
	만약 노드 하나에 자원 사용량이 많은 파드들이 모여있으면 파드들의 성능이 나빠지고,전체 클러스터의 자원사용 효율도 낮음
	어떤 노드에는 파드가 없어서 cpu메모리가 남고, 어떤 너드는 파드가 많아서 모자라는 그런일이 나타날수 있음
	
	그래서 쿠버네티스에서는 파드에서 자원의 최소치와 최대치를 지정할수 있게 준비해뒀음
	.spec.containers[].resources.limits.cpu
									   .memory
								.requests.cpu
										 .memory
							
	여기서 리미트는 그 파드가 쓸수있는 한도고,리퀘스트는 최소한 그정도는 써야한다는거임
	즉 노드에 배치될떄, 리퀘스트만큼 자원 여유가 있는 노드여야 거기에 스케줄링해서 배치해서 실행되고,
	그런게 없으면 pending로 대기하고 실행하지않다가 자원여유가 생기면 실행함
	
	리미트는 자원의 최대량임,파드가 아무리 많이써도 리미트 이상은 사용할수 없게 막음
	
	노드에 배치할때는 현재사용량을 보는게 아니라 리미트와 리퀘스트만 봄
	리미트가 설정되어있으면 리미트의 총합만큼을 보고 자리가 남으면 거기넣고,리미트가 설정안되면 리퀘스트의 총합을 가지고 넣음
	리미트가 설정안되면 오류가 날수있지만,노드자원을 최대한 많이 사용할수 있다는 장점이 있긴함
	
	그리고 cpu는 코어의 0.1단위로 잘라서 사용할수 있음

9.파드에 환경변수 설정
	컨테이너를 사용할때 장점은,개발환경에서 만든 컨테이너에 환경변수만 바꿔서 실제환경에서 실행해도 그대로 동작한다는 점임
	여기서 환경변수 설정하는방법은 스펙.컨테이너 밑에 env에 넣어주면됨
	
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
				env:
					-이름:이름1
					 값:값1
					-이름:이름2
					 valuefrom:
						필드ref:
						필드path:spec.nodeName이나 metadata.name,status.podIP등등 참조값 전부가능
					-이름:이름3
					 valuefrom:
						resourcefieldref:
							컨테이너이름:가져올컨테이너이름
							resource:requests.cpu,limits.cpu등등 
	
	이런식으로 가져오거나 설정할수있음
							
	이름은 환경변수의 이름이고(즉 저기에 적힌거:값 이런식으로 그대로 바로 적용됨 사용자정의아니면,즉 cpu사용량같은거 적어두면 바로적용됨)
	값(value)은 직접 적은 상수값
	valuefrom은 값을 어디서 참조하는거
	fieldref는 파드의 현재 설정 내용을 값으로 설정한다는 선언(현재파드내에서 가져온다는거)
	fieldpath는 어디서 값을 가져올것인지,즉 값을 참조하려는 항목의 위치를 지정
	resourcefieldref는 컨테이너에 cpu메모리 사용량을 얼마나 할당했는지에 관한 정보를 가져옴
	containername은 환경변수 설정을 가져올 컨테이너의 이름
	resource는 어떤 자원의 정보를 가져올지 설정


10.파드에 환경설정 내용 적용
	파드는 환경설정 내용을 yaml하나에 모두 작성한 후 적용해야함
	적용할떄는
		kubectl apply -f pod-all.yame
	을 실행하면됨
	
	확인할땐 
		kubectl exec -it kubernetes-simple-pod sh
	를 실행해서 컨테이너 안에 접속하고
		env
	치면 나옴
	
	
11.파드 구성 패턴
	1.사이드카패턴
		원래 사용하던 컨테이너의 기능을 확장하거나 강화하는 용도의 컨테이너를 추가하는것
		기본 컨테이너는 원래기능에만 충실하게 구성하고,나머지 공통부가기능들은 사이드카 컨테이너를 추가해서 사용하는것
		로그수집 컨테이너처럼 파일긁어서 보내는 역할만 하는 컨테이너가 예,이건 어딜붙여도 파일저장만 하면 똑같이쓸수있음
	2.앰배서더 패턴
		얘는 파드안에 프록시역할을 하는 컨테이너를 추가하는 패턴
		즉 파드 안에서 외부서버에 접근할때 내부프록시에 접근하게 하고,연결은 프록시에서 알아서 처리함
		이러면 파드의 트래픽을 더 세밀하게 제어할수 있음
	3.어댑터 패턴
		어댑터 패턴은 파드 외부로 노출되는 정보를 표준화하는 어댑터 컨테이너를 사용한다는 뜻임
		즉 파드 내부에서 정보를 어댑터로 보내고,저기서 정보를 취합해서 중앙모니터링에 보내면 거기서 모니터링하는데 이용하는 그런식
		





6.컨트롤러
1.레플리케이션 컨트롤러
	초창기 레플리카세트같은거임,요즘은 안씀
2.레플리카셋
	얘는 같은 파드를 복사해서 배포하는식으로 사용하는 컨트롤러임
	항상 선언한것과 같은 숫자의 파드가 클러스터 안에서 실행되도록 관리함
	
	그리고 얘는 롤링업데이트(25%정도씩 잘라서 파드를 업데이트-재시작하는것)을 하지못하니까 이거필요하면 디플로이먼트를 써야함
	1.레플리카셋 사용하기
	이거도 파드랑 똑같이 사용하면됨
		apiversion:앱버전
		kind:replicaset
		metadata:
			name:이름
		spec:
			template:
				metadata:
					name:이름1
					labels:
						app:이름1
				spec:
					containers:
						-name:이름1
						 image:이미지
						 ports:
						 -containerPort:포트번호
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
						
						
	자세한 명세는 .spec에 적으면되고,스펙밑에는 무슨파드를 실행할지를 적어야하니까 메타데이터 스펙이 한번 더 나옴	
	그리고 .spec.template.spec.container[]필드 밑에 파드의 정보를 설정함	
	.spec.replicas는 파드의 갯수를 설정하는것
	.spec.selector는 어떤 레이블의 파드를 선택해서 관리할지를 정함,즉 label.app에 있는거중 하나를 선택해서 그것만 관리함(몇개인지)
	즉 matchlabels랑 labels의 값이 같아야함
	만약 셀렉터를 비워두면 labels.app에 있는걸 기본값으로 설정함
	
	이 yaml을 apply로 클러스터에 적용하면됨
	
	여기서 만약 만들어진 파드를 지우면,바로 재생성이 되고,
	만약 yaml을 수정해서 파드의 갯수를 줄이면,바로 하나를 지움
	
	2.레플리카셋과 파드의 연관관계
		파드는 레이블기준으로 관리하므로 레플리카셋과 파드는 느슨하게 결합되어 있음
		레플리카셋과 파드를 한꺼번에 삭제할때는 delete replicaset 컨테이너이름으로 하면 삭제되지만,--cascade=false옵션을 주면
		연쇄삭제는 되지않고 레플리카셋만 삭제됨,그러면 현재 실행중인 파드를 관리하는 레플리카셋을 추가로 만들수도 있음(현재설정을 바꿔야하는게 바꿀게많을때)
		
		kubectl get replicaset으로 레플리카셋의 상태를 볼수있는데
		거기서 desired는 설정할때 지정한 파드의 수,current는 현재 파드의 수 임,정상적으로 동작하면 두개는 같아야함
		
		그리고 파드의 레이블을 바꾸면(.metadata.labels.app),
		레플리카셋은 레이블기반으로 체크를 하는데 레이블이 바뀌었으니까 다시 새로운걸 만들게됨
		
		이런식으로 레이블 설정 변경으로 실행중인 파드를 재시작하지 않고 서비스에서 분리해 디버깅하는등으로 사용할수 있음
		
3.디플로이먼트
	디플로이먼트는 쿠버네티스에서 상태가 없는 앱을 배포할때 사용하는 가장 기본적인 컨트롤러임
	요즘은 보통 디플로이먼트로 앱을 배포함
	
	디플로이먼트는 레플리카셋을 관리하면서앱 배포를 더 세밀하게 관리함
	단순히 실행시켜야 할 파드의 수를 유지하는거뿐 아니라,앱을 배포할때 롤링업데이트하거나,배포중 잠시 멈췄다가 배포하거나,업데이트를 롤백하거나 할수있음
	
	1,디플로이먼트 템플릿
		yaml구성은
			apiversion:api버전
			kind:Deployment
			metadata:
				name:이름1
				labels:
					apps:이름1
			spec:
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
				template:
					metadata:
						labels:
							app:이름1
						spec:
							containers:
								-name:이름1
								 image:이미지
								 ports:
									containerPort:포트번호
	
		이렇게 구성됨
		
		파드를 몇개구성할지는 스펙밑에 레플리카고(레플리카셋이랑 똑같이)
		.spec.selector.matchLabels의 하위필드는 메타데이터의 레이블의 하위필드랑 같아야함
		파드의 설정정보가 있는 스펙.템플릿.스펙.컨테이너[]에 컨테이너의 이름과 이미지가 있음
		
		즉 레플리카셋이랑 별다른건 없음
		
		디플로이먼트가 제대로 실행되었는지는 kubectl get deploy,rs,pods 로 3개 다 확인할수 있음(디플로이먼트,레플리카셋,파드)
		
		즉 디플로이먼트는 레플리카셋을 감싸는 어댑터임(롤링업데이트등을 사용하게 해주는)
		
		디플로이먼트에서 이미지를 업테이트 하는방법은 총 3개가 있음
			kubectl set으로 컨테이너이미지를 지정하는것
			kubectl edit로 파드를 연다음 이미지정보를 수정하는것
			yaml을 수정하고 kubectl apply하는것
			
		보통 1,3이 쓰이는데 yaml을 보통 사용하는듯
		그리고 이렇게 디플로이먼트의 설정이 변경되면,새로운 레플리카셋이 생성되고,거기에 맞게 파드들이 변경됨(파드들이 새로생성됨)
		
		
		
	2.디플로이먼트 롤백
		컨테이너 이미지 변경 내역은
			kubectl rollout history deploy 디플로이먼트이름
		으로 확인할수있음
		
		만약 특정 리비전의 상세내용을 확인하려면 
			kubectl rollout history deploy 디플로이먼트이름--revision=리비전숫자 
		를 주면 확인할수있음(이미지버전이라던가)
		
		그 버전으로 롤백하려면
			kubectl rollout undo deploy 디플로이먼트이름--to-revision=리비전숫자
		로 버전선택해서 돌릴수있음
		돌리면 그번호가 삭제되고 새로 갱신되어서 제일 큰숫자로 바뀌어서 나옴(총 3개일때 2번으로 언두하면 2->4로바뀜)
		
		이 숫자는 겹치지않고 계속 커져나감
		
		
	3.파드개수 조정
		현재 실행중인 디플로이먼트의 파드 개수를 조정하려면 
			kubectl scale deploy 디플로이먼트이름 --replicas=파드갯수
		를 해주면됨
			
	4.디플로이먼트 배포정지,재개,재시작하기
		kubectl rollout로 진행중인 배포를 멈췄다가 다시 시작할수 있음
		
			kubectl rollout pause deployment/디플로이먼트이름
		으로 업데이트를 멈추고 
		
			kubectl rollout resume deployment/디플로이먼트이름
		로 다시시작할수있음
		
		그리고 디플로이먼트를 재시작시킬땐
			kubectl rollout restart deployment/디플로이먼트이름
		하면됨
		재시작은 디플로이먼트 스테이트풀셋 데몬셋에서 가능함
		
	5.디플로이먼트 상태
		배포중에는 디플로이먼트 상태가 변함
		progressing(진행)이었다가 compleate(완료),혹은 failed(실패)로 바뀜
		
		kubectl rollout status로 배포 진행 상태를 확인할수 있음
		
		디플로이먼트가 
			새로운 레플리카셋을 만들떄,
			새로운 레플리카셋의 파드수를 늘릴때,
			예전레플리카셋의 파드수를 줄일떄,
			새로운 파드가 준비상태가 되거나 이용가능한 상태가 되었을때
		진행상태로 표시됨
		
		배포가 끝나면 완료가 되고 종료코드가 0으로 표시됨
			디플로이먼트가 관리하는 모든 레플리카셋이 업데이트 완료되었을때
			모든 레플리카셋이 사용가능해졌을때
			예전 레플리카셋이 모두 종료되었을때
		완료가 됨
		
		배포중 이상이 있으면 실패가 됨
			쿼터부족(자원부족)
			readinessprobe실패
			컨테이너 이미지 가져오기 실패
			권한부족
			제한범위 초과
			앱실행조건 잘못지정
		일때 실패함
		
		템플릿에 .sepc.progressDeadlineSeconds항목을 추가하면 지정된 시간이 지났을때 실패로 처리할수있음
		
		
4.데몬셋
	데몬셋은 클러스터 전체 노드에 특정 파드를 실행할때 사용하는 컨트롤러임
	클러스터안에 새롭게 노드가 추가되었을때,자동으로 데몬셋이 해당 노드에 파드를 실행시키고,
	만약 노드가 클러스터에서 빠지면 해당 노드에 있던 파드는 그냥 사라짐
	
	즉 모든 노드에 하나씩 꼭 있어야하는 파드를 실행시킬때 사용함(로그수집같은)
	
	1.데몬셋 템플릿
		데몬셋의 템플릿은
			apiversion:api버전
			kind:daemonset
			metadata:
				name:이름1
				namespace:지정할네임스페이스(보통 시스템에넣는듯 컨셉상 그런느낌이라서)
				labels:
					k8s-app:fluentd-logging 이름2
			spec:
				selector:
					matchLabels:
						name:이름1
				updateStrategy:
					type:Rollingupdate
				template:
					metadata:
						labels:
							name:이름1
					spec:
						container:
							-name:이름1
							 image:사용할이미지
							 env:(환경변수)
								-name:환경변수1
								 value:환경변수1값
							resources:
								limits:
									memory:메모리값
								requests:
									cpu:cpu값
									memory:메모리값 
									
	
				
		이런식으로 나감
		보통 데몬셋은 시스템같은 느낌으로 사용되기때문에 네임스페이스를 별도로 만들어서 사용함(ku-system이런 이름으로 )
		그리고 레이블은 k8s-app필드에 값은 fluentd-logging로 줌(이건 키워드인지 아닌지 모르겠음 나중에해보자)
		데몬셋을 업데이트 하는 방법은 updateStrategy.type필드값에 적힌대로 됨,롤링업데이트와 온딜리트 2개가있음
		컨테이너의 이미지는 컨테이너에 있음
		
		이런후 똑같이 apply하면됨
	
	2.데몬셋의 파드업데이트 방법 변경하기
		이거도 롤링업데이트하면 순차적으로 바뀌고,온딜리트하면 삭제되기전까진 안바뀜
		이거 바꿀때는 
			kubectl edit daemonset 데몬셋이름 -n 네임스페이스이름
		으로 에디터띄우고 값바꾸면 됨
		
	
		
5.스테이트풀셋
	앞에서 나온 레플리카셋,디플로이먼트는 상태가 없는 파드들을 관리하는 용도고,상태가 있는 파드들을 관리하려면 스테이트풀셋을 써야함
	
	스테이트풀셋을 사용하면 볼륨을 이용해서 데이터를 저장한후 그 데이터를 파드가 재시작해도 유지할수 있음
	여러 파드사이에 순서를 지정해서 실행되게 할수도 있음
	
	1.스테이트풀셋 사용하기
		yaml구조는
			apiversion:api버전
			kind:Service //서비스도 있어야함 상태저장해야하니까
			metadata:
				name:서비스이름1
				labels:
					app:서비스이름1
			spec:
				ports:
					-port:포트번호1
					 name:web 포트이름1
				clusterip:none  //내부망이니까
				selector:
					app:서비스이름1
					
			
			apiversion:api버전
			kind:StatefulSet
			metadata:
				name:web 포트이름1
			spec:
				selector:
					matchLabels:
						app:스테이트풀이름1
				servicename:서비스이름1
				replicas:파드갯수
				template:
					metadata:
						labels:
							app:스테이트풀이름1
					spec:
						terminationGracePeriodSeconds:10
						container:
							-name:스테이트풀이름1
							 image:사용이미지
							 ports:
								-containerPort:포트번호1
								 name:포트이름1


		여기서 스테이트풀은 통신을 해서 저장하고 읽어야하기때문에 서비스가 있어야하고,스테이트풀에서 서비스를 받아야함
		서비스이름과 스테이트풀셋에서 만들어질 파드 이름을 조합하면 클러스터안에서 사용하는 도메인을 만들수있음,파드이름.서비스이름 형식
		그리고 스테이트풀셋의 이름을 web(포트이름1)로 설정하고
		terminationGracePeriodSeconds는 그레이스풀의(실행중인 프로세스를 종료할때 끝낼시간을 주는것)대기시간을 설정한것
		
		그리고 yaml저장하고 apply하면됨
		
		스테이트풀셋으로 만들면 파드이름뒤에 난수가붙는게 아니라 포트이름뒤에 0,1,2순서대로 붙음
		실행할때는 작은숫자부터 실행되고,만약 앞에꺼가 실행되지않으면 뒤에거도 전부 실행되지 않음
		그리고 삭제할때는 큰숫자부터 삭제됨


	2.파드를 순서없이 실행하거나 종료하기
		기본값은 순서대로 관리하는거지만 .spec.podManagementPolocy필드로 순서를 없앨수도 있음
		기본값은 OrderedReady(순서대로)고 Parallel로 순서없이로 바꿀수있음
		
		실행중인 스테이트풀에서는 바꿀수없고,새로운 스테이트풀을 만들어야함
	
	3.스테이트풀셋으로 파드업데이트 하기
		업데이트방법은 .spec.updateStrategy.type에 설정할수 있음
		기본값은 롤링업데이트
		
		그리고 마지막거부터 업데이트되고
		.spec.updateStrategy.rollingUpdate.partition값을 바꾸면 그거보다 큰값만 업데이트됨
		즉 파드들이 분할이 됨
		이렇게하면 레이블항목에 레이블이 두개로 나눠지는데,
		
		이걸 사용하면 스테이트풀셋이 관리하는 전체파드들 중에서 특정 파드에만 서비스를 할수있게됨


6.잡
	잡은 실행된 후 종료되어야 하는 성격의 작업을 실행할때 사용하는 컨트롤러
	특정갯수만큼의 파드를 정상적으로 실행종료함을 보장함
	만약 파드실행실패,하드웨어장애발생,노드재시작등 문제가 발생하면 다시 파드를 실행함
	즉 일회성으로 뭘 실행시켜야할때 쓰거나 그럴듯
	
	1.잡 사용하기
		yaml은
			apiversion:api버전
			kind:job
			metadata:
				name:이름1
			spec:
				template:
					spec:
						containers:
							-name:이름1
							 image:이미지
							 command:실행할커맨드
						restartPolicy:never
				backofflimit:4
				
		restartPolicy는 never로 설정하면 파드가 항상 성공으로 끝나게하고,
		onfailure는 파드 안 컨테이너가 비정상종료하거나 다양한 이유로 정상종료되지않으면 컨테이너를 다시시작하게 함
		backofflimit는 잡 실행이 실패했을때 몇번까지 반복할것인지 설정함
		보통 한번실패하고 좀 기다렸다가 다시시작하고 그런식으로 점점 시간간격을늘리다가 실행성공하면 다시 재시도횟수는 0으로 초기화됨
	
	2.잡병렬성관리
		잡 하나가 몇개의 파드를 동시에 실행할수 있는지를 잡 병렬성이라고함
		기본값은 1이고 0으로주면 잡을 정지할수 있음
			단 여러이유로 옵션을 설정해도 지정한 값보다 잡이 파드를 적게 실행시킬수 있음
				정상완료되는 잡의 개수를 고정하려면,병렬로 실행되는 실제 파드의 수가 정상완료를 기다리며 남아있는 잡의 개수를 넘지 않아야 함
				즉 자리가 10자린데 현재 완료된게 9개면 병렬을 3으로줘도 1개만 실행됨
			워크큐용 잡에서는 파드 하나가 정상적으로 완료되었을때 새 파드가 실행되지않음,단 현재실행중인잡은 완료될때까지 실행됨
			잡 컨트롤러가 반응하지못할때도있음
			자원이 모자라거나 권한이 부족할수있음
			잡에서  파드들이 실패를 너무 많이했으면 새 파드생성을 제한할수 있음
			파드가 그레이스풀하게 종료되었을수 있음
	3.잡의 종류
		잡은 단일잡,완료된 잡갯수가 있는 병렬잡,워크큐가 있는 병렬잡이 있음
		
		단일잡은
			파드 하나만 실행됨,파드가 정상적으로 실행종료되면(succeeded) 잡 실행을 완료함
			spec.completions와 spec.parallelism필드를 설정하지 않음,둘의 기본값은 1
				spec.completions는 정상적으로 실행종료되어야하는 파드갯수
				spec.parallelism는 병렬적으로 몇개실행할지 정하는거
			즉 둘이1이면 하나만실행해서 하나만끝남
			
		완료갯수가있는 병렬잡은
			spec.completions를 양수로 설정하고
			spec.parallelism는 1로 설정
		즉 완료해야하는건 여러개고 실행은 하나만
		
		워크큐가 있는 병렬잡은
			spec.completions는 설정하지않고
			spec.parallelism는 양수로 설정
			
			만약 completions을 설정하지않고 parallelism를 설정하면,completions은 parallelism를 따라감(동일하게설정됨)
			(단 기본값인 1이니까 하나완료되면 끝임)
			
			파드 각각은 정상적으로 실행종료됐는지를 독립적으로 결정할수 있음
			즉 대기열에 있는 작업들이 동시에 실행할수도있음
			
			파드 하나라도 정상종료되면 새로운파드가 실행되지않음
			
			최소한 파드 1개가 정상종료된후 모든 파드가 실행종료되면 잡이 정상적으로 종료됨
			
			일단 파드1개가 정상종료되면 다른 파드는 더이상 동작하지 않거나 결과를 내지않고 종료함
			

	4.비정상종료된 파드관리
		만약 파드안에 비정상종료된 컨테이너가 있을때를 대비해서,
		컨테이너 재시작정책을 설정하는 .spec.template.spec.restartPolicy를 지정할수 있음
		저걸 never로 두면 비정상종료되었을때 재시작을 막고 잡에서 새 파드를 실행함(노드가 장애나 업그레이드등으로 정지되었을때)
		그런데 spec.parallelism과 spec.completions이 1이고 restartPolicy가 never이면 같은프로그램이 2번 실행될수있음
		만약 둘을 1보다 크게 설정하면 한번에 여러파드가 실행될수 있음
		이런 상황들을 알고있어야함
		
	5.잡 종료와 정리
		잡이 정상적으로 실행 종료되면 파드가 새로 생성되지도 삭제되지도 않고,잡도 남아있음
		파드나 잡이 삭제되지않고 남아있으면 로그에서 에러나 경고를 확인할수있고 잡의상태도 계속 확인할수있음
		특정시간을 지정해 잡실행을 종료하려면,spec.activedeadlineseconds에 시간을 지정하면됨
		그러면 그 시간에 잡 실행을 강제로 끝내고 파드실행을 종료함
		
		그러면 잡의 상태를 확인했을때 종료이유가 reason:deadlineexceeded로 표시됨
		
		잡 삭제는 kubectl delte job 잡이름 으로 사용자가 직접 삭제해야함
		그리고 잡을 삭제하면 관련 파드들도 같이 삭제됨
		
	6.잡패턴
		잡에서 파드를 병렬로 실행했을때 파드 각각이 서로 통신하면서 동작하지 않음
		각 파드는 독립적으로 동작하는걸 전제로 두는데,메일을 발송하거나 파일을 변환하는등은 분산작업이니까 한번에 실행해야해서 제대로 동작하지않음
		
		잡의 사용패턴은
			작업마다 잡을 하나씩 생성해 사용하는거보단 모든 작업을 관리하는 잡 하나를 사용하는게 좋음
			잡의 생성비용은 비쌈
			
			작업개수만큼의 파드를 생성하는거보다 파드가 여러작업을 처리하는게 좋음
			파드를 생성하는거도 비쌈
			
			워크큐를 사용한다면 카프카같은거로 구현해야함,기본설정그대로는 비효율적임



7.크론잡
	크론잡은 잡을 시간기준으로 관리함,즉 지정시간에 잡을 한번만 실행하거나,지정한시간동안 주기적으로잡을 반복실행할수있음
	실행한후에는 잡과 동일하게 동작함
	
	1.크론잡 템플릿
		yaml은
			apiversion:api버전
			kind:CronJob
			metadata:
				name:hello
			spec:
				schedule: '*/1 * * * *' //매 1분마다 실행
				jobTemplate:
					spec:
						template:
							spec:
								container:
									-name:이름
									 image:이미지
									 args:
										여기다가 셸스크립트를 넣을수있음
									 
								restartPolicy:onfailure		

		

		schedule: '*/1 * * * *'는 실행할 시간을 정하는건데 cron과 같은형식이라니까 그걸찾아보자
		어떤 작업을 실행할지는 이미지를 사용하는거고
		커맨드처럼 셀스크립트를 쓸수있고
		리스타트정책도 설정할수있음

		kubectl get cronjobs로 크론잡의 스케줄설정을 확인할수있고,현재 정지중인지 아닌지를 알수있음(가동중이면 false)
		그리고 active로 현재 잡이 몇개실행중인지 알수있고
		크론잡이 실행한 잡은 kubectl get jobs로 알수있음
		
		삭제는 kubectl delete cronjobs 크론잡이름 으로 삭제함
		그러면 크론잡이 생성한 잡들까지 한꺼번에 삭제됨
		
	2.크론잡 설정
		크론잡의 spec.startingdeadlineseconds는 지정된 시간에 크론잡이 실행되지못했을떄,값으로 설정한 시간이 지나면 실행되지않게 함
		이걸 설정안하면 시간이 많이지나도 제약없이 실행함
		
		크론잡의 spec.concurrencyPolicy는 크론잡이 실행하는 잡의 동시성을 관리함
		기본값은 allow로 잡을 여러개 동시에 실행할수 있게 함
		forbid로 설정하면 잡을 동시에 실행하지 않도록 함
		
		만약 이전에 실행했던 잡이 정상종료되지않고 실행중이면,
		만약 forbid면 해당시간에 새로 잡을 실행하지 않고 다음 시간에 실행함,즉 두개이상 실행막음
		만약 replace면 이전실행잡을 지우고 새 실행잡으로 대체함
		
		이건 스케줄밑에 넣으면 됨(크론잡공간에)
		
		spec.successfuljobshistirylimit와 spec.failedfuljobshistirylimit는 잡이 정상종료되었는지 비정상종료되었는지 로그를
		몇개까지 저장할지 설정함,기본값은 3,1이고 0으로하면 저장하지 않음
		그러면 kubectl get pods했을때 status가 compleated로 적혀서 갯수만큼 남아있음(새거로 갱신되면서)






7.서비스
1.서비스의 개념
	쿠버네티스 클러스터 안에 컨트롤러로 파드를 실행했으면,거기 접근할수 있어야하는데 그때 쓰는게 서비스임
	파드는 매번 죽었다 살아나서 재생성될수있고,그때마다 ip주소가 바뀌기때문에 직접접근하긴 머리아파서 쓰는게 서비스임
	서비스를 쓰면 서비스에 통신하면 알아서 파드로 연결해줌
2.서비스타입
	서비스타입은 크게 4가지가 있음
		clusterIP:기본타입이며 클러스터안에서만 사용할수 있음
				  클러스터 안 노드나 파드에서는 클러스터 ip를 사용해서 서비스에 연결된 파드에 접근,외부에선 이용할수없음
		
		NodePort:서비스 하나에 모든 노드의 지정된 포트를 할당함
				 node1:8080,node2:8080처럼 노드에 상관없이 서비스에 지정된 포트번호만 사용하면 파드에 접근할수 있음
				 노드의 포트를 사용하므로 클러스터 외부에서도 접근할수있음
				 특이한점은 파드가 노드1에만 실행되어있고 2에 없어도 2에 접근하면 자동으로 1로 연결해줌
				 외부에서 안으로 접글할때 가장 쉬운방법
		
		LoadBalancer:아마존,구글클라우드등에서 쿠버네티스를 지원하는 로드밸런서장비에서 사용함
					 클라우드에서 제공하는 로드밸런서와 파드를 연결한 후 해당 ip를 이용해 외부에서 파드에 접근할수 있게 해줌
					 kubectl get service로 서비스를 확인하면 external-ip에 로드밸런서 ip를 표시하는데,그걸로 외부에서 접근하면됨
		
		ExternalName:서비스를 .spec.externalName에 설정한값과 연결
					 이건 보통 클러스터내부에서 외부에 접근할때 사용,
					 특정 사이트같은데 도메인 넣어두고 접근해서 html받는식으로 씀
					 외부에 접근할떄 쓰는거라서 셀렉터가 필요없음


3.서비스 템플릿
	서비스의 기본 yaml은
		apiversion:api버전
		kind:Service 
		metadata:
			name:서비스이름1
		spec:
			type:ClusterIP
			clusterIP:10.0.10.10  사용할아이피주소,안적으면 자동으로 할당함
			selector:
				app:앱이름  자동연결할 파드 레이블값
			ports:
				-protocol:TCP
				 port:80
				 targetPort:9376
				 
	스펙의 타입에서 서비스타입을 설정할수있고
	스펙 클러스터ip에서 ip를 직접 설정할수있고
	selector에서 연결할 파드레이블 설정할수있고
	port에서 포트설정할수있음,한번에 여러개를 외부에 제공할떈 포트하위에 필드값을 설정하면 됨
	
	1.clusterIP타입 사용하기
		클러스터ip타입은 위에 있던 거임
		
		클러스터ip타입은 넷샷같은시스템파드에 접근하면 걔가 서비스로 통신하고 파드랑 연결해서 사용자와 파드를 연결시켜주는식임
		클러스터ip는 외부랑 직접연결할수없기떄문
		
		이거도 
			kubectl describe service 서비스이름
		으로 상태볼수있는데
		
		똑같이 endpoint가 연결된 하위파드들임
		
		여기 연결확인할떈 클러스터내부서비스ip로 파드하나 만들어서 거기서 연결해봐야함
		
	2.nodeport타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:NodePort
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
					 nodePort:30080 노드포트값
		이거도 다른건 다똑같고,타입선언하고 포트밑에 노드포트값만 넣어줌
		
		로컬일경우 localhost:노드포트값으로 접근하면 접근할수있고
		외부일경우 컴퓨터ip:노드포트값으로 접근할수있음
		즉 파드를 하나만들고 거기로 접근할필요없이 바로 접근할수있음
		
		내부적으로는 외부포트와 클러스터ip식으로 만들어진서비스를 매핑하는식으로 구현된듯
		
	3.LoadBalancer타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:LoadBalancer
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
		로 기본에서 그냥 타입만 바꾸면됨
		
		얘는 클러스터를 외부 로드밸런서와 연결할때 쓰기떄문에,외부와 연결한뒤 그걸 받아서 서비스를 하게됨
		뭐 쓰기는 노드포트랑 똑같음,단지 세팅할떄 외부와 연결해줘야한다는거고
		
	4.ExternalName
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:ExternalName
				externalName:url값
		이건 그냥 외부url과 연결하는거임
		구글같은데다 연결하면 그냥 구글html을 받아오고 그런식
				

4.헤드리스서비스
	.spec.clusterIP를 none로 설정하면 클러스터ip가 없는 서비스를 만들수있음
	이런걸 헤드리스 서비스라고 함
	이런건 로드밸런싱이 필요없거나 단일서비스 ip가 필요없을때 사용함
	
	헤드리스 서비스에 셀렉터를 설정하면 쿠버네티스api로 확인할수있는 엔드포인트가 만들어지고,
	서비스와 연결된 파드를 직접 가르키는 dns a레코드도 만들어짐(도메인주소와 서버의ip주소를 직접 매핑시키는거,196.123.22.1 이런식의 ip주소)
	셀렉터가 없으면 엔드포인트가 만들어지지않는데,
	셀렉터가 없어도 dns시스템은 externalName에서 사용할 cname레코드(naver.com처럼 문자로된 도메인같은걸 ip주소로 변경시키는거)는 만들어짐
	
	이건 그냥 클러스터ip설정에서 클러스터ip를 None로 주기만 하면 됨
	
5.kube-proxy
	프록시는 쿠버네티스에서 서비스를 만들었을떄,클러스터ip나 노드포트로 접근할수 있게 만들어서 실제 조작을 하는 컴포넌트임
	kube-proxy가 네트워크를 관리하는 방법은 userspace,iptables,ipvs가 있음 
	
	1.userspace
		유저스페이스는 클라이언트에서 서비스의 클러스터ip를 통해 요청을하면 iptables을 거쳐서 프록시가 요청을 받고,
		서비스의 클러스터ip는 연결되어야할 파드로 연결해줌,즉 포워드프록시임
		
	2.iptables
		iptables는 클라이언트가 iptables로 직접 접근하고,프록시는 iptables을 관리하는역할만 함,
		직접 클라이언트에서 트래픽을 받진 않고 클라이언트에서 오는 모든 요청은 iptables을 거쳐서 파드로 직접 전달됨
		그래서 중간에 프록시를 거치지않기때문에 요청처리성능이 좋음
		그리고 특징으론 유저스페이스는 프록시가 연결을 책임지기때문에 파드하나에 연결실패하면 다른파드에 계속 연결시켜주려고 재시도하는데,
		이건 책임이 클라이언트에 있으니까 그냥 실패하면 실패임
		컨테이너에서 readinessprobe가 설정되어있고 헬스체크가 성공해야 연결이 이루어짐
		
	3.IPVS
		ipvs는 리눅스에 있는 l4로드밸런싱기술임
		리눅스 커널 안 네트워크 프레임워크인 넷필터에 포함되어있음
		즉 ipvs커널 모듈이 노드에 설치되어있어야함
		
		구조자체는 iptables과 같은데,이건 클러스터ip가 가상서버로 동작하고,백엔드파드들이 실제서버임
		얘는 커널공간에서 동작하고 데이터구조가 해시테이블이라서 iptables보다 빠르고 좋은성능을 내고
		로드밸런싱 알고리즘이 많아서 이걸 이용할수있음
			rr:프로세스사이에 우선순위를 두지않고 순서와 시간단위로 cpu할당
			lc:접속 개수가 가장 적은 서버를 선택
			dh:목적지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sh:출발지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sed:응답속도가 가장 빠른 서버를 선택함 
			nq:sed+활성접속개수가 0개인 서버를 먼저 선택함
		
		가 있음
		














