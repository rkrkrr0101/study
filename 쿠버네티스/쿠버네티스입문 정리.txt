1.소개
	스킵
2.설치
	스킵

3.쿠버네티스로 컨테이너 실행	
1.kubectl
	kubectl의 기본 구조는
		kubectl 커맨드 타입or이름 (플래그)  플래그는 옵션
	임
	만약 kubectl get pod 를 하면 포드 전체를 겟(표시)하고
	   kubectl get abcd를 하면 abcd라는 이름을 가진걸 겟함
	   
	   
	여기서 커맨드에는
		get:명시된 데이터를 받아옴
		run:명시된 이름으로 명시된 이미지의 파드를 생성함
		apply:명시된 위치의 yaml파일을 기반으로 선언적으로 디플로이먼트를 생성해서 파드를 생성함
		scale:파드의 갯수를 늘이거나 줄임
		expose:명시된이름으로 명시된 타입의 서비스를 생성함
		port-forward:명시된 이름의 객체를 뒤에 포트(8080:25500)의 외부포트로 매핑함
		logs -f:명시된이름의 객체의 로그를 수집함
		exec:명시된 이름의 객체에 뒤에 나올 명령을 실행하라고 함
		  (kubectl (-n default) (exec) ((my-pod) (-c my-container)) (-- ls /) )
		          디폴트 네임스페이스에서 실행해라 마이포드를    마이컨테이너에 있는걸    쿠버네티스관련옵션을 전부 종료시키는
			마이컨테이너에 있는 마이포드의 쿠버네티스관련옵션을 전부 종료시키는 명령을 실행해라
		api-resources:사용할수 있는 자원들을 표시함	
			
	2.kubeconfig환경변수
		kubectl의 환경변수는 home/.kube/config에 있음
		여기서 클러스터에서 사용할수 있는 자원을 확인하는건 kubectl api-resources로 확인할수있음
		
		도커 데스크톱으로 쿠버네티스를 쓰면 자동으로 kubeconfig가 생성되고,
		--kubeconfig옵션으로 다른 설정파일을 지정할수 있음
		다중 클러스터에 다른 인증정보로 접근할때 사용함
		
	3.다양한 사용 예
		단순히 명령실행말고,파이프라인으로 현재값의 출력을 다음명령의 입력으로 넣을수도 있고 그런식으로 스크립트식 사용도 가능함
		
			
			
			
			
2.디플로이먼트를 이용해 컨테이너 실행
	앞에서 run으로 생성한건 직접 파드를 하나 추가한거고,보통은 apply로 디플로이먼트를 생성함
	이렇게해야 선언적으로 생성할수있어서 관리하기가 편함
			
3.클러스터 외부에서 클러스터 안 앱에 접근하기
	쿠버네티스 외부에서 쿠버네티스 내부에 접근하려면,직접 접근하면 안되고 쿠버네티스의 서비스를 통해서 접근해야함
	그래서 서비스를 생성해서 그쪽을 통해서 접근해야함
		kubectl expose deployment 이름 --type=NodePort
	하면 노드포트타입의 디플로이먼트를 생성하고(노드포트는 모든 노드의 포트를 할당함,즉 전체를 다받음 )
		kubectl get service로 포트를 확인하고 그 포트로 접속하면됨
		좀더 자세히 보려면
			kubectl describe service 이름
		으로 상세하게 볼수있음
			
			
			
			
			
			
4.쿠버네티스 아키텍쳐
1.쿠버네티스 클러스터 전체 구조
	쿠버네티스 클러스터는 클러스터를 관리하는 마스터와 실제컨테이너를 실행시키는 노드로 구성됨
	마스터에는 etcd(모든 설정등 적는건 여기다들어있음),apiserver(모든건 여기통해서 입출력을 함) 등이 들어있음
	
	노드는 kubelet,kube-porxy,docker등 컴포넌트가 실행되고,실제 사용하는 컨테이너의 대부분은 노드에서 실행됨
	
	구조는
		쿠버네티스에 명령을 주면 리버스프록시로 마스터api에 명령을 전달하고 걔가 노드랑 etc등으로 명령을 전달하고 실행시킴
	
	
	쿠버네티스의 관리용 컴포넌트들도 다 컨테이너로 실행됨
	
2.쿠버네티스의 주요 컴포넌트
	쿠버네티스는 기본적으로 클러스터를 관리함
	클러스터는 단일컴퓨터뿐만 아니라 여러대컴퓨터를 묶음으로 다루는걸 뜻하므로 여러가지의 컴포넌트를 포함함
	
	쿠버네티스의 컴포넌트는 관리에 필수인 마스터컴포넌트,노드컴포넌트와 추가로 붙인 애드온컴포넌트로 나눠짐
	
	1.마스터컴포넌트
		etcd:etcd는 키밸류 저장소임
			분산시스템에서 노드 사이의 상태를 공유하는,데이터베이스 역할을 함
			etcd는 서버 하나당 프로세스 하나만(즉 전체에서 하나밖에없음,클러스터링등으로 같은걸 복사할순있지만)존재함
			
		kube-apiserver:얘는 쿠버네티스의 api를 사용할수 있도록하는 컴포넌트임
						얘는 클러스터로 온 명령이 유효한지 검증하고(문법과 권한)그걸 실행해서 돌려줌
						얘는 수평적으로 확장 가능하니까(어짜피 api서버라서 유일성같은거없음)서버여러대에 여러개설치가능
		
		kube-scheduler:얘는 클러스터 안에 자원 할당이 가능한 노드중 알맞은 노드를 선택해서 파드를 생성하는 컨포넌트
						파드는 여러 요구조건을 받을수있으며,거기에 맞는 노드를 선택해서 생성함
		
		kube-controller-manager:얘는 파드들을 관리하는 컨트롤러를 실행하는 컴포넌트임
								클러스터에서 새로운 컨트롤러를 생성하고 실행할때 컨트롤러 매니저의 큐에 넣어서 실행하는식으로 동작함
		
		cloud-controller-manager:얘는 쿠버네티스의 컨트롤러를 클라우드와 연결해서 관리하는 컴포넌트임,필요해지면보자
		
	2.노드용 컴포넌트
		kubelet:얘는 클러스터 안의 모든 노드에서 실행하는 컴포넌트,파드컨테이너들의 실행을 직접 관리함,파드스펙이라는 조건설정을 전달받아서 실행하고
				컨테이너가 정상적으로 실행되는지 헬스체크를 진행함,단 노드안에 있는 컨테이너라도 쿠버네티스가 안만들었으면 관리하지않음
				(컨테이너안 파드 지웠을때 바로재시작거는게 얘인듯)
				
		kube-proxy:클러스터안에 별도의 가상 네트워크를 설정하고 관리하는 컴포넌트
		
		컨테이너 런타임:실제로 컨테이너를 실행시키는 컴포넌트,대표적으로 도커가 있음
		
	3.애드온
		네트워킹 애드온:클러스터 안에 가상네트워크를 구성해 사용할떄 프록시이외에 네트워킹 애드온을 사용함,얘가 직접 서버구성할때 가장 까다로움
		
		dns애드온:클러스터 안에서 동작하는 dns서버,쿠버네티스 서비스에 dns레코드를 제공함,쿠버네티스 안에 실행된 컨테이너들은 자동으로 dns서버에 등록됨
				주로 coreDNS를 사용함 
				
		대시보드 애드온:kubectl로 명령 주지만,gui로 볼때 대시보드애드온으로 사용할수있음
		
		컨테이너 자원 모니터링:컨테이너들의 자원사용량등을 시계열형식으로 저장해서 볼수있음
		
		클러스터 로깅:클러스터 안 개별 컨테이너의 로그와 구성요소의 로그를 모아서 보는 애드온
		
			
3.오브젝트와 컨트롤러			
	쿠버네티스는 오브젝트와 오브젝트를 관리하는 컨트롤러로 나눠짐
	사용자는 템플릿등으로 쿠버네티스에 자원의 바라는 상태를 정의하고,컨트롤러는 바라는상태와 현재상태가 일치하도록 오브젝트를 생성/삭제함
	오브젝트에는 파드,서비스,볼륨,네임스페이스 등이 있고 컨트롤러에는 레플리카셋,디플로이먼트,스테이트풀셋,데몬셋,잡등이 있음
	
	1.네임스페이스
		네임스페이스는 클러스터 하나를 논리적인 단위로 나눠서 실행하는것,
		이해하자면,컴퓨터 하나에 특정 폴더에 프로그램 바로가기로 싹 몰아두고 이름붙이는느낌임,그래서 그폴더를 전부 실행하거나 실행에 제한걸거나 하는식
		
		네임스페이스를 지정할때는 
			--namespace=붙일이름 
		으로 하나하나붙여도되는데
		
		그냥 디폴트값을 바꿀수도있음
			kubectl config current-context
		로 현재 컨텍스트 이름를 확인하고
			kubectl config set-context 컨텍스트이름 --namespace=붙일이름
		으로 디폴트값을 바꾸면 새로생성한값의 네임스페이스가 바뀜
		
		네임스페이스 전체검색하려면
			kubectl get pod --all-namespaces
		하면됨
		
		바꾼값 다시 디폴트로 바꾸려면 
			--namespace=""
		하면됨
		
	2.템플릿
		쿠버네티스 클러스터의 오브젝트나 컨트롤러가 어떤 상태여야 하는지 적용할떈 yaml형식의 템플릿을 적용함
		템플릿은 들여쓰기로 구조가 바뀌고(파이썬처럼),
		scalars(스트링,넘버),sequences(어레이,리스트),mappings(해시,딕셔너리) 3기초요소로 표현됨
		
		템플릿의 기본형식은
			apiversion:v1
			kind:Pod(생성종류)
			metadata:
			spec:
		로 구성됨
		apiversion은 api버전이고(쿠버네티스버전과 관련된)
		kind는 어떤 오브젝트나 컨트롤러의 작업인지를 명시하고
		metadata는 해당 오브젝트의 이름이나 레이블등을 설정하고
		spec는 파드가 어떤 컨테이너를 가지고 실행하며,실행할때 어떻게 동작해야할지를 명시함
		
		kubectl explain 생성종류  로 현재 생성할거에 무슨 하위필드가 있는지 출력해서 볼수있음
		
		하위필드를 포함해 특정필드를 커맨드라인에서 지정할때는 .metadata.anno이런식으로 .으로 이어가면됨,맨앞에도 .붙이는거에 주의
		필드설명없이 그 아래에 속한 모든필드를 보려면 --recursive옵션을 쓰면됨
		
		
			
			
			
			
5.파드
1.파드개념
	쿠버네티스는 파드라는 단위로 컨테이너를 묶어서 관리하므로,파드는 안에 컨테이너가 여러개로 구성되긴함
	그리고 파드 하나는 ip를 공유하고,내부의 컨테이너들끼리 포트로 구분해가며 데이터를 받음
	그리고 컨테이너 하나에 프로세스를 여러개 실행시킬수도 있지만,그러면 개빡세니까 보통은 컨테이너 하나에 프로세스 하나씩 해서 돌리는거같음
	
2.파드사용하기
	파드를 설정할때는
	yaml에
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
	
	이런식으로 설정함
	저기서 라벨은 오브젝트를 식별하는 레이블
	그리고 상위 바로 밑에 -를 붙이는건 여기서부터 하위필드를 배열로 묶겠다는 뜻임


3.파드 생명 주기
	파드는 생성부터 삭제까지의 과정에 생명주기가 있음
	
		pending:파드를 생성하는 중일때 나옴,이미지를 다운로드한후에 컨테이너를 실행해야하니까 시간 좀 걸림
		running:파드 안 모든 컨테이너가 실행중인 상태,1개이상의 컨테이너가 실행중이거나 시작,재시작상태일수 있음
		succeeded:성공적으로 모든 컨테이너가 실행 종료된상태,정상종료니까 재시작되지않음
		failed:파드 안 모든 컨테이너중 정상적인 실행 종료가 되지 않은 컨테이너가 있는 상태
		unknown:파드의 상태를 확인할수 없는 상태,파드가 있는 노드와 통신할수 없을때 주로 나옴
		
	
	현재 파드 생명주기는 
		kubectl describe pods 파드이름
	을 실행하고 status를 보면 나옴
	거기서 밑에 type가 있는데
	각 타입별 정보는
		initialized:모든 초기화컨테이너가 성공적으로 시작 완료됨
		Ready:파드는 요청을 실행할수 있고 모든 연결된 서비스의 로드밸런싱 풀에 추가되어야 한다는 뜻
		containersready:파드안 모든 컨테이너가 준비상태라는 뜻
		podScheduled:파드가 하나의 노드로 스케줄을 완료했다는 뜻
		unschedulable:스케줄러가 자원의 부족이나 다른 제약등으로 당장 파드를 스케줄 할수 없다는 뜻
	
4.kubelet로 컨테이너진단
	컨테이너가 실행 된 후에는 kubelet가 주기적으로 컨테이너를 진단함,이때 필요한 프로브는 2개가있음
		livenessprobe:컨테이너가 실행됐는지 확인함,이게 실패하면 kubelet는 컨테이너를 종료시키고 재시작 정책에 따라 컨테이너를 재시작함
		readinessprobe:컨테이너가 실행된 후 실제로 서비스 요청에 응답할수 있는지 진단함
					   이 진단이 실패하면 엔드포인트 컨트롤러는 해당 파드에 연결된 모든 서비스를 대상으로 엔드포인트 정보를 제거함
					   즉 서비스 연결이 안될거같으면 거기 연결 못하게 다지워버림
					   
					   
	
	컨테이너 진단할땐 컨테이너가 구현한 핸들러를 kubelet가 호출해서 실행함
	핸들러에는 3종류가 있음
		execaction:컨테이너 안에 지정된 명령을 실행하고 종료코드가 0일때 성공
		tcpsocketaction:컨테이너 안에 지정된 ip,포트로 tcp상태를 확인하고 열려있으면 성공
		httpgetaction:컨테이너 안에 지정된 ip,포트,경로로 http get명령을 보내서 응답코드가 200~400사이면 성공
		
	결과는 success,failure,unknown3개가 있음,언노운은 진단이 실패하서 컨테이너 상태를 알수없을때 나옴
		
	
5.초기화 컨테이너
	초기화 컨테이너는 앱 컨테이너가 실행되기 전 파드를 초기화함
	보안상이유로 앱 컨테이너이미지와 같이 두면 안되는 앱의 소스코드를 별도로 관리할때 유용
	초기화 컨테이너의 특징은
		초기화 컨테이너는 여러개를 구성할수 있음,초기화컨테이너가 여러개있으면 파드템플릿에 명시한 순서되로 초기화컨테이너가 실행됨
		
		초기화 컨테이너의 실행이 실패하면 성공할때까지 재시작함,그래서 무조건 순서대로 실행되니까 절차적으로 실행시켜서,
		쿠버네티스의 선언적이라는 특징에서 벗어날수있음
		
		초기화 컨테이너가 모두 실행 된 후 앱 컨테이너 실행이 시작됨
	
	
	그래서 이런특징을 이용해 파드를 실행할때 앱 컨테이너가 외부의 조건을 만족할때까지 기다렸다가 실행하게 만들수있음
	그리고 초기화 컨테이너는 readinessprobe를 지원하지 않음,파드가 준비되기 전 실행하고 사라지는 컨테이너이기때문
	
	초기화 컨테이너 yaml은
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			init컨테이너들
				-이름:이름
				이미지:이미지
				커맨드:실행하고 할 커맨드를 입력함(몇초 기다려라 이런거)
				
6.파드인프라컨테이너
	쿠버네티스에는 모든 파드에서 항상 실행되는 pause라는 컨테이너가 있음,이걸 파드 인프라 컨테이너 라고 함
	얘는 파드 안 기본 네트워크로 실행되고,프로세스 식별자가 1로 설정되므로 컨테이너의 부모 컨테이너 역할을 함
	파드 안 다른 컨테이너는 pause가 제공하는 네트워크를 공유해서 사용함,그래서 다른컨테이너가 재시작돼도 ip가 유지되지만,
	얘가 재시작되면 ip가 바뀌고 파드 안의 다른 모든 컨테이너도 전부 재시작함
	
	이 인프라를 바꿀수도 있는데 --pod-infra-container-image옵션으로 다른 컨테이너를 인트라컨테이너로 지정할수 있음
	

7.스태틱 파드
	얘들은 api서버를 거치지 않고 kubelet가 직접 실행하는 파드들임
	여기에 추가하고싶으면 --pod-mainfest-path라는 옵션에 지정한 디렉토리에 스태틱파드에 추가할 파드들을 넣어두면 kubelet가 감지해서 실행함
	
	얘들은 kubelet가 직접 관리하면서 이상이 생기면 재시작함
	그리고 kubelet가 실행중인 노드에서만 실행되고,다른노드에서는 실행되지 않음
	그리고 apiserver로 파드를 조회할수는있지만,거기에 명령을 내릴순 없음
	
	보통 스태틱파드는 apiserver나 etcd같은 시스템파드를 실행하는 용도로 많이 사용함 


8.파드에 cpu와 메모리 자원 할당
	만약 노드 하나에 자원 사용량이 많은 파드들이 모여있으면 파드들의 성능이 나빠지고,전체 클러스터의 자원사용 효율도 낮음
	어떤 노드에는 파드가 없어서 cpu메모리가 남고, 어떤 너드는 파드가 많아서 모자라는 그런일이 나타날수 있음
	
	그래서 쿠버네티스에서는 파드에서 자원의 최소치와 최대치를 지정할수 있게 준비해뒀음
	.spec.containers[].resources.limits.cpu
									   .memory
								.requests.cpu
										 .memory
							
	여기서 리미트는 그 파드가 쓸수있는 한도고,리퀘스트는 최소한 그정도는 써야한다는거임
	즉 노드에 배치될떄, 리퀘스트만큼 자원 여유가 있는 노드여야 거기에 스케줄링해서 배치해서 실행되고,
	그런게 없으면 pending로 대기하고 실행하지않다가 자원여유가 생기면 실행함
	
	리미트는 자원의 최대량임,파드가 아무리 많이써도 리미트 이상은 사용할수 없게 막음
	
	노드에 배치할때는 현재사용량을 보는게 아니라 리미트와 리퀘스트만 봄
	리미트가 설정되어있으면 리미트의 총합만큼을 보고 자리가 남으면 거기넣고,리미트가 설정안되면 리퀘스트의 총합을 가지고 넣음
	리미트가 설정안되면 오류가 날수있지만,노드자원을 최대한 많이 사용할수 있다는 장점이 있긴함
	
	그리고 cpu는 코어의 0.1단위로 잘라서 사용할수 있음

9.파드에 환경변수 설정
	컨테이너를 사용할때 장점은,개발환경에서 만든 컨테이너에 환경변수만 바꿔서 실제환경에서 실행해도 그대로 동작한다는 점임
	여기서 환경변수 설정하는방법은 스펙.컨테이너 밑에 env에 넣어주면됨
	
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
				env:
					-이름:이름1
					 값:값1
					-이름:이름2
					 valuefrom:
						필드ref:
						필드path:spec.nodeName이나 metadata.name,status.podIP등등 참조값 전부가능
					-이름:이름3
					 valuefrom:
						resourcefieldref:
							컨테이너이름:가져올컨테이너이름
							resource:requests.cpu,limits.cpu등등 
	
	이런식으로 가져오거나 설정할수있음
							
	이름은 환경변수의 이름이고(즉 저기에 적힌거:값 이런식으로 그대로 바로 적용됨 사용자정의아니면,즉 cpu사용량같은거 적어두면 바로적용됨)
	값(value)은 직접 적은 상수값
	valuefrom은 값을 어디서 참조하는거
	fieldref는 파드의 현재 설정 내용을 값으로 설정한다는 선언(현재파드내에서 가져온다는거)
	fieldpath는 어디서 값을 가져올것인지,즉 값을 참조하려는 항목의 위치를 지정
	resourcefieldref는 컨테이너에 cpu메모리 사용량을 얼마나 할당했는지에 관한 정보를 가져옴
	containername은 환경변수 설정을 가져올 컨테이너의 이름
	resource는 어떤 자원의 정보를 가져올지 설정


10.파드에 환경설정 내용 적용
	파드는 환경설정 내용을 yaml하나에 모두 작성한 후 적용해야함
	적용할떄는
		kubectl apply -f pod-all.yame
	을 실행하면됨
	
	확인할땐 
		kubectl exec -it kubernetes-simple-pod sh
	를 실행해서 컨테이너 안에 접속하고
		env
	치면 나옴
	
	
11.파드 구성 패턴
	1.사이드카패턴
		원래 사용하던 컨테이너의 기능을 확장하거나 강화하는 용도의 컨테이너를 추가하는것
		기본 컨테이너는 원래기능에만 충실하게 구성하고,나머지 공통부가기능들은 사이드카 컨테이너를 추가해서 사용하는것
		로그수집 컨테이너처럼 파일긁어서 보내는 역할만 하는 컨테이너가 예,이건 어딜붙여도 파일저장만 하면 똑같이쓸수있음
	2.앰배서더 패턴
		얘는 파드안에 프록시역할을 하는 컨테이너를 추가하는 패턴
		즉 파드 안에서 외부서버에 접근할때 내부프록시에 접근하게 하고,연결은 프록시에서 알아서 처리함
		이러면 파드의 트래픽을 더 세밀하게 제어할수 있음
	3.어댑터 패턴
		어댑터 패턴은 파드 외부로 노출되는 정보를 표준화하는 어댑터 컨테이너를 사용한다는 뜻임
		즉 파드 내부에서 정보를 어댑터로 보내고,저기서 정보를 취합해서 중앙모니터링에 보내면 거기서 모니터링하는데 이용하는 그런식
		





6.컨트롤러
1.레플리케이션 컨트롤러
	초창기 레플리카세트같은거임,요즘은 안씀
2.레플리카셋
	얘는 같은 파드를 복사해서 배포하는식으로 사용하는 컨트롤러임
	항상 선언한것과 같은 숫자의 파드가 클러스터 안에서 실행되도록 관리함
	
	그리고 얘는 롤링업데이트(25%정도씩 잘라서 파드를 업데이트-재시작하는것)을 하지못하니까 이거필요하면 디플로이먼트를 써야함
	1.레플리카셋 사용하기
	이거도 파드랑 똑같이 사용하면됨
		apiversion:앱버전
		kind:replicaset
		metadata:
			name:이름
		spec:
			template:
				metadata:
					name:이름1
					labels:
						app:이름1
				spec:
					containers:
						-name:이름1
						 image:이미지
						 ports:
						 -containerPort:포트번호
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
						
						
	자세한 명세는 .spec에 적으면되고,스펙밑에는 무슨파드를 실행할지를 적어야하니까 메타데이터 스펙이 한번 더 나옴	
	그리고 .spec.template.spec.container[]필드 밑에 파드의 정보를 설정함	
	.spec.replicas는 파드의 갯수를 설정하는것
	.spec.selector는 어떤 레이블의 파드를 선택해서 관리할지를 정함,즉 label.app에 있는거중 하나를 선택해서 그것만 관리함(몇개인지)
	즉 matchlabels랑 labels의 값이 같아야함
	만약 셀렉터를 비워두면 labels.app에 있는걸 기본값으로 설정함
	
	이 yaml을 apply로 클러스터에 적용하면됨
	
	여기서 만약 만들어진 파드를 지우면,바로 재생성이 되고,
	만약 yaml을 수정해서 파드의 갯수를 줄이면,바로 하나를 지움
	
	2.레플리카셋과 파드의 연관관계
		파드는 레이블기준으로 관리하므로 레플리카셋과 파드는 느슨하게 결합되어 있음
		레플리카셋과 파드를 한꺼번에 삭제할때는 delete replicaset 컨테이너이름으로 하면 삭제되지만,--cascade=false옵션을 주면
		연쇄삭제는 되지않고 레플리카셋만 삭제됨,그러면 현재 실행중인 파드를 관리하는 레플리카셋을 추가로 만들수도 있음(현재설정을 바꿔야하는게 바꿀게많을때)
		
		kubectl get replicaset으로 레플리카셋의 상태를 볼수있는데
		거기서 desired는 설정할때 지정한 파드의 수,current는 현재 파드의 수 임,정상적으로 동작하면 두개는 같아야함
		
		그리고 파드의 레이블을 바꾸면(.metadata.labels.app),
		레플리카셋은 레이블기반으로 체크를 하는데 레이블이 바뀌었으니까 다시 새로운걸 만들게됨
		
		이런식으로 레이블 설정 변경으로 실행중인 파드를 재시작하지 않고 서비스에서 분리해 디버깅하는등으로 사용할수 있음
		
3.디플로이먼트
	디플로이먼트는 쿠버네티스에서 상태가 없는 앱을 배포할때 사용하는 가장 기본적인 컨트롤러임
	요즘은 보통 디플로이먼트로 앱을 배포함
	
	디플로이먼트는 레플리카셋을 관리하면서앱 배포를 더 세밀하게 관리함
	단순히 실행시켜야 할 파드의 수를 유지하는거뿐 아니라,앱을 배포할때 롤링업데이트하거나,배포중 잠시 멈췄다가 배포하거나,업데이트를 롤백하거나 할수있음
	
	1,디플로이먼트 템플릿
		yaml구성은
			apiversion:api버전
			kind:Deployment
			metadata:
				name:이름1
				labels:
					apps:이름1
			spec:
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
				template:
					metadata:
						labels:
							app:이름1
						spec:
							containers:
								-name:이름1
								 image:이미지
								 ports:
									containerPort:포트번호
	
		이렇게 구성됨
		
		파드를 몇개구성할지는 스펙밑에 레플리카고(레플리카셋이랑 똑같이)
		.spec.selector.matchLabels의 하위필드는 메타데이터의 레이블의 하위필드랑 같아야함
		파드의 설정정보가 있는 스펙.템플릿.스펙.컨테이너[]에 컨테이너의 이름과 이미지가 있음
		
		즉 레플리카셋이랑 별다른건 없음
		
		디플로이먼트가 제대로 실행되었는지는 kubectl get deploy,rs,pods 로 3개 다 확인할수 있음(디플로이먼트,레플리카셋,파드)
		
		즉 디플로이먼트는 레플리카셋을 감싸는 어댑터임(롤링업데이트등을 사용하게 해주는)
		
		디플로이먼트에서 이미지를 업테이트 하는방법은 총 3개가 있음
			kubectl set으로 컨테이너이미지를 지정하는것
			kubectl edit로 파드를 연다음 이미지정보를 수정하는것
			yaml을 수정하고 kubectl apply하는것
			
		보통 1,3이 쓰이는데 yaml을 보통 사용하는듯
		그리고 이렇게 디플로이먼트의 설정이 변경되면,새로운 레플리카셋이 생성되고,거기에 맞게 파드들이 변경됨(파드들이 새로생성됨)
		
		
		
	2.디플로이먼트 롤백
		컨테이너 이미지 변경 내역은
			kubectl rollout history deploy 디플로이먼트이름
		으로 확인할수있음
		
		만약 특정 리비전의 상세내용을 확인하려면 
			kubectl rollout history deploy 디플로이먼트이름--revision=리비전숫자 
		를 주면 확인할수있음(이미지버전이라던가)
		
		그 버전으로 롤백하려면
			kubectl rollout undo deploy 디플로이먼트이름--to-revision=리비전숫자
		로 버전선택해서 돌릴수있음
		돌리면 그번호가 삭제되고 새로 갱신되어서 제일 큰숫자로 바뀌어서 나옴(총 3개일때 2번으로 언두하면 2->4로바뀜)
		
		이 숫자는 겹치지않고 계속 커져나감
		
		
	3.파드개수 조정
		현재 실행중인 디플로이먼트의 파드 개수를 조정하려면 
			kubectl scale deploy 디플로이먼트이름 --replicas=파드갯수
		를 해주면됨
			
	4.디플로이먼트 배포정지,재개,재시작하기
		kubectl rollout로 진행중인 배포를 멈췄다가 다시 시작할수 있음
		
			kubectl rollout pause deployment/디플로이먼트이름
		으로 업데이트를 멈추고 
		
			kubectl rollout resume deployment/디플로이먼트이름
		로 다시시작할수있음
		
		그리고 디플로이먼트를 재시작시킬땐
			kubectl rollout restart deployment/디플로이먼트이름
		하면됨
		재시작은 디플로이먼트 스테이트풀셋 데몬셋에서 가능함
		
	5.디플로이먼트 상태
		배포중에는 디플로이먼트 상태가 변함
		progressing(진행)이었다가 compleate(완료),혹은 failed(실패)로 바뀜
		
		kubectl rollout status로 배포 진행 상태를 확인할수 있음
		
		디플로이먼트가 
			새로운 레플리카셋을 만들떄,
			새로운 레플리카셋의 파드수를 늘릴때,
			예전레플리카셋의 파드수를 줄일떄,
			새로운 파드가 준비상태가 되거나 이용가능한 상태가 되었을때
		진행상태로 표시됨
		
		배포가 끝나면 완료가 되고 종료코드가 0으로 표시됨
			디플로이먼트가 관리하는 모든 레플리카셋이 업데이트 완료되었을때
			모든 레플리카셋이 사용가능해졌을때
			예전 레플리카셋이 모두 종료되었을때
		완료가 됨
		
		배포중 이상이 있으면 실패가 됨
			쿼터부족(자원부족)
			readinessprobe실패
			컨테이너 이미지 가져오기 실패
			권한부족
			제한범위 초과
			앱실행조건 잘못지정
		일때 실패함
		
		템플릿에 .sepc.progressDeadlineSeconds항목을 추가하면 지정된 시간이 지났을때 실패로 처리할수있음
		
		
4.데몬셋
	데몬셋은 클러스터 전체 노드에 특정 파드를 실행할때 사용하는 컨트롤러임
	클러스터안에 새롭게 노드가 추가되었을때,자동으로 데몬셋이 해당 노드에 파드를 실행시키고,
	만약 노드가 클러스터에서 빠지면 해당 노드에 있던 파드는 그냥 사라짐
	
	즉 모든 노드에 하나씩 꼭 있어야하는 파드를 실행시킬때 사용함(로그수집같은)
	
	1.데몬셋 템플릿
		데몬셋의 템플릿은
			apiversion:api버전
			kind:daemonset
			metadata:
				name:이름1
				namespace:지정할네임스페이스(보통 시스템에넣는듯 컨셉상 그런느낌이라서)
				labels:
					k8s-app:fluentd-logging 이름2
			spec:
				selector:
					matchLabels:
						name:이름1
				updateStrategy:
					type:Rollingupdate
				template:
					metadata:
						labels:
							name:이름1
					spec:
						container:
							-name:이름1
							 image:사용할이미지
							 env:(환경변수)
								-name:환경변수1
								 value:환경변수1값
							resources:
								limits:
									memory:메모리값
								requests:
									cpu:cpu값
									memory:메모리값 
									
	
				
		이런식으로 나감
		보통 데몬셋은 시스템같은 느낌으로 사용되기때문에 네임스페이스를 별도로 만들어서 사용함(ku-system이런 이름으로 )
		그리고 레이블은 k8s-app필드에 값은 fluentd-logging로 줌(이건 키워드인지 아닌지 모르겠음 나중에해보자)
		데몬셋을 업데이트 하는 방법은 updateStrategy.type필드값에 적힌대로 됨,롤링업데이트와 온딜리트 2개가있음
		컨테이너의 이미지는 컨테이너에 있음
		
		이런후 똑같이 apply하면됨
	
	2.데몬셋의 파드업데이트 방법 변경하기
		이거도 롤링업데이트하면 순차적으로 바뀌고,온딜리트하면 삭제되기전까진 안바뀜
		이거 바꿀때는 
			kubectl edit daemonset 데몬셋이름 -n 네임스페이스이름
		으로 에디터띄우고 값바꾸면 됨
		
	
		
5.스테이트풀셋
	앞에서 나온 레플리카셋,디플로이먼트는 상태가 없는 파드들을 관리하는 용도고,상태가 있는 파드들을 관리하려면 스테이트풀셋을 써야함
	
	스테이트풀셋을 사용하면 볼륨을 이용해서 데이터를 저장한후 그 데이터를 파드가 재시작해도 유지할수 있음
	여러 파드사이에 순서를 지정해서 실행되게 할수도 있음
	
	1.스테이트풀셋 사용하기
		yaml구조는
			apiversion:api버전
			kind:Service //서비스도 있어야함 상태저장해야하니까
			metadata:
				name:서비스이름1
				labels:
					app:서비스이름1
			spec:
				ports:
					-port:포트번호1
					 name:web 포트이름1
				clusterip:none  //내부망이니까
				selector:
					app:서비스이름1
					
			
			apiversion:api버전
			kind:StatefulSet
			metadata:
				name:web 포트이름1
			spec:
				selector:
					matchLabels:
						app:스테이트풀이름1
				servicename:서비스이름1
				replicas:파드갯수
				template:
					metadata:
						labels:
							app:스테이트풀이름1
					spec:
						terminationGracePeriodSeconds:10
						container:
							-name:스테이트풀이름1
							 image:사용이미지
							 ports:
								-containerPort:포트번호1
								 name:포트이름1


		여기서 스테이트풀은 통신을 해서 저장하고 읽어야하기때문에 서비스가 있어야하고,스테이트풀에서 서비스를 받아야함
		서비스이름과 스테이트풀셋에서 만들어질 파드 이름을 조합하면 클러스터안에서 사용하는 도메인을 만들수있음,파드이름.서비스이름 형식
		그리고 스테이트풀셋의 이름을 web(포트이름1)로 설정하고
		terminationGracePeriodSeconds는 그레이스풀의(실행중인 프로세스를 종료할때 끝낼시간을 주는것)대기시간을 설정한것
		
		그리고 yaml저장하고 apply하면됨
		
		스테이트풀셋으로 만들면 파드이름뒤에 난수가붙는게 아니라 포트이름뒤에 0,1,2순서대로 붙음
		실행할때는 작은숫자부터 실행되고,만약 앞에꺼가 실행되지않으면 뒤에거도 전부 실행되지 않음
		그리고 삭제할때는 큰숫자부터 삭제됨


	2.파드를 순서없이 실행하거나 종료하기
		기본값은 순서대로 관리하는거지만 .spec.podManagementPolocy필드로 순서를 없앨수도 있음
		기본값은 OrderedReady(순서대로)고 Parallel로 순서없이로 바꿀수있음
		
		실행중인 스테이트풀에서는 바꿀수없고,새로운 스테이트풀을 만들어야함
	
	3.스테이트풀셋으로 파드업데이트 하기
		업데이트방법은 .spec.updateStrategy.type에 설정할수 있음
		기본값은 롤링업데이트
		
		그리고 마지막거부터 업데이트되고
		.spec.updateStrategy.rollingUpdate.partition값을 바꾸면 그거보다 큰값만 업데이트됨
		즉 파드들이 분할이 됨
		이렇게하면 레이블항목에 레이블이 두개로 나눠지는데,
		
		이걸 사용하면 스테이트풀셋이 관리하는 전체파드들 중에서 특정 파드에만 서비스를 할수있게됨


6.잡
	잡은 실행된 후 종료되어야 하는 성격의 작업을 실행할때 사용하는 컨트롤러
	특정갯수만큼의 파드를 정상적으로 실행종료함을 보장함
	만약 파드실행실패,하드웨어장애발생,노드재시작등 문제가 발생하면 다시 파드를 실행함
	즉 일회성으로 뭘 실행시켜야할때 쓰거나 그럴듯
	
	1.잡 사용하기
		yaml은
			apiversion:api버전
			kind:job
			metadata:
				name:이름1
			spec:
				template:
					spec:
						containers:
							-name:이름1
							 image:이미지
							 command:실행할커맨드
						restartPolicy:never
				backofflimit:4
				
		restartPolicy는 never로 설정하면 파드가 항상 성공으로 끝나게하고,
		onfailure는 파드 안 컨테이너가 비정상종료하거나 다양한 이유로 정상종료되지않으면 컨테이너를 다시시작하게 함
		backofflimit는 잡 실행이 실패했을때 몇번까지 반복할것인지 설정함
		보통 한번실패하고 좀 기다렸다가 다시시작하고 그런식으로 점점 시간간격을늘리다가 실행성공하면 다시 재시도횟수는 0으로 초기화됨
	
	2.잡병렬성관리
		잡 하나가 몇개의 파드를 동시에 실행할수 있는지를 잡 병렬성이라고함
		기본값은 1이고 0으로주면 잡을 정지할수 있음
			단 여러이유로 옵션을 설정해도 지정한 값보다 잡이 파드를 적게 실행시킬수 있음
				정상완료되는 잡의 개수를 고정하려면,병렬로 실행되는 실제 파드의 수가 정상완료를 기다리며 남아있는 잡의 개수를 넘지 않아야 함
				즉 자리가 10자린데 현재 완료된게 9개면 병렬을 3으로줘도 1개만 실행됨
			워크큐용 잡에서는 파드 하나가 정상적으로 완료되었을때 새 파드가 실행되지않음,단 현재실행중인잡은 완료될때까지 실행됨
			잡 컨트롤러가 반응하지못할때도있음
			자원이 모자라거나 권한이 부족할수있음
			잡에서  파드들이 실패를 너무 많이했으면 새 파드생성을 제한할수 있음
			파드가 그레이스풀하게 종료되었을수 있음
	3.잡의 종류
		잡은 단일잡,완료된 잡갯수가 있는 병렬잡,워크큐가 있는 병렬잡이 있음
		
		단일잡은
			파드 하나만 실행됨,파드가 정상적으로 실행종료되면(succeeded) 잡 실행을 완료함
			spec.completions와 spec.parallelism필드를 설정하지 않음,둘의 기본값은 1
				spec.completions는 정상적으로 실행종료되어야하는 파드갯수
				spec.parallelism는 병렬적으로 몇개실행할지 정하는거
			즉 둘이1이면 하나만실행해서 하나만끝남
			
		완료갯수가있는 병렬잡은
			spec.completions를 양수로 설정하고
			spec.parallelism는 1로 설정
		즉 완료해야하는건 여러개고 실행은 하나만
		
		워크큐가 있는 병렬잡은
			spec.completions는 설정하지않고
			spec.parallelism는 양수로 설정
			
			만약 completions을 설정하지않고 parallelism를 설정하면,completions은 parallelism를 따라감(동일하게설정됨)
			(단 기본값인 1이니까 하나완료되면 끝임)
			
			파드 각각은 정상적으로 실행종료됐는지를 독립적으로 결정할수 있음
			즉 대기열에 있는 작업들이 동시에 실행할수도있음
			
			파드 하나라도 정상종료되면 새로운파드가 실행되지않음
			
			최소한 파드 1개가 정상종료된후 모든 파드가 실행종료되면 잡이 정상적으로 종료됨
			
			일단 파드1개가 정상종료되면 다른 파드는 더이상 동작하지 않거나 결과를 내지않고 종료함
			

	4.비정상종료된 파드관리
		만약 파드안에 비정상종료된 컨테이너가 있을때를 대비해서,
		컨테이너 재시작정책을 설정하는 .spec.template.spec.restartPolicy를 지정할수 있음
		저걸 never로 두면 비정상종료되었을때 재시작을 막고 잡에서 새 파드를 실행함(노드가 장애나 업그레이드등으로 정지되었을때)
		그런데 spec.parallelism과 spec.completions이 1이고 restartPolicy가 never이면 같은프로그램이 2번 실행될수있음
		만약 둘을 1보다 크게 설정하면 한번에 여러파드가 실행될수 있음
		이런 상황들을 알고있어야함
		
	5.잡 종료와 정리
		잡이 정상적으로 실행 종료되면 파드가 새로 생성되지도 삭제되지도 않고,잡도 남아있음
		파드나 잡이 삭제되지않고 남아있으면 로그에서 에러나 경고를 확인할수있고 잡의상태도 계속 확인할수있음
		특정시간을 지정해 잡실행을 종료하려면,spec.activedeadlineseconds에 시간을 지정하면됨
		그러면 그 시간에 잡 실행을 강제로 끝내고 파드실행을 종료함
		
		그러면 잡의 상태를 확인했을때 종료이유가 reason:deadlineexceeded로 표시됨
		
		잡 삭제는 kubectl delte job 잡이름 으로 사용자가 직접 삭제해야함
		그리고 잡을 삭제하면 관련 파드들도 같이 삭제됨
		
	6.잡패턴
		잡에서 파드를 병렬로 실행했을때 파드 각각이 서로 통신하면서 동작하지 않음
		각 파드는 독립적으로 동작하는걸 전제로 두는데,메일을 발송하거나 파일을 변환하는등은 분산작업이니까 한번에 실행해야해서 제대로 동작하지않음
		
		잡의 사용패턴은
			작업마다 잡을 하나씩 생성해 사용하는거보단 모든 작업을 관리하는 잡 하나를 사용하는게 좋음
			잡의 생성비용은 비쌈
			
			작업개수만큼의 파드를 생성하는거보다 파드가 여러작업을 처리하는게 좋음
			파드를 생성하는거도 비쌈
			
			워크큐를 사용한다면 카프카같은거로 구현해야함,기본설정그대로는 비효율적임



7.크론잡
	크론잡은 잡을 시간기준으로 관리함,즉 지정시간에 잡을 한번만 실행하거나,지정한시간동안 주기적으로잡을 반복실행할수있음
	실행한후에는 잡과 동일하게 동작함
	
	1.크론잡 템플릿
		yaml은
			apiversion:api버전
			kind:CronJob
			metadata:
				name:hello
			spec:
				schedule: '*/1 * * * *' //매 1분마다 실행
				jobTemplate:
					spec:
						template:
							spec:
								container:
									-name:이름
									 image:이미지
									 args:
										여기다가 셸스크립트를 넣을수있음
									 
								restartPolicy:onfailure		

		

		schedule: '*/1 * * * *'는 실행할 시간을 정하는건데 cron과 같은형식이라니까 그걸찾아보자
		어떤 작업을 실행할지는 이미지를 사용하는거고
		커맨드처럼 셀스크립트를 쓸수있고
		리스타트정책도 설정할수있음

		kubectl get cronjobs로 크론잡의 스케줄설정을 확인할수있고,현재 정지중인지 아닌지를 알수있음(가동중이면 false)
		그리고 active로 현재 잡이 몇개실행중인지 알수있고
		크론잡이 실행한 잡은 kubectl get jobs로 알수있음
		
		삭제는 kubectl delete cronjobs 크론잡이름 으로 삭제함
		그러면 크론잡이 생성한 잡들까지 한꺼번에 삭제됨
		
	2.크론잡 설정
		크론잡의 spec.startingdeadlineseconds는 지정된 시간에 크론잡이 실행되지못했을떄,값으로 설정한 시간이 지나면 실행되지않게 함
		이걸 설정안하면 시간이 많이지나도 제약없이 실행함
		
		크론잡의 spec.concurrencyPolicy는 크론잡이 실행하는 잡의 동시성을 관리함
		기본값은 allow로 잡을 여러개 동시에 실행할수 있게 함
		forbid로 설정하면 잡을 동시에 실행하지 않도록 함
		
		만약 이전에 실행했던 잡이 정상종료되지않고 실행중이면,
		만약 forbid면 해당시간에 새로 잡을 실행하지 않고 다음 시간에 실행함,즉 두개이상 실행막음
		만약 replace면 이전실행잡을 지우고 새 실행잡으로 대체함
		
		이건 스케줄밑에 넣으면 됨(크론잡공간에)
		
		spec.successfuljobshistirylimit와 spec.failedfuljobshistirylimit는 잡이 정상종료되었는지 비정상종료되었는지 로그를
		몇개까지 저장할지 설정함,기본값은 3,1이고 0으로하면 저장하지 않음
		그러면 kubectl get pods했을때 status가 compleated로 적혀서 갯수만큼 남아있음(새거로 갱신되면서)






7.서비스
1.서비스의 개념
	쿠버네티스 클러스터 안에 컨트롤러로 파드를 실행했으면,거기 접근할수 있어야하는데 그때 쓰는게 서비스임
	파드는 매번 죽었다 살아나서 재생성될수있고,그때마다 ip주소가 바뀌기때문에 직접접근하긴 머리아파서 쓰는게 서비스임
	서비스를 쓰면 서비스에 통신하면 알아서 파드로 연결해줌
2.서비스타입
	서비스타입은 크게 4가지가 있음
		clusterIP:기본타입이며 클러스터안에서만 사용할수 있음
				  클러스터 안 노드나 파드에서는 클러스터 ip를 사용해서 서비스에 연결된 파드에 접근,외부에선 이용할수없음
		
		NodePort:서비스 하나에 모든 노드의 지정된 포트를 할당함
				 node1:8080,node2:8080처럼 노드에 상관없이 서비스에 지정된 포트번호만 사용하면 파드에 접근할수 있음
				 노드의 포트를 사용하므로 클러스터 외부에서도 접근할수있음
				 특이한점은 파드가 노드1에만 실행되어있고 2에 없어도 2에 접근하면 자동으로 1로 연결해줌
				 외부에서 안으로 접글할때 가장 쉬운방법
		
		LoadBalancer:아마존,구글클라우드등에서 쿠버네티스를 지원하는 로드밸런서장비에서 사용함
					 클라우드에서 제공하는 로드밸런서와 파드를 연결한 후 해당 ip를 이용해 외부에서 파드에 접근할수 있게 해줌
					 kubectl get service로 서비스를 확인하면 external-ip에 로드밸런서 ip를 표시하는데,그걸로 외부에서 접근하면됨
		
		ExternalName:서비스를 .spec.externalName에 설정한값과 연결
					 이건 보통 클러스터내부에서 외부에 접근할때 사용,
					 특정 사이트같은데 도메인 넣어두고 접근해서 html받는식으로 씀
					 외부에 접근할떄 쓰는거라서 셀렉터가 필요없음


3.서비스 템플릿
	서비스의 기본 yaml은
		apiversion:api버전
		kind:Service 
		metadata:
			name:서비스이름1
		spec:
			type:ClusterIP
			clusterIP:10.0.10.10  사용할아이피주소,안적으면 자동으로 할당함
			selector:
				app:앱이름  자동연결할 파드 레이블값
			ports:
				-protocol:TCP
				 port:80
				 targetPort:9376
				 
	스펙의 타입에서 서비스타입을 설정할수있고
	스펙 클러스터ip에서 ip를 직접 설정할수있고
	selector에서 연결할 파드레이블 설정할수있고
	port에서 포트설정할수있음,한번에 여러개를 외부에 제공할떈 포트하위에 필드값을 설정하면 됨
	
	1.clusterIP타입 사용하기
		클러스터ip타입은 위에 있던 거임
		
		클러스터ip타입은 넷샷같은시스템파드에 접근하면 걔가 서비스로 통신하고 파드랑 연결해서 사용자와 파드를 연결시켜주는식임
		클러스터ip는 외부랑 직접연결할수없기떄문
		
		이거도 
			kubectl describe service 서비스이름
		으로 상태볼수있는데
		
		똑같이 endpoint가 연결된 하위파드들임
		
		여기 연결확인할떈 클러스터내부서비스ip로 파드하나 만들어서 거기서 연결해봐야함
		
	2.nodeport타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:NodePort
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
					 nodePort:30080 노드포트값
		이거도 다른건 다똑같고,타입선언하고 포트밑에 노드포트값만 넣어줌
		
		로컬일경우 localhost:노드포트값으로 접근하면 접근할수있고
		외부일경우 컴퓨터ip:노드포트값으로 접근할수있음
		즉 파드를 하나만들고 거기로 접근할필요없이 바로 접근할수있음
		
		내부적으로는 외부포트와 클러스터ip식으로 만들어진서비스를 매핑하는식으로 구현된듯
		
	3.LoadBalancer타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:LoadBalancer
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
		로 기본에서 그냥 타입만 바꾸면됨
		
		얘는 클러스터를 외부 로드밸런서와 연결할때 쓰기떄문에,외부와 연결한뒤 그걸 받아서 서비스를 하게됨
		뭐 쓰기는 노드포트랑 똑같음,단지 세팅할떄 외부와 연결해줘야한다는거고
		
	4.ExternalName
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:ExternalName
				externalName:url값
		이건 그냥 외부url과 연결하는거임
		구글같은데다 연결하면 그냥 구글html을 받아오고 그런식
				

4.헤드리스서비스
	.spec.clusterIP를 none로 설정하면 클러스터ip가 없는 서비스를 만들수있음
	이런걸 헤드리스 서비스라고 함
	이런건 로드밸런싱이 필요없거나 단일서비스 ip가 필요없을때 사용함
	
	헤드리스 서비스에 셀렉터를 설정하면 쿠버네티스api로 확인할수있는 엔드포인트가 만들어지고,
	서비스와 연결된 파드를 직접 가르키는 dns a레코드도 만들어짐(도메인주소와 서버의ip주소를 직접 매핑시키는거,196.123.22.1 이런식의 ip주소)
	셀렉터가 없으면 엔드포인트가 만들어지지않는데,
	셀렉터가 없어도 dns시스템은 externalName에서 사용할 cname레코드(naver.com처럼 문자로된 도메인같은걸 ip주소로 변경시키는거)는 만들어짐
	
	이건 그냥 클러스터ip설정에서 클러스터ip를 None로 주기만 하면 됨
	
5.kube-proxy
	프록시는 쿠버네티스에서 서비스를 만들었을떄,클러스터ip나 노드포트로 접근할수 있게 만들어서 실제 조작을 하는 컴포넌트임
	kube-proxy가 네트워크를 관리하는 방법은 userspace,iptables,ipvs가 있음 
	
	1.userspace
		유저스페이스는 클라이언트에서 서비스의 클러스터ip를 통해 요청을하면 iptables을 거쳐서 프록시가 요청을 받고,
		서비스의 클러스터ip는 연결되어야할 파드로 연결해줌,즉 포워드프록시임
		
	2.iptables
		iptables는 클라이언트가 iptables로 직접 접근하고,프록시는 iptables을 관리하는역할만 함,
		직접 클라이언트에서 트래픽을 받진 않고 클라이언트에서 오는 모든 요청은 iptables을 거쳐서 파드로 직접 전달됨
		그래서 중간에 프록시를 거치지않기때문에 요청처리성능이 좋음
		그리고 특징으론 유저스페이스는 프록시가 연결을 책임지기때문에 파드하나에 연결실패하면 다른파드에 계속 연결시켜주려고 재시도하는데,
		이건 책임이 클라이언트에 있으니까 그냥 실패하면 실패임
		컨테이너에서 readinessprobe가 설정되어있고 헬스체크가 성공해야 연결이 이루어짐
		
	3.IPVS
		ipvs는 리눅스에 있는 l4로드밸런싱기술임
		리눅스 커널 안 네트워크 프레임워크인 넷필터에 포함되어있음
		즉 ipvs커널 모듈이 노드에 설치되어있어야함
		
		구조자체는 iptables과 같은데,이건 클러스터ip가 가상서버로 동작하고,백엔드파드들이 실제서버임
		얘는 커널공간에서 동작하고 데이터구조가 해시테이블이라서 iptables보다 빠르고 좋은성능을 내고
		로드밸런싱 알고리즘이 많아서 이걸 이용할수있음
			rr:프로세스사이에 우선순위를 두지않고 순서와 시간단위로 cpu할당
			lc:접속 개수가 가장 적은 서버를 선택
			dh:목적지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sh:출발지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sed:응답속도가 가장 빠른 서버를 선택함 
			nq:sed+활성접속개수가 0개인 서버를 먼저 선택함
		
		가 있음
		



8.인그레스
	인그레스는 클러스터 외부에서 안으로 접근하는 요청을 어떻게 처리할지 정의해둔 규칙모음임
	클러스터 외부에서 접근할 url을 사용할수있게 하고,트래픽로드밸런싱,인증서처리,도메인기반 가상호스팅도 지원함
	인그레스 자체는 이런규칙들을 정의해둔 자원이고,실제동작시키는건 인그레스 컨트롤러
	
	클라우드 서비스를 사용하면 별다른 설정없이 자체로드밸런서와 연동해서 인그레스를 사용할수 있음
	클라우드 서비스를 사용하지않고 직접 쿠버네티스 클러스터를 구축해서 사용한다면 인그레스컨트롤러를 인그레스와 연동해야함
	이때 가장 많이 사용하는 도구는 ingress-ngibx임
	얘는 인그레스에 설정한 내용을 nginx환경설정으로 변환해서 ngibx에 적용함
	
	이거말고도 다양한 컨트롤러등이 있음
	
1.인그레스 템플릿
	yaml은
		apiversion:api버전
		kind:ingress
		metadata:
			name:이름
			annotations:
				nginx.ingress.kubernetes.io/rewrite-target: /  엔진엑스컨트롤러 설정으로,/(루트)로 리다이렉트하라는뜻
		spec:
			rules:
				-host:foo.bar.com  url 아무거나
				 http:
					paths:
						-path:/foos1
						 backend:
							serviceName:서비스1
							servicePort:서비스포트1
						-path:/bars2
						 backend:
							serviceName:서비스2
							servicePort:서비스포트1	
				-host:bar.foo.com
				 http:
					paths:
						-backend:
							serviceName:서비스2
							servicePort:서비스포트1


	이런식으로 쓰는데
	인그레스 설정은 어노테이션 하위필드를 쓰는데,인그레스 컨트롤러마다 상세항목은 다 다름
	그리고 spec.rules의 하위필드에서 어떤 규칙을 사용할지 지정할수있는데
	host필드값은 url값임(foo.bar.com)
	저거로 요청이 들어오면 여기 하위로 들어온다는소리
	
	거기 뒤에 뭐가 붙었냐에 따라(path) 백엔드로 선택할 포트를 나눠지게 설정됨(foo.bar.com/foos1)
	거기로 나눠진걸 서비스네임에서 지정된 서비스의 포트로 던지라는소리
	
	확인할떈 똑같이 
		kubectl describe ingress 인그레스이름
	으로 확인할수있음
	
	즉 인그레스는 규칙에따라 서비스를 배분해주는 역할임
	
2.ingress-nginx 컨트롤러
	인그레스는 설정일뿐이고 설정내용대로 동작하는 주체는 인그레스컨트롤러임
	엔진엑스 인그레스컨트롤러를 설치하고
		kubectl apply -k.
	로 인그레스컨트롤러와 노드포트타임의 인그레스엔진엑스 컨트롤러에 접근하는 서비스까지 만들어짐
	
	확인해보면 디플로이먼트와 서비스가 하나씩 생성됨
	
	그리고 로컬로 테스트할때는 dns가 없으니까 dns저장파일(시스템32밑에 있는거)를 수동으로 127.0.0.1 사용할url을 매핑해줘야함
	그리고 인그레스에서 지정한 이름으로 서비스를 만들고 url뒤에 값(foos1)을 붙여서 던져보면 그쪽서비스로 연결됨

	또 인그레스 컨트롤러의 네트워크 옵션을 호스트모드로 설정하면,별도의 nodeport타입서비스 없이도 인그레스 컨트롤러에 접근할수 있고,
	다시 컨트롤러에서 파드로 직접 접근할수 있으므로 중간의 서비스들을 생략해서 좀 더 좋은 성능을 낼수 있음
		
3.인그레스 SSL인증서 설정하기
	인그레스를 이용하면 요청으로 들어오는 트래픽에 다양한 설정을 할수 있음
	인그레스로 ssl인증서를 설정하면 파드 각각에 ssl설정을 할필요가 없고,기한이 만료되어도 인그레스에서만 인증서를 업데이트 하면 됨
	
	ssl인증서의 인증구조는
		ca에서 서명된 인증서를 발급
		인증서를 검증한후 ssl로 통신
	인데 인증서 발급을 요청하면 ca측에서 관리하는 키와 인증서로 서명한 인증서를 발급해줌,
	그걸 서버에 설정하면 웹브라우저에서 통신할때 서버에 있는 인증서가 유효한 인증서인지 확인하고 ssl통신을 함
	
	설정하는 밥법은 ca에서 crt파일과 key파일을 받고
		kubectl create secret tls 시크릿이름 --key 키이름.key --cert crt이름.crt
	명령으로 시크릿을 생성하고
		apiversion:api버전
		kind:ingress
		metadata:
			name:이름
		spec:
			tls:
				-hosts:
					-url이름
				secretName:시크릿이름
			rules:
				-host:url이름
				 http:
					paths:
						-path:/
						 backend:
							serviceName:서비스이름
							servicePort:서비스포트
							
	이렇게 스펙의 하위필드로 tls필드에서 url과 연결할 시크릿을 넣어주면됨
				
	이렇게해서 인증서가 들어갔는지 확인하려면
		curl -vl https://url:포트/
	를 날려보면 리턴됨
	

4.무중단배포를 할때 주의할점
	인그레스를 이용해 외부에서 컨테이너를 접근할때,새로운 버전의 컨테이너를 배포할때와 겹치면
	정상적인 상황에서는 새로운버전을 만들고 헬스체크를 한 다음 서비스가 그쪽으로 연결을 옮기고 전버전을 삭제함
	그러면 무중단배포임
	
	1.maxsurge와 maxunavailable설정
		파드관리를 롤링업데이트로 설정했을때,maxsurge와 maxunavailable 필드 설정이 필요함
		디플로이먼트를 이용해서 컨테이너를 배포할때,
		maxsurge는 디플로이먼트에 설정된 기본 파드 개수에 여분의 파드를 몇개 더 추가할수 있는지를 설정하고
		maxunavailable은 디플로이먼트를 업데이트하는동안 몇개의 파드를 이용할수 없어도 되는지를 설정함
		
		이 2개를 운영중인 서비스에 맞게 적절히 조정해야 일정한수의 파드를 이용할수 있고,그래야 트래픽유실이 없음
		두필드를 한꺼번에 0으로 설정하면안됨,파드가 없을수도 있기때문

	2.파드가 readinessprobe를 지원하는지 확인
		파드가 readinessprobe를 가지고있으면 그거받아서 파드가 초기화완료됐는지를 확인할수있는데(책임이 이쪽에없음)
		파드가 readinessprobe가 없으면 준비되지 않은 컨테이너에 요청이 가서 응답을 제대로 하지 못할수 있음
		
		그래서 서비스가 준비된 상태인지 진단하는 readinessprobe를 지원하는게 좋은데,없으면
		.spec.minReadySeconds필드로 어느정도 비슷한 효과를 낼수있음
		이건 파드가 준비상태일때까지의 최소대기시간으로, 파드가 실행된후 저기 적힌시간동안은 트래픽을 받지 않음
		그러니 좀 초기화시간에 맞춰서 좀 길게잡아주는게 좋음
		물론 readinessprobe있으면 그거쓰면되고
	
	3.쿠버네티스와 컨테이너 안에 그레이스풀 종료 설정
		노드 안 컨테이너를 관리하는 컴포넌트인 kubelet는 새 파드가 실행되고 이전 파드를 종료할떄 파드에 sigterm신호를 먼저 보냄
		무중단 배포를 하려면 컨테이너에서 sigterm신호를 받았을때 기존에 받은 요청만 처리를 완료하고 새 요청을 받지 않는 그레이스풀 종료가
		설정되어있어야 함
		
		그렇지않으면 종료된 파드로 요청 전달하는상황이 생김,
		인그레스 컨트롤러가 업데이트 되기전까진 파드가 살아있는거로 나와서 트래픽을 전버전파드에 보내는거임
		
		만약 파드에 그레이스풀 종료를 설정하지 못하는상황이면 프리스톱훅을 이용할수 있음
		
		쿠버네티스에서는 파드 생명주기 중 훅을 설정할수 있음
		파드가 실행된 직후 실행하는 포스트스타트훅과 파드가 종료되기 직전 실행되는 프리스톱 훅이 있음
		프리스톱훅은 파드에 sigterm신호를 보내기 전 실행되므로 컨테이너와 별개로 그레이스풀 종료와 같은 효과를 낼수 있음
		또한 프리스톱 훅의 실행이 완료되기 전까지는 컨테이너에 sigterm신호를 보내지 않으므로,컨테이너 설정과 별개로 종료전 대기시간도 설정할수 있음
		이렇게 대기시간을 설정해도 terminationGracePeriodSeconds필드에 설정할 대기시간을 초과하면 프로세스 종료되니까 이건 주의
		
		


9.레이블
	레이블은 키밸류쌍으로 구성하며,사용자가 클러스터 안에 오브젝트를 만들때 메타데이터로 설정할수 있음
	레이블이 생성된 다음에도 언제든지 수정할수 있음
	
	레이블의 키는 쿠버네티스안에서 컨트롤러들이 파드를 관리할 때 자신이 관리해야 할 파드를 구분하는 역할
	쿠버네티스는 레이블만으로 관리대상을 구분하므로,컨트롤러가 만든 파드라도 레이블을 바꾸면 인식할수없음
	그래서 유연성이 있는것
	
	이걸 사용한예로 운영중인 파드 하나의 레이블을 바꿔서 파드상태를 확인할수있고,그거로 디버깅도 할수있음
	
	이거 외에도 노드에도 레이블을 붙일수 있으니까,
	클러스터 안 노드들을 레이블로 구분한다음 특정 레이블(gpu같은)이 있는데만 자원을 할당해 실행하는거도 가능 
	
	보통 사용자가 이름붙일땐 접두어없이하는게 관습임
	
	레이블을 선택할때는 레이블셀렉터를 사용하는데,등호기반과(a=b)집합기반(a in b)이 있음
	그리고 and연산할때는 ,로 연결하면 됨
	등호기반은 그냥 그대로고
	집합기반에서 여러개값을 설정할떄는
		a in (b,c,d)
	이런식으로하면 되고
		gpu
		!gpu
	이렇게하면 값은확인하지않고 gpu라는 키가 있는 레이블을 전부 선택함
	
1.에너테이션 
	에너테이션은 레이블과 마찬가지로 키밸류쌍으로 구성하며 레이블처럼 사용자가 설정할수 있음
	레이블이 사용자가 설정한 특정 레이블의 오브젝트들을 선택한다면,에너테이션은 쿠버네티스 시스템이 필요한 정보들을 담고,걔들이 자원관리하는데 이용함
	그래서 에너테이션의 키는 쿠버네티스 시스템이 인식할수 있는 값을 사용함
	예를들어 앱을 배포할때 변경사유를 적는 CHANGE-CAUSE처럼,키워드를 써야함 사용자정의변수가 아니라
	

2.레이블을 이용한 카나리배포
	배포에는 롤링업데이트,블루/그린(기존파드와 같은개수의 신규파드를 실행하고,헬스체크가 끝나면 트래픽을 한꺼번에 신규파드로옮김),카나리등이 있음
	여기서 카나리는 기존파드에서 신규파드 한두개정도 섞어넣어서 정상적으로 작동하는지 체크하는것
	
	카나리를 하는방법은 그냥 레이블을 2개쓰는거임
	기존에 배포할때 쓰는레이블과,카나리구분용 레이블을 둘다 적어서 기존배포레이블에 카나리가 섞이게 만든다음,문제가생기면 카나리레이블로 걷어내는거
	
	배포자체는 디플로이먼트 2개를 쓰는거임 원래쓰던거1개 카나리1개,거기서 서비스는 레이블만 보고 트래픽배분하니까 기존배포레이블을 서비스에넣으면
	카나리랑 기존거랑 구분없이 트래픽을 줌
	
	여기서 문제가 생기면 카나리 디플로이먼트를 삭제하거나 카나리디플로이먼트의 레플리카를 0으로 설정하는거처럼 해서 서비스에서 제외시키면됨
	




10.컨피그맵
	컨피그맵은 설정값들을(db패스워드아이디같은거를) 저장해뒀다가 테스트랑 사용서비스를 다르게쓸때 그때 편하게 바꿔치기할려고 사용하는거임
	그래서 컨테이너는 그대로 쓰고 컨피그맵만 갈아치우는식으로 쓰는거
	
1.컨피그맵 템플릿
	컴피그맵 yaml은
		apiversion:api버전
		kind:ConfigMap
		metadata:
			name:컨피그맵이름1
			namespace:네임스페이스이름1
		data:
			db_url:localhost 키밸류쌍,뭘적어도됨
			db_user:myuser
			db_pass:mypass
			debug_info:debug
	
	이런식으로 데이터밑에 원하는 키밸류쌍을 적어서 그걸 가져다 쓰는거
	
	저걸 가져다쓰려면
	디플로이먼트에서
		앱버전
		카인드
		메타데이터
		스펙
		템플릿
			메타데이터
			스펙
				컨테이너
				env:
					name:설정이름
					valuefrom:
						configMapKeyRef:
							name:가져올컨피그맵이름
							key:컨피그맵의 키값 
			
		서비스생성파트는 생략
		

		
	이렇게 값을 한두개 가져올수도 있고,
	전체를 가져오려면
		env를
		envfrom:
			-configMapKeyRef:
				name:가져올컨피그맵이름
	으로 전체를 가져올수있음
	
	연결은 이렇게하고,실제로 쓸때는 상용과 테스트용 2개만든후에 저기 가져올컨피그맵이름만 바꾸고 apply하면됨
	
2.컨피그맵을 볼륨에 불러와서 사용하기
	컨피그맵을 컨테이너의 환경변수로 설정하는것이 아닌 다른 방식으로 사용할수도 있음
	컨테이너의 볼륨형식으로 컨피그맵을 설정해서 파일로 컨테이너에 제공할수 있음 
	
	사용법은
		앱버전
		카인드
		메타데이터
		스펙
		템플릿
			메타데이터
			스펙
				컨테이너
					이름:
					이미지:
					포트:
					volumeMounts:
						-name:볼륨이름
						 mountPath:/etc/config
				volumes:
					-name:볼륨이름
					 configMap:
						이름:컨피그맵이름
			
		서비스생성파트는 생략
		
		
	볼륨마운트로 볼륨을 선언하고 이름을붙이고 마운트위치를 정하고
	볼륨으로 볼륨이름이 가질 컨피그맵을 선언하고 이름을 연결해주면
	컨피그맵의 필드값 하나하나마다 파일이 하나씩 생성됨 마운트위치에
	
	


11.시크릿
1.시크릿 만들기
	시크릿은 비밀번호나 개인키같은 민감한정보를 저장하는 용도로 사용함,
	이런건 컨테이너안에 저장하지 않고 별도로 보관했다가 실제 파드를 실행할때의 템플릿으로 컨테이너에 제공함
	
	시크릿은 내장시크릿과 사용자정의 시크릿이 있음
	내장시크릿은 클러스터안에서 쿠버네티스 api에 접근할때 사용함
	클러스터 안에서 사용하는 serviceAccount라는 계정을 생성하면 자동으로 관련 시크릿을 만듬
	이 시크릿으로 서비스 어카운트가 사용권한을 갖는 api에 접근할수 있음
	
	사용자 정의 시크릿은 사용자가 만든 시크릿임
	
	서비스어카운트 생성방법은
			apiversion:api버전
			kind:ServiceAccount
			metadata:
				name:이름
				namespace:네임스페이스이름
	
	시크릿을 만드는 방법은 2개가있는데
	
		kubectl create secret generic 시크릿이름 --from-file=./파일경로 --from-file=./파일경로 --from-file=./파일경로
	저렇게 만들수도있고,
	템플릿으로 만들수도 있음
		apiversion:api버전
		kind:Secret
		metadata:
			name:시크릿이름
		type:Opaque
		data:
			키1:값1
			키2:값2
	
	여기서 type는 4개가있는데
		Opaque는 기본값으로 키값형식으로 임의의 데이터를 설정할수 있고
		kibernetes.io/service-account-token은 쿠버네티스 인증 토큰을 저장하고
		kubernetes.io/dockerconfigjson은 도커 저장소 인증정보를 저장하고
		kubernetes.io/tls는 tls인증서를 저장함
		
	그리고 시크릿을 확인하는건 똑같이
		kubectl get secret 이름 -o yaml
	로확인하면됨
	
	그리고 필드값은 base64인코딩방식으로 인코딩되어서 나옴


2.시크릿 사용하기
	시크릿은 파드의 환경변수나 볼륨을 이용한 파일형식으로 사용할수 있음
	1.환경변수로 시크릿사용
		디플로이먼트에선
			.spec.template.spec.container[].env 단에
			
			env:
				-name:붙일환경변수이름
				 valuefrom:
					secretKeyRef:
						name:연결할시크릿이름
						key:원하는키값
	
		이렇게 env하단에 저런식으로 연결하면됨
		
		그리고 사용할 시크릿은 미리 만들어져 있어야 함,
		만약 오타등으로 없는시크릿을 참조하거나 시크릿이 없으면 에러가 발생해 파드가 실행되지못함
		
		kube-apiserver에선 문제없다고 생각해서 스케줄링하는데,
		kubelet에서 불러올때 에러가 발생함,
		그리고 kubelet는 계속 불러오려고 재시도하니까 조심
		
		
	2.볼륨형식으로 파드에 시크릿 제공
		볼륨형식으로 파드에 시크릿넣을땐 그 컨피그맵때 했던거랑 똑같이 volumeMounts와 volumes로 하면됨
		
		디플로이먼트에서
			.spec.template.spec.container[].volumeMounts[]와 .spec.template.spec.container[].volumes[]
			
			volumeMounts:
				-name:볼륨이름1
				 mountPath:'경로'
				 readonly:true
			volumes:
				-name:볼륨이름1
				 secret:
					secretName:연결할시크릿이름
					
		이러면 볼륨마운트로 볼륨을 생성하고 경로를 지정하고 볼륨에서 볼륨이름에 시크릿을 연결해서 파일로 저장함
		
		제대로됐나 확인하려면 
			http://localhost:30900/volume-config?path=/etc/volume-secret/username
		이런식으로 경로에 접근해서 파일을 켜서 확인할수 있음
		
		그리고 kubectl get pods로 시크릿 설정이 된 파드이름을 확인하고,
		kubectl exec -it 파드이름 sh 로 컨테이너에 직접 접근해서 파일위치에 가서 확인할수도있음
		
	
	3.프라이빗 컨테이너 이미지를 가져올때 시크릿 사용
		보통 컨테이너 이미지를 가져올땐 공개된 이미지를 사용하지만,프라이빗이미지를 사용할땐 인증정보가 필요함
		로컬서버를 사용하면 인증정보를 저장해서 사용할수 있지만,그러면 보안상의 위험이 있어서 그렇게 설정하지 않음
		인증정보를 시크릿에 설정해 저장한 후 사용함
		
		쿠버네티스에는 kubectl create secret의 하위명령으로 도커 컨테이너 이미지저장소용 시크릿을 만드는 docker-registry가 있음
			kubectl create secret docker-registry dockersecret 
			--docker-username:이름 --docker-password:패스워드 --docker-email=이메일 --docker-server=url주소
			
		이런식으로 시크릿을 만들수있음 
		저기서 이름 패스워드 url을 넣어서 거기로 연결하면됨
		
		저걸 kubectl get secrets dockersecret -o yaml로 확인하면
		data아래에 dockerconfigjson이 있는데 그게 도커인증정보값임
		그리고 type는 kubernetes.io/dockerconfigjson으로 자동으로 되어있고
		
		이걸 디플로이먼트에서 사용할땐
			컨테이너 밑에 이미지를 불러오고
			containers:
				...
				imagePullSecrets:
					-name:dockersecret
		이렇게 시크릿을 연결해주면됨
		
		만약 시크릿이 틀리거나 설정을 안하면 errimagepull에러가 나게 됨
		
		
	4.시크릿으로 tls 인증서를 저장해 사용하기
		https 인증서를 저장하는 용도로 시크릿을 사용할수 있음
		인증서파일(tls.key tls.crt)를 받고
			kubectl create secret tls 시크릿이름 --key tls.key --cert tls.crt
			
		로 시크릿을 생성하고
			kubectl get secrets 시크릿이름 -o yaml
		으로 확인하면
		
		data 밑에 tls.crt tls.key가 있고,type는 kubernetes.io/tls로 설정되어있음
		이걸 인그레스와 연결해서 사용할수 있음
						

3.시크릿 데이터 용량 제한
	시크릿데이터는 etcd에 암호화 하지 않은 텍스트로 저장됨
	이때 시크릿 데이터의 용량이 너무 크면 쿠버네티스의 apiserver나 kubelet의 메모리 용량을 많이 차지함
	그래서 개별시크릿 데이터의 최대용량은 1메가고,작은거도 많이있으면 같은문제생기니까 총용량제한도 있음
	
	그리고 만약 누가 etcd에 직접 접근하면 시크릿데이터의 내용을 확인할수 있음
	etcd에는 이외에도 중요한 데이터가 많이 있으므로 중요한 서비스에 쿠버네티스를 사용중이라면 etcd의 접근을 제한해야함
	
	기본적으로 etcd를 실행할때 etcd 관련 명령을 사용하는 api통신에 tls인증이 적용되어있으므로 인증서가 있는 사용자만 etcd에 접근해 관련 명령을
	사용할수 있음
	
	그외에 etcd가 실행중인 마스터 자체에 접속해서 데이터 접근하는걸 막기위해 마스터에 접근을 계정단위로 막거나,
	etcd에 저장되는 시크릿데이터를 암호화할수 있음,이때는 별도로 암호화옵션을 지정해야함





12.파드 스케줄링
	파드 스케줄링은 파드를 어떤 노드에 실행할것인지에 관해 옵션을 줘서 원하는대로 노드에 배치하는것
	특정 레이블의 파드를 노드 하나에 모아두거나 특정ip대역의 노드에서만 실행할수도있고,
	반대로도 할수있음
	같은 기능이 있는 파드들이 한군데 모여있지 않게 골고루 분산해서 실행할수도 있음
	관리가 필요한 노드들을 다른 노드로 옮길수도 있음
1.노드셀렉터
	노드셀럭터는 파드의 .spec에 설정할수 있음
	이건 단어그대로 노드를 선택하는 기능임
	
	파드가 클러스터 안 어떤 노드에서 실행될지를 키 값 노드로 설정할수있음
	
	노드의 레이블을 보는건
		kubectl get nodes --show-labels
	로 볼수있고
		kubectl label nodes 이름 키=밸류
	로 레이블을 추가할수도 있음
	
	그런다음 파드의 .spec.nodeSelector 밑에 키:밸류를 명시하는것으로 거기에 속한데서만 실행할수 있게 설정할수있음
	

2.어피니티와 안티어피니티
	어피니티는 파드들을 한데 묶어서 같은 노드에서 실행하게 하는거고
	안티어피니티는 파드들을 다른 노드에 나누어서 실행하도록 설정하는것
	
	1.노드 어피니티
		노드 어피니티는 노드셀렉터와 비슷하게 노드의 레이블 기반으로 파드를 스케줄링함
		
		노드 어피니티와 노드 셀렉터를 동시에 설정할수도 있으며,이떄는 둘다 만족하는 노드에 파드를 스케줄링함
		
		노드 어피니티에는 두가지 필드가 있는데
			requiredDuringSchedulingIgnoredDuringExecution 은 스케줄링하는데 꼭 필요한 조건
			preferredDuringSchedulingIgnoredDuringExecution 은 스케줄링하는동안 만족하면 좋은 조건
			
		두 필드는 실행중에 조건이 바뀌어도 무시함,즉 스케줄링 할때만 영향을 끼치고,중간에 레이블이 바뀌어도 이미 실행중인 파드는 그대로 실행됨
		실행중에 노드조건변경을 감지하는 필드도 생긴다고함(이미생겼는진모르겠음)
		
		이거도 파드에 .spec밑에
			spec:
				affinity:
					nodeAffinity:
						requiredDuringSchedulingIgnoredDuringExecution:
							nodeSelectorTerms:
								-matchExpressions:
									-key:키1
									 operator:In (어케할건지 조건 설정)
									 values:
										-linux
										-window
									-key:키2
									 operator:Exists(어케할건지 조건설정)
						preferredDuringSchedulingIgnoredDuringExecution:
							-weight:10
							 preference:
								matchExpressions:
									-key:키
									 operator:In
									 values:
										-worker-node01


		이런식으로 씀
		키는 노드의 레이블키중 하나를 성정하고,오퍼레이터는 키가 만족할 조건임
		오퍼레이터는
			In:밸류필드에 설정한값중 레이블에 있는 값과 일치하는게 하나라도 있는지 확인
			notIn:in과 반대로 모두 맞지않는지 확인
			Exists:밸류없이 키필드에 있는값이 노드레이블에 있는지 확인
			doesnotExist:exists와 반대로 노드의 레이블에 key값이 없는지만 확인
			gt:밸류필드에 설정된값보다 큰 숫자형 데이터인지만 확인,이때는 값이 하나만있어야함
			lt:밸류필드에 설정된값보다 작은 숫자형 데이터인지만 확인,이때는 값이 하나만있어야함

		가 있음
		
		그리고 preferredDuringSchedulingIgnoredDuringExecution은
		weight가 있고, nodeSelectorTerms대신 preference를 쓴다는거 대신에는 똑같음
		preference는 가능하면 그 조건에 맞는걸 선호한다는 뜻
		조건을 만족하는거랑 아닌거있으면 만족하는노드를 선택하고,전부 아니면 그냥 아무데나 들어감
		weight는 1부터 100까지 값을 선택할수 있는데
		각 설정마다 웨이스트를 줄수있고,만족하는걸 전부 더해 가장 큰 노드를 선택함
		
	2.파드의 어피니티와 안티 어피니티
		파드 사이의 어피니티와 안티 어피니티는,
		디플로이먼트나 스테이트풀셋으로 파드를 배포했을때 개별 파드 사이의 관계를 정의하는 용도임
		
		컨테이너로 서비스를 운영하다 보면 서비스 a의 파드와 서비스 b의 파드들끼리 자주 통신할때가 있으면,서로 같은노드에 속하게 만들수있음
		대표적으로 db나 캐시와 통신하는 앱컨테이너를 같은노드에 두는것
		
		쿠버네티스 클러스터를 구성할때 리눅스의 bpf와 xd를 이용하는 실리엄 네트워크 플러그인을 쓰면 같은노드의 컨테이너끼리의 통신성능이 더 올라간대
		
		안티어피니티는 cpu나 네트워크를 많이 사용하는 컨테이너들을 여러 노드로 파드를 분산하는것,
		이걸 설정하지않으면 디플로이먼트로 배포한 파드가 노드 하나에만 실행되어서 자원을 많이 소모할수 있음
		그리고 서버를 늘렸을때 이미 시스템사용률이 높은 노드에 다시 가서 오히려 심해지거나 할수있기때문에,
		안티 어피니티로 자원을 많이쓰는 파드들은 분산시키는것
		
		안티어피니티의 설정은
			.spec.template.spec.affinity에 하면됨
			
				spec:
				...
				template:
					...
					spec:
						...
						affinity:
							podAntiAffinity:
								requiredDuringSchedulingIgnoredDuringExecution:
									-labelSelector:
										matchExpressions:
											-key:키1
											 operator:In
											 values:
												-밸류1
									topologyKey:'kubernetes.io/hostname'
									
							podAffinity:
								requiredDuringSchedulingIgnoredDuringExecution:
									-labelSelector:
										matchExpressions:
											-key:키2
											 operator:In
											 values:
												-밸류2
									topologyKey:'kubernetes.io/hostname'
									
		파드의 어피니티와 안티어피니티는 노드어피니티와 다르게,
		.spec.template.spec.affinity의 하위필드인 .podAffinity와 .podAntiAffinity로 설정함
		그이후에는 노드어피니티와 거의 같음
		
		
		topologyKey는 노드의 레이블을 이용해 파드의 어피니티와 안티 어피니티를 설정할수 있는 또 하나의 기준임
		쿠버네티스는 파드를 스케줄링할때 먼저 파드의 레이블기준으로 대상노드를 찾고,
		그리고 topologyKey필드를 확인해서 해당 노드가 원하는노드인지 확인함
		hostname을 기준으로 어피니티 설정을 만족하면 같은노드에 파드를 실행하고,안티어피니티 설정을 만족하면 다른노드에 파드를 실행함
		즉,노드뿐 아니라 idc의 같은렉인지 다른idc인지까지 구분할때를 위해 있는거임
		전체적인 조건을 하나 더 사용할수 있는거
		
		topologyKey는 성능이나 보안상의 이유로 제약사항이 있는데
			podAffinity의 하위필드와 podAffinity.requiredDuringSchedulingIgnoredDuringExecution의 하위필드엔
			topologyKey가 필수로 명시해야함
			
			podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution하위와 어드미션컨트롤러의 하위에 설정하는
			topologyKey는 kubernetes.io/hostname만 설정할수 있음
			다른걸쓰고싶으면 어드미션 컨트롤러의 필드설정을 바꿔야함
			
			podAffinity.preferredDuringSchedulingIgnoredDuringExecution의 하위필드는,
			topologyKey필드를 설정하지 않아도됨 이경우에는 전체 토폴로지를 대상으로 안티어피니티설정을 만족하는지 확인함
			전체 토폴로지는 kubernetes.io/hostname,failure-domain.kubernetes.io/zone,
			failure-domain.beta.kubernetes.io/region
			을 뜻함
			
		저 3가지를 제외하면 topologyKey는 필드에서 레이블에 사용하는 키라면 무엇이든 설정할수 있음
			

		이렇게 설정하면 어피니티끼리는 뭉치고 안티어피니티끼리는 흩어짐



3.테인트와 톨러테이션
	테인트는 특정노드에 테인트를 설정할수 있음
	그러면 테인트를 설정한 노드에는 파드들을 스케줄링 하지 않음
	
	그 테인트를 설정한 노드에 파드들을 스케줄링 하려면 톨러레이션을 설정해야 함
	그럼 테인트는 톨러레이션에서 설정한 특정 파드들만 실행하고 다른 파드는 실행하지 못하게 할수잇음
	
	이건 노드를 특정 역할만 하도록 만들때 사용함
	
	예를 들어 데이터베이스용 파드를 실행한 후,노드 전체의 cpu나 ram을 독점해서 사용할수 있게 설정하고
	gpu가 있는 노드에는 gpu를 사용하는 파드들만 실행되게 설정하는것
	
	테인트는 키,값,효과 3개로 구성됨
		kubectl taint nodes 노드이름 키=값:효과
	로 작성함
	
	효과는
		NoSchedule:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,기존에 실행되던 파드에는 적용되지않음
		PreferNoSchedule:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,
						하지만 클러스터 안 자원이 부족하면 테인트를 설정한 노드에서도 파드를 스케줄링할수 있음
		NoExecute:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,기존파드도 톨러레이션이없으면 종료시킴
		
	
	확인하는건
		kubectl describe nodes 노드이름
	으로 확인할수 있음
	
	톨러레이션은
		spec.template.spec.tolerations에 적으면됨
			tolerations:
				-keys:'키1'
				 operator:'equal' 오퍼레이터
				 value:'밸류1'
				 effect:'noSchedule'효과
				 
	
	그리고 저걸 넣은 디플로이먼트를 적용하면 테인트가 있는데에 스케줄링함
	
	테인트를 삭제하려면
		kubectl taint nodes 노드이름 키:효과-
	로 삭제할수있음
	
	tolerations의 하위 필드값으로 원하는 테인트의 설정값을 넣고,
	operator는 equal과 exists가 있음
	이퀄은 키 밸류 이펙트가 다 같은지 확인하고 exist는 앞 3개중 선별해서 사용할때 사용함
	즉 exist를 설정하면 value필드를 사용할수 없음
	
	그리고 operator값으로 exists만 설정하고 아무것도 안넣으면,
	테인트가 있든없든 아무데나 가서 스케줄링하고,
	키값만 설정하면 키값만 볼수있음(이펙트무시하고)
		tolerations:
			operator:'exists'
	와
		tolerations:
			-key:'키값1'
			 operator:'exists'
	이런식
	
			
4.클러스터를 관리하는 커든과 드레인
	특정 노드에 있는 파드들을 모두 다른 노드로 옮기거나 특정 노드에 파드들을 스케줄링 하지 않도록 제한할때 사용함
	앞에서 본 테인트도 같은용도의 명령어임
	
	1.커든설정
		커든을 설정하려면
			kubectl cordon 노드이름
		을 하면됨
		커든을 설정하면 그 노드에 더이상 파드를 스케줄링 하지 않음
		
		커든을 해제하려면
			kubectl uncordon 노드이름
		을 하면 됨
	2.드레인설정
		드레인은 노드 관리등의 이유로 지정된 노드에 있는 파드들을 다른 파드로 이동시키는 명령임
		먼저 새로운 파드를 노드에 스케줄링 하지 못하게 막고(커든)기존 해당노드에서 실행중이던 파드들을 삭제함
		
		노드에 데몬셋으로 실행한 파드들이 있으면 드레인을 쓸수없는데(삭제해도 즉시 재실행시켜서)
		이걸 무시하고 드레인하려면 --ignore-daemonsets=true옵션을 주면됨
		
		그리고 컨트롤러를 이용하지 않고 실행한 파드들도 드레인설정을 적용할수 없음
		컨트롤러를 이용하지 않은애들은 다시 복구할수 없기때문
		이걸 무시하고 강제삭제하려면 --force옵션을 붙이면됨
		
		그리고 kubectl이 직접 실행한 스태틱파드들은 apiserver로 실행되지 않아서 삭제되지 않음
		여기에 드레인을 적용하면 그레이스풀하게 파드들을 종료함
		파드들이 정상적으로 잘 종료되게 설정되었으면 종료명령을 받은 즉시 삭제하는건 아니고 기존작업을 정리한 후 종료됨
		
		사용법은 
			kubectl drain 노드이름 (옵션)
		으로 데몬셋있으면 뒤에 옵션붙이고 그냥실행한 파드있어도 옵션붙이면됨
		
		드레인을 해제하려면 커든해제하는거랑똑같이
			kubectl uncordon 노드이름
		하면됨,커든걸고 다 삭제하는게 내부구조이기때문
		
		
		
	

13.인증과 권한 관리
1.인증
	apiserver는 테스트목적으로 로컬호스트:8080에 http서버를 실행함
	그리고 일반적인 https인증은 접근하는 클라이언트에 인증서를 요구하지 않음
	
	하지만 사용자가 쿠버네티스의 api에 접근하려면 인증을 거쳐야함
	외부에서 쿠버네티스 api에 접근할수 있는 기본포트는 6443이고,tls인증이 적용되어 있음
	6443포트에 접근해 통신하려면 apiserver에 있는 인증서와 클라이언트의 인증서 사이의 검증을 통과해야함,
	인증되지않은 클라이언트가 api서버에 접근하는걸 막는것
	
	쿠버네티스는 일반적인 사용자계정과 서비스계정으로 인증을 요청함
	일반적인 사용자계정은 구글계정이나 그런 외부의 인증시스템을 연결해 사용하는거고,서비스계정은 쿠버네티스가 직접 관리하는 사용자계정임,
	시크릿을 할당해서 비밀번호역할을 하는것
	
	1.kubectl의 config파일에 있는 tsl인증정보구조 확인하기
		쿠버네티스는 apiserver와 통신할때의 기본 인증방법으로 tls(트랜스포트 레이어 시큐리티)를 사용함
		tls인증은 통신할떄 오가는 패킷을 암호화함
		보통 https웹서버를 설정할때는 서버에만 인증서를 설정하면되지만,tls는 클라이언트가 유효한지도 체크함
		그래서 kube-apiserver의 인증서와 연결되는 클라이언트 인증서를 이용해 접속함
		
		기본적으로 설치하면 디폴트값에 tsl인증정보가 포함되어있고,이걸 바꿔서 계정에 제한을 걸수있음
		
		kube/config파일을 열면됨
		이 안에
			cluster.insecure-skip-tls-verify는 true로주면 공인기관의 인증서인지 체크하지않고,false면 검증을 함
			보통은 프라이빗인증서를 쓰기떄문에(외부공개를안하니까) 기본값은 true임
			cluster.server는 외부에서 쿠버네티스api에 접속할 주소를 설정함
			외부에있는 쿠버네티스에 접근하려면 외부쿠버네티스의 kube-apiserver접근주소로 변경하면됨
			name은 클러스터의 이름을 설정함
			
			contexts필드는 사용자나 네임스페이스를 연결하는 설정임,상황에따라 여러컨텍스트설정이 있을수 있음
			
			context.cluster는 접근할 클러스터를 설정함
			context.user는 클러스터에 접근할 사용자 그룹이 누구인지를 설정함,users.name에 만들거 연결시키면됨
			context.namespace는 default네임스페이스가 아닌 특정 네임스페이스를 설정할수있음
			name는 컨텍스트의 이름
			
			current-context필드는 contexts필드가 여러개있을때 무엇을 선택해서 클러스터에 접근할지 결정함
			보통 접근할떄 kubectl config set-context 컨텍스트이름 으로 컨텍스트를 변경해서 다양한클러스터에 다양한 사용자로 접근할수있음

			users필드는 클러스터를 사용할 사용자 그룹을 명시함
			
			name은 사용자 그룹의 이름을 설정함
			name.client-certificate-data:클라이언트인증에 필요한 해시값을 설정함(tsl인증기반의 해시값쓰면됨)
			name.client-key-data:클라이언트의 키 해시값을 설정함(tsl인증기반의 해시값쓰면됨)
			
	2.서비스계정 토큰으로 인증하기
		config파일의 iser필드를 tls인증이 아닌 서비스계정을 사용하도록 변경할수도 있음
		
		kubectl get serviceaccount로 서비스계정을 확인하고
		kubectl get serviceaccount 서비스계정이름 -o yaml 로 전체출력한후
		시크릿의 name로 시크릿이름을 따고
		그거로 시크릿검색해서 토큰을 딸수있음
		
		그리고 config에
			.contexts[].context.user을 서비스계정이름으로 바꾸고
			.users[].name필드값으로 서비스계정이름으로 바꾸고
			.users[].user.token에 시크릿에서 딴 토큰 넣어주면됨
		
2.권한 관리
	쿠버네티스 클러스터의 api에 접근하려면 먼저 접근할수 있는 사용자인지부터 인증을 받아야함
	인증후엔 사용자가 접근하려는 api를 사용할 권한이 있는지 확인한 후에 api를 사용할수 있음
	
	클러스터 하나를 여러명이 사용할때는 api나 네임스페이스별로 권한을구분해서 권한이 있는곳에만 접근하게 할수있고,
	특정자원의 읽기권한만 추가해서 읽기만 할수있게 할수있음
	
	보통은 역할기반권한관리를 사용함
	사용자와 역할을 별개로 선언한후,사용자에 역할을 매핑시켜서 권한을 부여하는식
	
	1.롤
		롤은 특정 api나 자원사용권한들을 명시해둔 규칙집합임
		롤은 일반롤과 클러스터롤 두가지가 있음
		
		일반롤은 해당 롤이 속한 네임스페이스에만 적용됨
		추가로 네임스페이스에 한정되지 않은 자원과 api들의 사용권한을 설정할수있음
		노드사용권한이나 헬스체크용 url인 '/healthz'같은 엔드포인트사용권한도 관리함
		
		롤 yaml은
			kind:Role
			apiversion:api버전 rbac.authorization.k8s.io/v1
			metadata:
				namespace:네임스페이스이름
				name:롤이름
			rules:
				-apiGroups:['사용api'] 비어있으면 전체사용
				 resources:['사용자원(pod같은)']
				 verbs:['get','list'같은 사용가능한 동작지정]
				 
		metadata.namespace는 이 롤이 속한 네임스페이스를 적고(여기만적용됨)
		metadata.name은 이 롤의 이름
		rules는 구체적으로 규칙을 적는곳
			apiGroups는 롤이 사용할 api그룹을 설정함
			resources는 어떤 자원에 접근할수있는지 명시함
			verbs는 어떤 동작을 할수 있는지 설정함
			
			여기서 verv에서 설정할수있는값은
				Create:자원생성
				Get:개별자원조회
				List:여러자원조회
				Update:기존자원내용 전체 업데이트
				Patch:기존자원중 일부내용 변경
				Delete:개별자원 삭제
				deletecollection:여러자원삭제
			가 있음
			
		이거도 똑같이 apply로 적용하면됨
		
		그리고 전체파드가 아니라 특정파드에만 규칙을 설정할수도있는데
			rules:
				resourceNames:['파드이름']
		을 하면 개별값(겟,델리트,업데이트,패치)를 건드리는건 저 파드만 할수있고,
		전체값(리스트,크리에이트,딜리트컬렉션)을 건드리는건 저 옵션에 영향을받지않고 할수있음
		
	2.클러스터롤
		이건 특정 네임스페이스권한이 아닌 클러스터 전체 사용권한을 관리함
		사용법은 똑같은데,메타데이터에 네임스페이스지정이 없다는거정도만 다름
		
		그외 특이점은,
		.aggregationRule로 다른 클러스터롤을 조합해 사용할수 있음
		
			kind:ClusterRole
			apiversion:api버전
			metadata:
				name:이름
			aggregationRule:
				clusterRoleSelectors:
					-matchLabels:
						레이블키:레이블밸류
			rules:[]
			
		이렇게 aggregationRule로 레이블로 서치하고 가져와서 거기있는 룰을 적용시킬수있음
		그래서 룰은 비워둬도 되고 추가설정해도됨
		
		그리고 클러스터롤은 자원이 아니라 url식으로도 규칙을 설정할수 있음
		rules[]의 하위필드로 .nonResourceURLs을 설정하는것
			nonResourceURLs:['/healthcheck','/metrics/*']
		이런식으로 할수있음
		그리고 이렇게설정하면 verbs필드값은 get과 post만 설정할수있음
		
		
	3.롤바인딩
		롤바인딩은 롤과 사용자를 바인딩하는 역할임
		사용자가 어떤 롤을 사용할지설정함
		이거도 롤바인딩과 클러스터롤바인딩으로 구분되고,롤바인딩은 네임스페이스에,클러스터롤바인딩은 클러스터 전체에 적용된다는거도 같음
		
		롤바인딩의 yaml은
			kind:RoleBinding
			apiversion:api버전
			metadata:
				name:이름1
				namespace:네임스페이스이름1
			subjects:
				-kind:ServiceAccount
				 name:서비스어카운트이름1
				 apiGroup:''
			roleRef:
				kind:Role
				name:롤이름
				apiGroup:rbac.authorization.k8s.io
				
		메타데이터 연결하는건똑같고
		subjects필드는 어떤 유형의 계정과 연결하는지 설정함
		여기선 만들어둔 서비스어카운트와 연결하고,
		apiGroup가 ''인건 코어api그룹으로 설정했다는것,v1과 같음
		
		roleref는 이미 만들어둔 롤에 연결하는것
		카인드와 네임으로 만들어둔거에 연결하고
		여기서 apiGroup은 rbac api를뜻하는 rbac.authorization.k8s.io를 설정
		
	4.클러스터롤바인딩
		이거도 거의 똑같이 쓰면됨
		클러스터롤바인딩의 yaml은
			kind:ClusterRoleBinding
			apiversion:api버전
			metadata:
				name:이름1
				
			subjects:
				-kind:ServiceAccount
				 name:서비스어카운트이름1
				 namespace:네임스페이스이름1
				 apiGroup:''
			roleRef:
				kind:Role
				name:롤이름
				apiGroup:rbac.authorization.k8s.io
		거의 똑같은데,알아둬야할건 subjects의 kind에서 선택할수있는건 User,Group,ServiceAccount3개가 있는데,
		여기서 user과 serviceAccount는 네임스페이스 정보가 필요하고
		(롤바인딩은 어짜피 네임스페이스하나적용이니까 위에있는거 가져다쓰면되는데 클러스터롤바인딩은 위에 없으니까 여기적어줘야함),
		group는 클러스터전체에 사용할수있어서 필요없음
		클러스터 관련설정이니까 메타데이터밑에 네임스페이스가없고 서브젝트하위에 있음
		서비스계정은 네임스페이스에 한정되어있으므로 서브젝트의 네임스페이스필드값으로는 어떤 네임스페이스에 속한계정인지 명시함
		
				
	5.정리
		롤바인딩은 사용자와 롤을 묶어서 특정네임스페이스에 권한을 할당하고
		클러스터롤바인딩은 사용자와 클러스터롤을 묶어서 클러스터에 권한을 할당함
		
		kubectl config 로 사용자관련정보를 설정할수도있음(파일에서 직접바꾸는거랑 같음) 
		
		그리고 룰에 '*'하면 모든권한 다 여는거임




14.데이터 저장(볼륨)








