1.소개
	스킵
2.설치
	스킵

3.쿠버네티스로 컨테이너 실행	
1.kubectl
	kubectl의 기본 구조는
		kubectl 커맨드 타입or이름 (플래그)  플래그는 옵션
	임
	만약 kubectl get pod 를 하면 포드 전체를 겟(표시)하고
	   kubectl get abcd를 하면 abcd라는 이름을 가진걸 겟함
	   
	   
	여기서 커맨드에는
		get:명시된 데이터를 받아옴
		run:명시된 이름으로 명시된 이미지의 파드를 생성함
		apply:명시된 위치의 yaml파일을 기반으로 선언적으로 디플로이먼트를 생성해서 파드를 생성함
		scale:파드의 갯수를 늘이거나 줄임
		expose:명시된이름으로 명시된 타입의 서비스를 생성함
		port-forward:명시된 이름의 객체를 뒤에 포트(8080:25500)의 외부포트로 매핑함
		logs -f:명시된이름의 객체의 로그를 수집함
		exec:명시된 이름의 객체에 뒤에 나올 명령을 실행하라고 함
		  (kubectl (-n default) (exec) ((my-pod) (-c my-container)) (-- ls /) )
		          디폴트 네임스페이스에서 실행해라 마이포드를    마이컨테이너에 있는걸    쿠버네티스관련옵션을 전부 종료시키는
			마이컨테이너에 있는 마이포드의 쿠버네티스관련옵션을 전부 종료시키는 명령을 실행해라
		api-resources:사용할수 있는 자원들을 표시함	
			
	2.kubeconfig환경변수
		kubectl의 환경변수는 home/.kube/config에 있음
		여기서 클러스터에서 사용할수 있는 자원을 확인하는건 kubectl api-resources로 확인할수있음
		
		도커 데스크톱으로 쿠버네티스를 쓰면 자동으로 kubeconfig가 생성되고,
		--kubeconfig옵션으로 다른 설정파일을 지정할수 있음
		다중 클러스터에 다른 인증정보로 접근할때 사용함
		
	3.다양한 사용 예
		단순히 명령실행말고,파이프라인으로 현재값의 출력을 다음명령의 입력으로 넣을수도 있고 그런식으로 스크립트식 사용도 가능함
		
			
			
			
			
2.디플로이먼트를 이용해 컨테이너 실행
	앞에서 run으로 생성한건 직접 파드를 하나 추가한거고,보통은 apply로 디플로이먼트를 생성함
	이렇게해야 선언적으로 생성할수있어서 관리하기가 편함
			
3.클러스터 외부에서 클러스터 안 앱에 접근하기
	쿠버네티스 외부에서 쿠버네티스 내부에 접근하려면,직접 접근하면 안되고 쿠버네티스의 서비스를 통해서 접근해야함
	그래서 서비스를 생성해서 그쪽을 통해서 접근해야함
		kubectl expose deployment 이름 --type=NodePort
	하면 노드포트타입의 디플로이먼트를 생성하고(노드포트는 모든 노드의 포트를 할당함,즉 전체를 다받음 )
		kubectl get service로 포트를 확인하고 그 포트로 접속하면됨
		좀더 자세히 보려면
			kubectl describe service 이름
		으로 상세하게 볼수있음
			
			
			
			
			
			
4.쿠버네티스 아키텍쳐
1.쿠버네티스 클러스터 전체 구조
	쿠버네티스 클러스터는 클러스터를 관리하는 마스터와 실제컨테이너를 실행시키는 노드로 구성됨
	마스터에는 etcd(모든 설정등 적는건 여기다들어있음),apiserver(모든건 여기통해서 입출력을 함) 등이 들어있음
	
	노드는 kubelet,kube-porxy,docker등 컴포넌트가 실행되고,실제 사용하는 컨테이너의 대부분은 노드에서 실행됨
	
	구조는
		쿠버네티스에 명령을 주면 리버스프록시로 마스터api에 명령을 전달하고 걔가 노드랑 etc등으로 명령을 전달하고 실행시킴
	
	
	쿠버네티스의 관리용 컴포넌트들도 다 컨테이너로 실행됨
	
2.쿠버네티스의 주요 컴포넌트
	쿠버네티스는 기본적으로 클러스터를 관리함
	클러스터는 단일컴퓨터뿐만 아니라 여러대컴퓨터를 묶음으로 다루는걸 뜻하므로 여러가지의 컴포넌트를 포함함
	
	쿠버네티스의 컴포넌트는 관리에 필수인 마스터컴포넌트,노드컴포넌트와 추가로 붙인 애드온컴포넌트로 나눠짐
	
	1.마스터컴포넌트
		etcd:etcd는 키밸류 저장소임
			분산시스템에서 노드 사이의 상태를 공유하는,데이터베이스 역할을 함
			etcd는 서버 하나당 프로세스 하나만(즉 전체에서 하나밖에없음,클러스터링등으로 같은걸 복사할순있지만)존재함
			
		kube-apiserver:얘는 쿠버네티스의 api를 사용할수 있도록하는 컴포넌트임
						얘는 클러스터로 온 명령이 유효한지 검증하고(문법과 권한)그걸 실행해서 돌려줌
						얘는 수평적으로 확장 가능하니까(어짜피 api서버라서 유일성같은거없음)서버여러대에 여러개설치가능
		
		kube-scheduler:얘는 클러스터 안에 자원 할당이 가능한 노드중 알맞은 노드를 선택해서 파드를 생성하는 컨포넌트
						파드는 여러 요구조건을 받을수있으며,거기에 맞는 노드를 선택해서 생성함
		
		kube-controller-manager:얘는 파드들을 관리하는 컨트롤러를 실행하는 컴포넌트임
								클러스터에서 새로운 컨트롤러를 생성하고 실행할때 컨트롤러 매니저의 큐에 넣어서 실행하는식으로 동작함
		
		cloud-controller-manager:얘는 쿠버네티스의 컨트롤러를 클라우드와 연결해서 관리하는 컴포넌트임,필요해지면보자
		
	2.노드용 컴포넌트
		kubelet:얘는 클러스터 안의 모든 노드에서 실행하는 컴포넌트,파드컨테이너들의 실행을 직접 관리함,파드스펙이라는 조건설정을 전달받아서 실행하고
				컨테이너가 정상적으로 실행되는지 헬스체크를 진행함,단 노드안에 있는 컨테이너라도 쿠버네티스가 안만들었으면 관리하지않음
				(컨테이너안 파드 지웠을때 바로재시작거는게 얘인듯)
				
		kube-proxy:클러스터안에 별도의 가상 네트워크를 설정하고 관리하는 컴포넌트
		
		컨테이너 런타임:실제로 컨테이너를 실행시키는 컴포넌트,대표적으로 도커가 있음
		
	3.애드온
		네트워킹 애드온:클러스터 안에 가상네트워크를 구성해 사용할떄 프록시이외에 네트워킹 애드온을 사용함,얘가 직접 서버구성할때 가장 까다로움
		
		dns애드온:클러스터 안에서 동작하는 dns서버,쿠버네티스 서비스에 dns레코드를 제공함,쿠버네티스 안에 실행된 컨테이너들은 자동으로 dns서버에 등록됨
				주로 coreDNS를 사용함 
				
		대시보드 애드온:kubectl로 명령 주지만,gui로 볼때 대시보드애드온으로 사용할수있음
		
		컨테이너 자원 모니터링:컨테이너들의 자원사용량등을 시계열형식으로 저장해서 볼수있음
		
		클러스터 로깅:클러스터 안 개별 컨테이너의 로그와 구성요소의 로그를 모아서 보는 애드온
		
			
3.오브젝트와 컨트롤러			
	쿠버네티스는 오브젝트와 오브젝트를 관리하는 컨트롤러로 나눠짐
	사용자는 템플릿등으로 쿠버네티스에 자원의 바라는 상태를 정의하고,컨트롤러는 바라는상태와 현재상태가 일치하도록 오브젝트를 생성/삭제함
	오브젝트에는 파드,서비스,볼륨,네임스페이스 등이 있고 컨트롤러에는 레플리카셋,디플로이먼트,스테이트풀셋,데몬셋,잡등이 있음
	
	1.네임스페이스
		네임스페이스는 클러스터 하나를 논리적인 단위로 나눠서 실행하는것,
		이해하자면,컴퓨터 하나에 특정 폴더에 프로그램 바로가기로 싹 몰아두고 이름붙이는느낌임,그래서 그폴더를 전부 실행하거나 실행에 제한걸거나 하는식
		
		네임스페이스를 지정할때는 
			--namespace=붙일이름 
		으로 하나하나붙여도되는데
		
		그냥 디폴트값을 바꿀수도있음
			kubectl config current-context
		로 현재 컨텍스트 이름를 확인하고
			kubectl config set-context 컨텍스트이름 --namespace=붙일이름
		으로 디폴트값을 바꾸면 새로생성한값의 네임스페이스가 바뀜
		
		네임스페이스 전체검색하려면
			kubectl get pod --all-namespaces
		하면됨
		
		바꾼값 다시 디폴트로 바꾸려면 
			--namespace=""
		하면됨
		
	2.템플릿
		쿠버네티스 클러스터의 오브젝트나 컨트롤러가 어떤 상태여야 하는지 적용할떈 yaml형식의 템플릿을 적용함
		템플릿은 들여쓰기로 구조가 바뀌고(파이썬처럼),
		scalars(스트링,넘버),sequences(어레이,리스트),mappings(해시,딕셔너리) 3기초요소로 표현됨
		
		템플릿의 기본형식은
			apiversion:v1
			kind:Pod(생성종류)
			metadata:
			spec:
		로 구성됨
		apiversion은 api버전이고(쿠버네티스버전과 관련된)
		kind는 어떤 오브젝트나 컨트롤러의 작업인지를 명시하고
		metadata는 해당 오브젝트의 이름이나 레이블등을 설정하고
		spec는 파드가 어떤 컨테이너를 가지고 실행하며,실행할때 어떻게 동작해야할지를 명시함
		
		kubectl explain 생성종류  로 현재 생성할거에 무슨 하위필드가 있는지 출력해서 볼수있음
		
		하위필드를 포함해 특정필드를 커맨드라인에서 지정할때는 .metadata.anno이런식으로 .으로 이어가면됨,맨앞에도 .붙이는거에 주의
		필드설명없이 그 아래에 속한 모든필드를 보려면 --recursive옵션을 쓰면됨
		
		
			
			
			
			
5.파드
1.파드개념
	쿠버네티스는 파드라는 단위로 컨테이너를 묶어서 관리하므로,파드는 안에 컨테이너가 여러개로 구성되긴함
	그리고 파드 하나는 ip를 공유하고,내부의 컨테이너들끼리 포트로 구분해가며 데이터를 받음
	그리고 컨테이너 하나에 프로세스를 여러개 실행시킬수도 있지만,그러면 개빡세니까 보통은 컨테이너 하나에 프로세스 하나씩 해서 돌리는거같음
	
2.파드사용하기
	파드를 설정할때는
	yaml에
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
	
	이런식으로 설정함
	저기서 라벨은 오브젝트를 식별하는 레이블
	그리고 상위 바로 밑에 -를 붙이는건 여기서부터 하위필드를 배열로 묶겠다는 뜻임


3.파드 생명 주기
	파드는 생성부터 삭제까지의 과정에 생명주기가 있음
	
		pending:파드를 생성하는 중일때 나옴,이미지를 다운로드한후에 컨테이너를 실행해야하니까 시간 좀 걸림
		running:파드 안 모든 컨테이너가 실행중인 상태,1개이상의 컨테이너가 실행중이거나 시작,재시작상태일수 있음
		succeeded:성공적으로 모든 컨테이너가 실행 종료된상태,정상종료니까 재시작되지않음
		failed:파드 안 모든 컨테이너중 정상적인 실행 종료가 되지 않은 컨테이너가 있는 상태
		unknown:파드의 상태를 확인할수 없는 상태,파드가 있는 노드와 통신할수 없을때 주로 나옴
		
	
	현재 파드 생명주기는 
		kubectl describe pods 파드이름
	을 실행하고 status를 보면 나옴
	거기서 밑에 type가 있는데
	각 타입별 정보는
		initialized:모든 초기화컨테이너가 성공적으로 시작 완료됨
		Ready:파드는 요청을 실행할수 있고 모든 연결된 서비스의 로드밸런싱 풀에 추가되어야 한다는 뜻
		containersready:파드안 모든 컨테이너가 준비상태라는 뜻
		podScheduled:파드가 하나의 노드로 스케줄을 완료했다는 뜻
		unschedulable:스케줄러가 자원의 부족이나 다른 제약등으로 당장 파드를 스케줄 할수 없다는 뜻
	
4.kubelet로 컨테이너진단
	컨테이너가 실행 된 후에는 kubelet가 주기적으로 컨테이너를 진단함,이때 필요한 프로브는 2개가있음
		livenessprobe:컨테이너가 실행됐는지 확인함,이게 실패하면 kubelet는 컨테이너를 종료시키고 재시작 정책에 따라 컨테이너를 재시작함
		readinessprobe:컨테이너가 실행된 후 실제로 서비스 요청에 응답할수 있는지 진단함
					   이 진단이 실패하면 엔드포인트 컨트롤러는 해당 파드에 연결된 모든 서비스를 대상으로 엔드포인트 정보를 제거함
					   즉 서비스 연결이 안될거같으면 거기 연결 못하게 다지워버림
					   
					   
	
	컨테이너 진단할땐 컨테이너가 구현한 핸들러를 kubelet가 호출해서 실행함
	핸들러에는 3종류가 있음
		execaction:컨테이너 안에 지정된 명령을 실행하고 종료코드가 0일때 성공
		tcpsocketaction:컨테이너 안에 지정된 ip,포트로 tcp상태를 확인하고 열려있으면 성공
		httpgetaction:컨테이너 안에 지정된 ip,포트,경로로 http get명령을 보내서 응답코드가 200~400사이면 성공
		
	결과는 success,failure,unknown3개가 있음,언노운은 진단이 실패하서 컨테이너 상태를 알수없을때 나옴
		
	
5.초기화 컨테이너
	초기화 컨테이너는 앱 컨테이너가 실행되기 전 파드를 초기화함
	보안상이유로 앱 컨테이너이미지와 같이 두면 안되는 앱의 소스코드를 별도로 관리할때 유용
	초기화 컨테이너의 특징은
		초기화 컨테이너는 여러개를 구성할수 있음,초기화컨테이너가 여러개있으면 파드템플릿에 명시한 순서되로 초기화컨테이너가 실행됨
		
		초기화 컨테이너의 실행이 실패하면 성공할때까지 재시작함,그래서 무조건 순서대로 실행되니까 절차적으로 실행시켜서,
		쿠버네티스의 선언적이라는 특징에서 벗어날수있음
		
		초기화 컨테이너가 모두 실행 된 후 앱 컨테이너 실행이 시작됨
	
	
	그래서 이런특징을 이용해 파드를 실행할때 앱 컨테이너가 외부의 조건을 만족할때까지 기다렸다가 실행하게 만들수있음
	그리고 초기화 컨테이너는 readinessprobe를 지원하지 않음,파드가 준비되기 전 실행하고 사라지는 컨테이너이기때문
	
	초기화 컨테이너 yaml은
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			init컨테이너들
				-이름:이름
				이미지:이미지
				커맨드:실행하고 할 커맨드를 입력함(몇초 기다려라 이런거)
				
6.파드인프라컨테이너
	쿠버네티스에는 모든 파드에서 항상 실행되는 pause라는 컨테이너가 있음,이걸 파드 인프라 컨테이너 라고 함
	얘는 파드 안 기본 네트워크로 실행되고,프로세스 식별자가 1로 설정되므로 컨테이너의 부모 컨테이너 역할을 함
	파드 안 다른 컨테이너는 pause가 제공하는 네트워크를 공유해서 사용함,그래서 다른컨테이너가 재시작돼도 ip가 유지되지만,
	얘가 재시작되면 ip가 바뀌고 파드 안의 다른 모든 컨테이너도 전부 재시작함
	
	이 인프라를 바꿀수도 있는데 --pod-infra-container-image옵션으로 다른 컨테이너를 인트라컨테이너로 지정할수 있음
	

7.스태틱 파드
	얘들은 api서버를 거치지 않고 kubelet가 직접 실행하는 파드들임
	여기에 추가하고싶으면 --pod-mainfest-path라는 옵션에 지정한 디렉토리에 스태틱파드에 추가할 파드들을 넣어두면 kubelet가 감지해서 실행함
	
	얘들은 kubelet가 직접 관리하면서 이상이 생기면 재시작함
	그리고 kubelet가 실행중인 노드에서만 실행되고,다른노드에서는 실행되지 않음
	그리고 apiserver로 파드를 조회할수는있지만,거기에 명령을 내릴순 없음
	
	보통 스태틱파드는 apiserver나 etcd같은 시스템파드를 실행하는 용도로 많이 사용함 


8.파드에 cpu와 메모리 자원 할당
	만약 노드 하나에 자원 사용량이 많은 파드들이 모여있으면 파드들의 성능이 나빠지고,전체 클러스터의 자원사용 효율도 낮음
	어떤 노드에는 파드가 없어서 cpu메모리가 남고, 어떤 너드는 파드가 많아서 모자라는 그런일이 나타날수 있음
	
	그래서 쿠버네티스에서는 파드에서 자원의 최소치와 최대치를 지정할수 있게 준비해뒀음
	.spec.containers[].resources.limits.cpu
									   .memory
								.requests.cpu
										 .memory
							
	여기서 리미트는 그 파드가 쓸수있는 한도고,리퀘스트는 최소한 그정도는 써야한다는거임
	즉 노드에 배치될떄, 리퀘스트만큼 자원 여유가 있는 노드여야 거기에 스케줄링해서 배치해서 실행되고,
	그런게 없으면 pending로 대기하고 실행하지않다가 자원여유가 생기면 실행함
	
	리미트는 자원의 최대량임,파드가 아무리 많이써도 리미트 이상은 사용할수 없게 막음
	
	노드에 배치할때는 현재사용량을 보는게 아니라 리미트와 리퀘스트만 봄
	리미트가 설정되어있으면 리미트의 총합만큼을 보고 자리가 남으면 거기넣고,리미트가 설정안되면 리퀘스트의 총합을 가지고 넣음
	리미트가 설정안되면 오류가 날수있지만,노드자원을 최대한 많이 사용할수 있다는 장점이 있긴함
	
	그리고 cpu는 코어의 0.1단위로 잘라서 사용할수 있음

9.파드에 환경변수 설정
	컨테이너를 사용할때 장점은,개발환경에서 만든 컨테이너에 환경변수만 바꿔서 실제환경에서 실행해도 그대로 동작한다는 점임
	여기서 환경변수 설정하는방법은 스펙.컨테이너 밑에 env에 넣어주면됨
	
		api버전
		종류 파드
		메타데이터
			이름:붙일이름
			라벨
				앱:붙일이름
		스펙
			컨테이너들
				-이름:붙일이름
				 이미지:사용할이미지
				 포트
					-컨테이너포트:사용할포트번호
				env:
					-이름:이름1
					 값:값1
					-이름:이름2
					 valuefrom:
						필드ref:
						필드path:spec.nodeName이나 metadata.name,status.podIP등등 참조값 전부가능
					-이름:이름3
					 valuefrom:
						resourcefieldref:
							컨테이너이름:가져올컨테이너이름
							resource:requests.cpu,limits.cpu등등 
	
	이런식으로 가져오거나 설정할수있음
							
	이름은 환경변수의 이름이고(즉 저기에 적힌거:값 이런식으로 그대로 바로 적용됨 사용자정의아니면,즉 cpu사용량같은거 적어두면 바로적용됨)
	값(value)은 직접 적은 상수값
	valuefrom은 값을 어디서 참조하는거
	fieldref는 파드의 현재 설정 내용을 값으로 설정한다는 선언(현재파드내에서 가져온다는거)
	fieldpath는 어디서 값을 가져올것인지,즉 값을 참조하려는 항목의 위치를 지정
	resourcefieldref는 컨테이너에 cpu메모리 사용량을 얼마나 할당했는지에 관한 정보를 가져옴
	containername은 환경변수 설정을 가져올 컨테이너의 이름
	resource는 어떤 자원의 정보를 가져올지 설정


10.파드에 환경설정 내용 적용
	파드는 환경설정 내용을 yaml하나에 모두 작성한 후 적용해야함
	적용할떄는
		kubectl apply -f pod-all.yame
	을 실행하면됨
	
	확인할땐 
		kubectl exec -it kubernetes-simple-pod sh
	를 실행해서 컨테이너 안에 접속하고
		env
	치면 나옴
	
	
11.파드 구성 패턴
	1.사이드카패턴
		원래 사용하던 컨테이너의 기능을 확장하거나 강화하는 용도의 컨테이너를 추가하는것
		기본 컨테이너는 원래기능에만 충실하게 구성하고,나머지 공통부가기능들은 사이드카 컨테이너를 추가해서 사용하는것
		로그수집 컨테이너처럼 파일긁어서 보내는 역할만 하는 컨테이너가 예,이건 어딜붙여도 파일저장만 하면 똑같이쓸수있음
	2.앰배서더 패턴
		얘는 파드안에 프록시역할을 하는 컨테이너를 추가하는 패턴
		즉 파드 안에서 외부서버에 접근할때 내부프록시에 접근하게 하고,연결은 프록시에서 알아서 처리함
		이러면 파드의 트래픽을 더 세밀하게 제어할수 있음
	3.어댑터 패턴
		어댑터 패턴은 파드 외부로 노출되는 정보를 표준화하는 어댑터 컨테이너를 사용한다는 뜻임
		즉 파드 내부에서 정보를 어댑터로 보내고,저기서 정보를 취합해서 중앙모니터링에 보내면 거기서 모니터링하는데 이용하는 그런식
		





6.컨트롤러
1.레플리케이션 컨트롤러
	초창기 레플리카세트같은거임,요즘은 안씀
2.레플리카셋
	얘는 같은 파드를 복사해서 배포하는식으로 사용하는 컨트롤러임
	항상 선언한것과 같은 숫자의 파드가 클러스터 안에서 실행되도록 관리함
	
	그리고 얘는 롤링업데이트(25%정도씩 잘라서 파드를 업데이트-재시작하는것)을 하지못하니까 이거필요하면 디플로이먼트를 써야함
	1.레플리카셋 사용하기
	이거도 파드랑 똑같이 사용하면됨
		apiversion:앱버전
		kind:replicaset
		metadata:
			name:이름
		spec:
			template:
				metadata:
					name:이름1
					labels:
						app:이름1
				spec:
					containers:
						-name:이름1
						 image:이미지
						 ports:
						 -containerPort:포트번호
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
						
						
	자세한 명세는 .spec에 적으면되고,스펙밑에는 무슨파드를 실행할지를 적어야하니까 메타데이터 스펙이 한번 더 나옴	
	그리고 .spec.template.spec.container[]필드 밑에 파드의 정보를 설정함	
	.spec.replicas는 파드의 갯수를 설정하는것
	.spec.selector는 어떤 레이블의 파드를 선택해서 관리할지를 정함,즉 label.app에 있는거중 하나를 선택해서 그것만 관리함(몇개인지)
	즉 matchlabels랑 labels의 값이 같아야함
	만약 셀렉터를 비워두면 labels.app에 있는걸 기본값으로 설정함
	
	이 yaml을 apply로 클러스터에 적용하면됨
	
	여기서 만약 만들어진 파드를 지우면,바로 재생성이 되고,
	만약 yaml을 수정해서 파드의 갯수를 줄이면,바로 하나를 지움
	
	2.레플리카셋과 파드의 연관관계
		파드는 레이블기준으로 관리하므로 레플리카셋과 파드는 느슨하게 결합되어 있음
		레플리카셋과 파드를 한꺼번에 삭제할때는 delete replicaset 컨테이너이름으로 하면 삭제되지만,--cascade=false옵션을 주면
		연쇄삭제는 되지않고 레플리카셋만 삭제됨,그러면 현재 실행중인 파드를 관리하는 레플리카셋을 추가로 만들수도 있음(현재설정을 바꿔야하는게 바꿀게많을때)
		
		kubectl get replicaset으로 레플리카셋의 상태를 볼수있는데
		거기서 desired는 설정할때 지정한 파드의 수,current는 현재 파드의 수 임,정상적으로 동작하면 두개는 같아야함
		
		그리고 파드의 레이블을 바꾸면(.metadata.labels.app),
		레플리카셋은 레이블기반으로 체크를 하는데 레이블이 바뀌었으니까 다시 새로운걸 만들게됨
		
		이런식으로 레이블 설정 변경으로 실행중인 파드를 재시작하지 않고 서비스에서 분리해 디버깅하는등으로 사용할수 있음
		
3.디플로이먼트
	디플로이먼트는 쿠버네티스에서 상태가 없는 앱을 배포할때 사용하는 가장 기본적인 컨트롤러임
	요즘은 보통 디플로이먼트로 앱을 배포함
	
	디플로이먼트는 레플리카셋을 관리하면서앱 배포를 더 세밀하게 관리함
	단순히 실행시켜야 할 파드의 수를 유지하는거뿐 아니라,앱을 배포할때 롤링업데이트하거나,배포중 잠시 멈췄다가 배포하거나,업데이트를 롤백하거나 할수있음
	
	1,디플로이먼트 템플릿
		yaml구성은
			apiversion:api버전
			kind:Deployment
			metadata:
				name:이름1
				labels:
					apps:이름1
			spec:
				replicas:파드갯수
				selector:
					matchLabels:
						app:이름1
				template:
					metadata:
						labels:
							app:이름1
						spec:
							containers:
								-name:이름1
								 image:이미지
								 ports:
									containerPort:포트번호
	
		이렇게 구성됨
		
		파드를 몇개구성할지는 스펙밑에 레플리카고(레플리카셋이랑 똑같이)
		.spec.selector.matchLabels의 하위필드는 메타데이터의 레이블의 하위필드랑 같아야함
		파드의 설정정보가 있는 스펙.템플릿.스펙.컨테이너[]에 컨테이너의 이름과 이미지가 있음
		
		즉 레플리카셋이랑 별다른건 없음
		
		디플로이먼트가 제대로 실행되었는지는 kubectl get deploy,rs,pods 로 3개 다 확인할수 있음(디플로이먼트,레플리카셋,파드)
		
		즉 디플로이먼트는 레플리카셋을 감싸는 어댑터임(롤링업데이트등을 사용하게 해주는)
		
		디플로이먼트에서 이미지를 업테이트 하는방법은 총 3개가 있음
			kubectl set으로 컨테이너이미지를 지정하는것
			kubectl edit로 파드를 연다음 이미지정보를 수정하는것
			yaml을 수정하고 kubectl apply하는것
			
		보통 1,3이 쓰이는데 yaml을 보통 사용하는듯
		그리고 이렇게 디플로이먼트의 설정이 변경되면,새로운 레플리카셋이 생성되고,거기에 맞게 파드들이 변경됨(파드들이 새로생성됨)
		
		
		
	2.디플로이먼트 롤백
		컨테이너 이미지 변경 내역은
			kubectl rollout history deploy 디플로이먼트이름
		으로 확인할수있음
		
		만약 특정 리비전의 상세내용을 확인하려면 
			kubectl rollout history deploy 디플로이먼트이름--revision=리비전숫자 
		를 주면 확인할수있음(이미지버전이라던가)
		
		그 버전으로 롤백하려면
			kubectl rollout undo deploy 디플로이먼트이름--to-revision=리비전숫자
		로 버전선택해서 돌릴수있음
		돌리면 그번호가 삭제되고 새로 갱신되어서 제일 큰숫자로 바뀌어서 나옴(총 3개일때 2번으로 언두하면 2->4로바뀜)
		
		이 숫자는 겹치지않고 계속 커져나감
		
		
	3.파드개수 조정
		현재 실행중인 디플로이먼트의 파드 개수를 조정하려면 
			kubectl scale deploy 디플로이먼트이름 --replicas=파드갯수
		를 해주면됨
			
	4.디플로이먼트 배포정지,재개,재시작하기
		kubectl rollout로 진행중인 배포를 멈췄다가 다시 시작할수 있음
		
			kubectl rollout pause deployment/디플로이먼트이름
		으로 업데이트를 멈추고 
		
			kubectl rollout resume deployment/디플로이먼트이름
		로 다시시작할수있음
		
		그리고 디플로이먼트를 재시작시킬땐
			kubectl rollout restart deployment/디플로이먼트이름
		하면됨
		재시작은 디플로이먼트 스테이트풀셋 데몬셋에서 가능함
		
	5.디플로이먼트 상태
		배포중에는 디플로이먼트 상태가 변함
		progressing(진행)이었다가 compleate(완료),혹은 failed(실패)로 바뀜
		
		kubectl rollout status로 배포 진행 상태를 확인할수 있음
		
		디플로이먼트가 
			새로운 레플리카셋을 만들떄,
			새로운 레플리카셋의 파드수를 늘릴때,
			예전레플리카셋의 파드수를 줄일떄,
			새로운 파드가 준비상태가 되거나 이용가능한 상태가 되었을때
		진행상태로 표시됨
		
		배포가 끝나면 완료가 되고 종료코드가 0으로 표시됨
			디플로이먼트가 관리하는 모든 레플리카셋이 업데이트 완료되었을때
			모든 레플리카셋이 사용가능해졌을때
			예전 레플리카셋이 모두 종료되었을때
		완료가 됨
		
		배포중 이상이 있으면 실패가 됨
			쿼터부족(자원부족)
			readinessprobe실패
			컨테이너 이미지 가져오기 실패
			권한부족
			제한범위 초과
			앱실행조건 잘못지정
		일때 실패함
		
		템플릿에 .sepc.progressDeadlineSeconds항목을 추가하면 지정된 시간이 지났을때 실패로 처리할수있음
		
		
4.데몬셋
	데몬셋은 클러스터 전체 노드에 특정 파드를 실행할때 사용하는 컨트롤러임
	클러스터안에 새롭게 노드가 추가되었을때,자동으로 데몬셋이 해당 노드에 파드를 실행시키고,
	만약 노드가 클러스터에서 빠지면 해당 노드에 있던 파드는 그냥 사라짐
	
	즉 모든 노드에 하나씩 꼭 있어야하는 파드를 실행시킬때 사용함(로그수집같은)
	
	1.데몬셋 템플릿
		데몬셋의 템플릿은
			apiversion:api버전
			kind:daemonset
			metadata:
				name:이름1
				namespace:지정할네임스페이스(보통 시스템에넣는듯 컨셉상 그런느낌이라서)
				labels:
					k8s-app:fluentd-logging 이름2
			spec:
				selector:
					matchLabels:
						name:이름1
				updateStrategy:
					type:Rollingupdate
				template:
					metadata:
						labels:
							name:이름1
					spec:
						container:
							-name:이름1
							 image:사용할이미지
							 env:(환경변수)
								-name:환경변수1
								 value:환경변수1값
							resources:
								limits:
									memory:메모리값
								requests:
									cpu:cpu값
									memory:메모리값 
									
	
				
		이런식으로 나감
		보통 데몬셋은 시스템같은 느낌으로 사용되기때문에 네임스페이스를 별도로 만들어서 사용함(ku-system이런 이름으로 )
		그리고 레이블은 k8s-app필드에 값은 fluentd-logging로 줌(이건 키워드인지 아닌지 모르겠음 나중에해보자)
		데몬셋을 업데이트 하는 방법은 updateStrategy.type필드값에 적힌대로 됨,롤링업데이트와 온딜리트 2개가있음
		컨테이너의 이미지는 컨테이너에 있음
		
		이런후 똑같이 apply하면됨
	
	2.데몬셋의 파드업데이트 방법 변경하기
		이거도 롤링업데이트하면 순차적으로 바뀌고,온딜리트하면 삭제되기전까진 안바뀜
		이거 바꿀때는 
			kubectl edit daemonset 데몬셋이름 -n 네임스페이스이름
		으로 에디터띄우고 값바꾸면 됨
		
	
		
5.스테이트풀셋
	앞에서 나온 레플리카셋,디플로이먼트는 상태가 없는 파드들을 관리하는 용도고,상태가 있는 파드들을 관리하려면 스테이트풀셋을 써야함
	
	스테이트풀셋을 사용하면 볼륨을 이용해서 데이터를 저장한후 그 데이터를 파드가 재시작해도 유지할수 있음
	여러 파드사이에 순서를 지정해서 실행되게 할수도 있음
	
	1.스테이트풀셋 사용하기
		yaml구조는
			apiversion:api버전
			kind:Service //서비스도 있어야함 상태저장해야하니까
			metadata:
				name:서비스이름1
				labels:
					app:서비스이름1
			spec:
				ports:
					-port:포트번호1
					 name:web 포트이름1
				clusterip:none  //내부망이니까
				selector:
					app:서비스이름1
					
			
			apiversion:api버전
			kind:StatefulSet
			metadata:
				name:web 포트이름1
			spec:
				selector:
					matchLabels:
						app:스테이트풀이름1
				servicename:서비스이름1
				replicas:파드갯수
				template:
					metadata:
						labels:
							app:스테이트풀이름1
					spec:
						terminationGracePeriodSeconds:10
						container:
							-name:스테이트풀이름1
							 image:사용이미지
							 ports:
								-containerPort:포트번호1
								 name:포트이름1


		여기서 스테이트풀은 통신을 해서 저장하고 읽어야하기때문에 서비스가 있어야하고,스테이트풀에서 서비스를 받아야함
		서비스이름과 스테이트풀셋에서 만들어질 파드 이름을 조합하면 클러스터안에서 사용하는 도메인을 만들수있음,파드이름.서비스이름 형식
		그리고 스테이트풀셋의 이름을 web(포트이름1)로 설정하고
		terminationGracePeriodSeconds는 그레이스풀의(실행중인 프로세스를 종료할때 끝낼시간을 주는것)대기시간을 설정한것
		
		그리고 yaml저장하고 apply하면됨
		
		스테이트풀셋으로 만들면 파드이름뒤에 난수가붙는게 아니라 포트이름뒤에 0,1,2순서대로 붙음
		실행할때는 작은숫자부터 실행되고,만약 앞에꺼가 실행되지않으면 뒤에거도 전부 실행되지 않음
		그리고 삭제할때는 큰숫자부터 삭제됨


	2.파드를 순서없이 실행하거나 종료하기
		기본값은 순서대로 관리하는거지만 .spec.podManagementPolocy필드로 순서를 없앨수도 있음
		기본값은 OrderedReady(순서대로)고 Parallel로 순서없이로 바꿀수있음
		
		실행중인 스테이트풀에서는 바꿀수없고,새로운 스테이트풀을 만들어야함
	
	3.스테이트풀셋으로 파드업데이트 하기
		업데이트방법은 .spec.updateStrategy.type에 설정할수 있음
		기본값은 롤링업데이트
		
		그리고 마지막거부터 업데이트되고
		.spec.updateStrategy.rollingUpdate.partition값을 바꾸면 그거보다 큰값만 업데이트됨
		즉 파드들이 분할이 됨
		이렇게하면 레이블항목에 레이블이 두개로 나눠지는데,
		
		이걸 사용하면 스테이트풀셋이 관리하는 전체파드들 중에서 특정 파드에만 서비스를 할수있게됨


6.잡
	잡은 실행된 후 종료되어야 하는 성격의 작업을 실행할때 사용하는 컨트롤러
	특정갯수만큼의 파드를 정상적으로 실행종료함을 보장함
	만약 파드실행실패,하드웨어장애발생,노드재시작등 문제가 발생하면 다시 파드를 실행함
	즉 일회성으로 뭘 실행시켜야할때 쓰거나 그럴듯
	
	1.잡 사용하기
		yaml은
			apiversion:api버전
			kind:job
			metadata:
				name:이름1
			spec:
				template:
					spec:
						containers:
							-name:이름1
							 image:이미지
							 command:실행할커맨드
						restartPolicy:never
				backofflimit:4
				
		restartPolicy는 never로 설정하면 파드가 항상 성공으로 끝나게하고,
		onfailure는 파드 안 컨테이너가 비정상종료하거나 다양한 이유로 정상종료되지않으면 컨테이너를 다시시작하게 함
		backofflimit는 잡 실행이 실패했을때 몇번까지 반복할것인지 설정함
		보통 한번실패하고 좀 기다렸다가 다시시작하고 그런식으로 점점 시간간격을늘리다가 실행성공하면 다시 재시도횟수는 0으로 초기화됨
	
	2.잡병렬성관리
		잡 하나가 몇개의 파드를 동시에 실행할수 있는지를 잡 병렬성이라고함
		기본값은 1이고 0으로주면 잡을 정지할수 있음
			단 여러이유로 옵션을 설정해도 지정한 값보다 잡이 파드를 적게 실행시킬수 있음
				정상완료되는 잡의 개수를 고정하려면,병렬로 실행되는 실제 파드의 수가 정상완료를 기다리며 남아있는 잡의 개수를 넘지 않아야 함
				즉 자리가 10자린데 현재 완료된게 9개면 병렬을 3으로줘도 1개만 실행됨
			워크큐용 잡에서는 파드 하나가 정상적으로 완료되었을때 새 파드가 실행되지않음,단 현재실행중인잡은 완료될때까지 실행됨
			잡 컨트롤러가 반응하지못할때도있음
			자원이 모자라거나 권한이 부족할수있음
			잡에서  파드들이 실패를 너무 많이했으면 새 파드생성을 제한할수 있음
			파드가 그레이스풀하게 종료되었을수 있음
	3.잡의 종류
		잡은 단일잡,완료된 잡갯수가 있는 병렬잡,워크큐가 있는 병렬잡이 있음
		
		단일잡은
			파드 하나만 실행됨,파드가 정상적으로 실행종료되면(succeeded) 잡 실행을 완료함
			spec.completions와 spec.parallelism필드를 설정하지 않음,둘의 기본값은 1
				spec.completions는 정상적으로 실행종료되어야하는 파드갯수
				spec.parallelism는 병렬적으로 몇개실행할지 정하는거
			즉 둘이1이면 하나만실행해서 하나만끝남
			
		완료갯수가있는 병렬잡은
			spec.completions를 양수로 설정하고
			spec.parallelism는 1로 설정
		즉 완료해야하는건 여러개고 실행은 하나만
		
		워크큐가 있는 병렬잡은
			spec.completions는 설정하지않고
			spec.parallelism는 양수로 설정
			
			만약 completions을 설정하지않고 parallelism를 설정하면,completions은 parallelism를 따라감(동일하게설정됨)
			(단 기본값인 1이니까 하나완료되면 끝임)
			
			파드 각각은 정상적으로 실행종료됐는지를 독립적으로 결정할수 있음
			즉 대기열에 있는 작업들이 동시에 실행할수도있음
			
			파드 하나라도 정상종료되면 새로운파드가 실행되지않음
			
			최소한 파드 1개가 정상종료된후 모든 파드가 실행종료되면 잡이 정상적으로 종료됨
			
			일단 파드1개가 정상종료되면 다른 파드는 더이상 동작하지 않거나 결과를 내지않고 종료함
			

	4.비정상종료된 파드관리
		만약 파드안에 비정상종료된 컨테이너가 있을때를 대비해서,
		컨테이너 재시작정책을 설정하는 .spec.template.spec.restartPolicy를 지정할수 있음
		저걸 never로 두면 비정상종료되었을때 재시작을 막고 잡에서 새 파드를 실행함(노드가 장애나 업그레이드등으로 정지되었을때)
		그런데 spec.parallelism과 spec.completions이 1이고 restartPolicy가 never이면 같은프로그램이 2번 실행될수있음
		만약 둘을 1보다 크게 설정하면 한번에 여러파드가 실행될수 있음
		이런 상황들을 알고있어야함
		
	5.잡 종료와 정리
		잡이 정상적으로 실행 종료되면 파드가 새로 생성되지도 삭제되지도 않고,잡도 남아있음
		파드나 잡이 삭제되지않고 남아있으면 로그에서 에러나 경고를 확인할수있고 잡의상태도 계속 확인할수있음
		특정시간을 지정해 잡실행을 종료하려면,spec.activedeadlineseconds에 시간을 지정하면됨
		그러면 그 시간에 잡 실행을 강제로 끝내고 파드실행을 종료함
		
		그러면 잡의 상태를 확인했을때 종료이유가 reason:deadlineexceeded로 표시됨
		
		잡 삭제는 kubectl delte job 잡이름 으로 사용자가 직접 삭제해야함
		그리고 잡을 삭제하면 관련 파드들도 같이 삭제됨
		
	6.잡패턴
		잡에서 파드를 병렬로 실행했을때 파드 각각이 서로 통신하면서 동작하지 않음
		각 파드는 독립적으로 동작하는걸 전제로 두는데,메일을 발송하거나 파일을 변환하는등은 분산작업이니까 한번에 실행해야해서 제대로 동작하지않음
		
		잡의 사용패턴은
			작업마다 잡을 하나씩 생성해 사용하는거보단 모든 작업을 관리하는 잡 하나를 사용하는게 좋음
			잡의 생성비용은 비쌈
			
			작업개수만큼의 파드를 생성하는거보다 파드가 여러작업을 처리하는게 좋음
			파드를 생성하는거도 비쌈
			
			워크큐를 사용한다면 카프카같은거로 구현해야함,기본설정그대로는 비효율적임



7.크론잡
	크론잡은 잡을 시간기준으로 관리함,즉 지정시간에 잡을 한번만 실행하거나,지정한시간동안 주기적으로잡을 반복실행할수있음
	실행한후에는 잡과 동일하게 동작함
	
	1.크론잡 템플릿
		yaml은
			apiversion:api버전
			kind:CronJob
			metadata:
				name:hello
			spec:
				schedule: '*/1 * * * *' //매 1분마다 실행
				jobTemplate:
					spec:
						template:
							spec:
								container:
									-name:이름
									 image:이미지
									 args:
										여기다가 셸스크립트를 넣을수있음
									 
								restartPolicy:onfailure		

		

		schedule: '*/1 * * * *'는 실행할 시간을 정하는건데 cron과 같은형식이라니까 그걸찾아보자
		어떤 작업을 실행할지는 이미지를 사용하는거고
		커맨드처럼 셀스크립트를 쓸수있고
		리스타트정책도 설정할수있음

		kubectl get cronjobs로 크론잡의 스케줄설정을 확인할수있고,현재 정지중인지 아닌지를 알수있음(가동중이면 false)
		그리고 active로 현재 잡이 몇개실행중인지 알수있고
		크론잡이 실행한 잡은 kubectl get jobs로 알수있음
		
		삭제는 kubectl delete cronjobs 크론잡이름 으로 삭제함
		그러면 크론잡이 생성한 잡들까지 한꺼번에 삭제됨
		
	2.크론잡 설정
		크론잡의 spec.startingdeadlineseconds는 지정된 시간에 크론잡이 실행되지못했을떄,값으로 설정한 시간이 지나면 실행되지않게 함
		이걸 설정안하면 시간이 많이지나도 제약없이 실행함
		
		크론잡의 spec.concurrencyPolicy는 크론잡이 실행하는 잡의 동시성을 관리함
		기본값은 allow로 잡을 여러개 동시에 실행할수 있게 함
		forbid로 설정하면 잡을 동시에 실행하지 않도록 함
		
		만약 이전에 실행했던 잡이 정상종료되지않고 실행중이면,
		만약 forbid면 해당시간에 새로 잡을 실행하지 않고 다음 시간에 실행함,즉 두개이상 실행막음
		만약 replace면 이전실행잡을 지우고 새 실행잡으로 대체함
		
		이건 스케줄밑에 넣으면 됨(크론잡공간에)
		
		spec.successfuljobshistirylimit와 spec.failedfuljobshistirylimit는 잡이 정상종료되었는지 비정상종료되었는지 로그를
		몇개까지 저장할지 설정함,기본값은 3,1이고 0으로하면 저장하지 않음
		그러면 kubectl get pods했을때 status가 compleated로 적혀서 갯수만큼 남아있음(새거로 갱신되면서)






7.서비스
1.서비스의 개념
	쿠버네티스 클러스터 안에 컨트롤러로 파드를 실행했으면,거기 접근할수 있어야하는데 그때 쓰는게 서비스임
	파드는 매번 죽었다 살아나서 재생성될수있고,그때마다 ip주소가 바뀌기때문에 직접접근하긴 머리아파서 쓰는게 서비스임
	서비스를 쓰면 서비스에 통신하면 알아서 파드로 연결해줌
2.서비스타입
	서비스타입은 크게 4가지가 있음
		clusterIP:기본타입이며 클러스터안에서만 사용할수 있음
				  클러스터 안 노드나 파드에서는 클러스터 ip를 사용해서 서비스에 연결된 파드에 접근,외부에선 이용할수없음
		
		NodePort:서비스 하나에 모든 노드의 지정된 포트를 할당함
				 node1:8080,node2:8080처럼 노드에 상관없이 서비스에 지정된 포트번호만 사용하면 파드에 접근할수 있음
				 노드의 포트를 사용하므로 클러스터 외부에서도 접근할수있음
				 특이한점은 파드가 노드1에만 실행되어있고 2에 없어도 2에 접근하면 자동으로 1로 연결해줌
				 외부에서 안으로 접글할때 가장 쉬운방법
		
		LoadBalancer:아마존,구글클라우드등에서 쿠버네티스를 지원하는 로드밸런서장비에서 사용함
					 클라우드에서 제공하는 로드밸런서와 파드를 연결한 후 해당 ip를 이용해 외부에서 파드에 접근할수 있게 해줌
					 kubectl get service로 서비스를 확인하면 external-ip에 로드밸런서 ip를 표시하는데,그걸로 외부에서 접근하면됨
		
		ExternalName:서비스를 .spec.externalName에 설정한값과 연결
					 이건 보통 클러스터내부에서 외부에 접근할때 사용,
					 특정 사이트같은데 도메인 넣어두고 접근해서 html받는식으로 씀
					 외부에 접근할떄 쓰는거라서 셀렉터가 필요없음


3.서비스 템플릿
	서비스의 기본 yaml은
		apiversion:api버전
		kind:Service 
		metadata:
			name:서비스이름1
		spec:
			type:ClusterIP
			clusterIP:10.0.10.10  사용할아이피주소,안적으면 자동으로 할당함
			selector:
				app:앱이름  자동연결할 파드 레이블값
			ports:
				-protocol:TCP
				 port:80
				 targetPort:9376
				 
	스펙의 타입에서 서비스타입을 설정할수있고
	스펙 클러스터ip에서 ip를 직접 설정할수있고
	selector에서 연결할 파드레이블 설정할수있고
	port에서 포트설정할수있음,한번에 여러개를 외부에 제공할떈 포트하위에 필드값을 설정하면 됨
	
	1.clusterIP타입 사용하기
		클러스터ip타입은 위에 있던 거임
		
		클러스터ip타입은 넷샷같은시스템파드에 접근하면 걔가 서비스로 통신하고 파드랑 연결해서 사용자와 파드를 연결시켜주는식임
		클러스터ip는 외부랑 직접연결할수없기떄문
		
		이거도 
			kubectl describe service 서비스이름
		으로 상태볼수있는데
		
		똑같이 endpoint가 연결된 하위파드들임
		
		여기 연결확인할떈 클러스터내부서비스ip로 파드하나 만들어서 거기서 연결해봐야함
		
	2.nodeport타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:NodePort
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
					 nodePort:30080 노드포트값
		이거도 다른건 다똑같고,타입선언하고 포트밑에 노드포트값만 넣어줌
		
		로컬일경우 localhost:노드포트값으로 접근하면 접근할수있고
		외부일경우 컴퓨터ip:노드포트값으로 접근할수있음
		즉 파드를 하나만들고 거기로 접근할필요없이 바로 접근할수있음
		
		내부적으로는 외부포트와 클러스터ip식으로 만들어진서비스를 매핑하는식으로 구현된듯
		
	3.LoadBalancer타입 사용하기
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:LoadBalancer
				selector:
					app:앱이름  자동연결할 파드 레이블값
				ports:
					-protocol:TCP
					 port:80
					 targetPort:9376
		로 기본에서 그냥 타입만 바꾸면됨
		
		얘는 클러스터를 외부 로드밸런서와 연결할때 쓰기떄문에,외부와 연결한뒤 그걸 받아서 서비스를 하게됨
		뭐 쓰기는 노드포트랑 똑같음,단지 세팅할떄 외부와 연결해줘야한다는거고
		
	4.ExternalName
		서비스의 기본 yaml은
			apiversion:api버전
			kind:Service 
			metadata:
				name:서비스이름1
			spec:
				type:ExternalName
				externalName:url값
		이건 그냥 외부url과 연결하는거임
		구글같은데다 연결하면 그냥 구글html을 받아오고 그런식
				

4.헤드리스서비스
	.spec.clusterIP를 none로 설정하면 클러스터ip가 없는 서비스를 만들수있음
	이런걸 헤드리스 서비스라고 함
	이런건 로드밸런싱이 필요없거나 단일서비스 ip가 필요없을때 사용함
	
	헤드리스 서비스에 셀렉터를 설정하면 쿠버네티스api로 확인할수있는 엔드포인트가 만들어지고,
	서비스와 연결된 파드를 직접 가르키는 dns a레코드도 만들어짐(도메인주소와 서버의ip주소를 직접 매핑시키는거,196.123.22.1 이런식의 ip주소)
	셀렉터가 없으면 엔드포인트가 만들어지지않는데,
	셀렉터가 없어도 dns시스템은 externalName에서 사용할 cname레코드(naver.com처럼 문자로된 도메인같은걸 ip주소로 변경시키는거)는 만들어짐
	
	이건 그냥 클러스터ip설정에서 클러스터ip를 None로 주기만 하면 됨
	
5.kube-proxy
	프록시는 쿠버네티스에서 서비스를 만들었을떄,클러스터ip나 노드포트로 접근할수 있게 만들어서 실제 조작을 하는 컴포넌트임
	kube-proxy가 네트워크를 관리하는 방법은 userspace,iptables,ipvs가 있음 
	
	1.userspace
		유저스페이스는 클라이언트에서 서비스의 클러스터ip를 통해 요청을하면 iptables을 거쳐서 프록시가 요청을 받고,
		서비스의 클러스터ip는 연결되어야할 파드로 연결해줌,즉 포워드프록시임
		
	2.iptables
		iptables는 클라이언트가 iptables로 직접 접근하고,프록시는 iptables을 관리하는역할만 함,
		직접 클라이언트에서 트래픽을 받진 않고 클라이언트에서 오는 모든 요청은 iptables을 거쳐서 파드로 직접 전달됨
		그래서 중간에 프록시를 거치지않기때문에 요청처리성능이 좋음
		그리고 특징으론 유저스페이스는 프록시가 연결을 책임지기때문에 파드하나에 연결실패하면 다른파드에 계속 연결시켜주려고 재시도하는데,
		이건 책임이 클라이언트에 있으니까 그냥 실패하면 실패임
		컨테이너에서 readinessprobe가 설정되어있고 헬스체크가 성공해야 연결이 이루어짐
		
	3.IPVS
		ipvs는 리눅스에 있는 l4로드밸런싱기술임
		리눅스 커널 안 네트워크 프레임워크인 넷필터에 포함되어있음
		즉 ipvs커널 모듈이 노드에 설치되어있어야함
		
		구조자체는 iptables과 같은데,이건 클러스터ip가 가상서버로 동작하고,백엔드파드들이 실제서버임
		얘는 커널공간에서 동작하고 데이터구조가 해시테이블이라서 iptables보다 빠르고 좋은성능을 내고
		로드밸런싱 알고리즘이 많아서 이걸 이용할수있음
			rr:프로세스사이에 우선순위를 두지않고 순서와 시간단위로 cpu할당
			lc:접속 개수가 가장 적은 서버를 선택
			dh:목적지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sh:출발지 ip주소로 해시값을 계산해 분산할 실제 서버를 선택함
			sed:응답속도가 가장 빠른 서버를 선택함 
			nq:sed+활성접속개수가 0개인 서버를 먼저 선택함
		
		가 있음
		



8.인그레스
	인그레스는 클러스터 외부에서 안으로 접근하는 요청을 어떻게 처리할지 정의해둔 규칙모음임
	클러스터 외부에서 접근할 url을 사용할수있게 하고,트래픽로드밸런싱,인증서처리,도메인기반 가상호스팅도 지원함
	인그레스 자체는 이런규칙들을 정의해둔 자원이고,실제동작시키는건 인그레스 컨트롤러
	
	클라우드 서비스를 사용하면 별다른 설정없이 자체로드밸런서와 연동해서 인그레스를 사용할수 있음
	클라우드 서비스를 사용하지않고 직접 쿠버네티스 클러스터를 구축해서 사용한다면 인그레스컨트롤러를 인그레스와 연동해야함
	이때 가장 많이 사용하는 도구는 ingress-ngibx임
	얘는 인그레스에 설정한 내용을 nginx환경설정으로 변환해서 ngibx에 적용함
	
	이거말고도 다양한 컨트롤러등이 있음
	
1.인그레스 템플릿
	yaml은
		apiversion:api버전
		kind:ingress
		metadata:
			name:이름
			annotations:
				nginx.ingress.kubernetes.io/rewrite-target: /  엔진엑스컨트롤러 설정으로,/(루트)로 리다이렉트하라는뜻
		spec:
			rules:
				-host:foo.bar.com  url 아무거나
				 http:
					paths:
						-path:/foos1
						 backend:
							serviceName:서비스1
							servicePort:서비스포트1
						-path:/bars2
						 backend:
							serviceName:서비스2
							servicePort:서비스포트1	
				-host:bar.foo.com
				 http:
					paths:
						-backend:
							serviceName:서비스2
							servicePort:서비스포트1


	이런식으로 쓰는데
	인그레스 설정은 어노테이션 하위필드를 쓰는데,인그레스 컨트롤러마다 상세항목은 다 다름
	그리고 spec.rules의 하위필드에서 어떤 규칙을 사용할지 지정할수있는데
	host필드값은 url값임(foo.bar.com)
	저거로 요청이 들어오면 여기 하위로 들어온다는소리
	
	거기 뒤에 뭐가 붙었냐에 따라(path) 백엔드로 선택할 포트를 나눠지게 설정됨(foo.bar.com/foos1)
	거기로 나눠진걸 서비스네임에서 지정된 서비스의 포트로 던지라는소리
	
	확인할떈 똑같이 
		kubectl describe ingress 인그레스이름
	으로 확인할수있음
	
	즉 인그레스는 규칙에따라 서비스를 배분해주는 역할임
	
2.ingress-nginx 컨트롤러
	인그레스는 설정일뿐이고 설정내용대로 동작하는 주체는 인그레스컨트롤러임
	엔진엑스 인그레스컨트롤러를 설치하고
		kubectl apply -k.
	로 인그레스컨트롤러와 노드포트타임의 인그레스엔진엑스 컨트롤러에 접근하는 서비스까지 만들어짐
	
	확인해보면 디플로이먼트와 서비스가 하나씩 생성됨
	
	그리고 로컬로 테스트할때는 dns가 없으니까 dns저장파일(시스템32밑에 있는거)를 수동으로 127.0.0.1 사용할url을 매핑해줘야함
	그리고 인그레스에서 지정한 이름으로 서비스를 만들고 url뒤에 값(foos1)을 붙여서 던져보면 그쪽서비스로 연결됨

	또 인그레스 컨트롤러의 네트워크 옵션을 호스트모드로 설정하면,별도의 nodeport타입서비스 없이도 인그레스 컨트롤러에 접근할수 있고,
	다시 컨트롤러에서 파드로 직접 접근할수 있으므로 중간의 서비스들을 생략해서 좀 더 좋은 성능을 낼수 있음
		
3.인그레스 SSL인증서 설정하기
	인그레스를 이용하면 요청으로 들어오는 트래픽에 다양한 설정을 할수 있음
	인그레스로 ssl인증서를 설정하면 파드 각각에 ssl설정을 할필요가 없고,기한이 만료되어도 인그레스에서만 인증서를 업데이트 하면 됨
	
	ssl인증서의 인증구조는
		ca에서 서명된 인증서를 발급
		인증서를 검증한후 ssl로 통신
	인데 인증서 발급을 요청하면 ca측에서 관리하는 키와 인증서로 서명한 인증서를 발급해줌,
	그걸 서버에 설정하면 웹브라우저에서 통신할때 서버에 있는 인증서가 유효한 인증서인지 확인하고 ssl통신을 함
	
	설정하는 밥법은 ca에서 crt파일과 key파일을 받고
		kubectl create secret tls 시크릿이름 --key 키이름.key --cert crt이름.crt
	명령으로 시크릿을 생성하고
		apiversion:api버전
		kind:ingress
		metadata:
			name:이름
		spec:
			tls:
				-hosts:
					-url이름
				secretName:시크릿이름
			rules:
				-host:url이름
				 http:
					paths:
						-path:/
						 backend:
							serviceName:서비스이름
							servicePort:서비스포트
							
	이렇게 스펙의 하위필드로 tls필드에서 url과 연결할 시크릿을 넣어주면됨
				
	이렇게해서 인증서가 들어갔는지 확인하려면
		curl -vl https://url:포트/
	를 날려보면 리턴됨
	

4.무중단배포를 할때 주의할점
	인그레스를 이용해 외부에서 컨테이너를 접근할때,새로운 버전의 컨테이너를 배포할때와 겹치면
	정상적인 상황에서는 새로운버전을 만들고 헬스체크를 한 다음 서비스가 그쪽으로 연결을 옮기고 전버전을 삭제함
	그러면 무중단배포임
	
	1.maxsurge와 maxunavailable설정
		파드관리를 롤링업데이트로 설정했을때,maxsurge와 maxunavailable 필드 설정이 필요함
		디플로이먼트를 이용해서 컨테이너를 배포할때,
		maxsurge는 디플로이먼트에 설정된 기본 파드 개수에 여분의 파드를 몇개 더 추가할수 있는지를 설정하고
		maxunavailable은 디플로이먼트를 업데이트하는동안 몇개의 파드를 이용할수 없어도 되는지를 설정함
		
		이 2개를 운영중인 서비스에 맞게 적절히 조정해야 일정한수의 파드를 이용할수 있고,그래야 트래픽유실이 없음
		두필드를 한꺼번에 0으로 설정하면안됨,파드가 없을수도 있기때문

	2.파드가 readinessprobe를 지원하는지 확인
		파드가 readinessprobe를 가지고있으면 그거받아서 파드가 초기화완료됐는지를 확인할수있는데(책임이 이쪽에없음)
		파드가 readinessprobe가 없으면 준비되지 않은 컨테이너에 요청이 가서 응답을 제대로 하지 못할수 있음
		
		그래서 서비스가 준비된 상태인지 진단하는 readinessprobe를 지원하는게 좋은데,없으면
		.spec.minReadySeconds필드로 어느정도 비슷한 효과를 낼수있음
		이건 파드가 준비상태일때까지의 최소대기시간으로, 파드가 실행된후 저기 적힌시간동안은 트래픽을 받지 않음
		그러니 좀 초기화시간에 맞춰서 좀 길게잡아주는게 좋음
		물론 readinessprobe있으면 그거쓰면되고
	
	3.쿠버네티스와 컨테이너 안에 그레이스풀 종료 설정
		노드 안 컨테이너를 관리하는 컴포넌트인 kubelet는 새 파드가 실행되고 이전 파드를 종료할떄 파드에 sigterm신호를 먼저 보냄
		무중단 배포를 하려면 컨테이너에서 sigterm신호를 받았을때 기존에 받은 요청만 처리를 완료하고 새 요청을 받지 않는 그레이스풀 종료가
		설정되어있어야 함
		
		그렇지않으면 종료된 파드로 요청 전달하는상황이 생김,
		인그레스 컨트롤러가 업데이트 되기전까진 파드가 살아있는거로 나와서 트래픽을 전버전파드에 보내는거임
		
		만약 파드에 그레이스풀 종료를 설정하지 못하는상황이면 프리스톱훅을 이용할수 있음
		
		쿠버네티스에서는 파드 생명주기 중 훅을 설정할수 있음
		파드가 실행된 직후 실행하는 포스트스타트훅과 파드가 종료되기 직전 실행되는 프리스톱 훅이 있음
		프리스톱훅은 파드에 sigterm신호를 보내기 전 실행되므로 컨테이너와 별개로 그레이스풀 종료와 같은 효과를 낼수 있음
		또한 프리스톱 훅의 실행이 완료되기 전까지는 컨테이너에 sigterm신호를 보내지 않으므로,컨테이너 설정과 별개로 종료전 대기시간도 설정할수 있음
		이렇게 대기시간을 설정해도 terminationGracePeriodSeconds필드에 설정할 대기시간을 초과하면 프로세스 종료되니까 이건 주의
		
		


9.레이블
	레이블은 키밸류쌍으로 구성하며,사용자가 클러스터 안에 오브젝트를 만들때 메타데이터로 설정할수 있음
	레이블이 생성된 다음에도 언제든지 수정할수 있음
	
	레이블의 키는 쿠버네티스안에서 컨트롤러들이 파드를 관리할 때 자신이 관리해야 할 파드를 구분하는 역할
	쿠버네티스는 레이블만으로 관리대상을 구분하므로,컨트롤러가 만든 파드라도 레이블을 바꾸면 인식할수없음
	그래서 유연성이 있는것
	
	이걸 사용한예로 운영중인 파드 하나의 레이블을 바꿔서 파드상태를 확인할수있고,그거로 디버깅도 할수있음
	
	이거 외에도 노드에도 레이블을 붙일수 있으니까,
	클러스터 안 노드들을 레이블로 구분한다음 특정 레이블(gpu같은)이 있는데만 자원을 할당해 실행하는거도 가능 
	
	보통 사용자가 이름붙일땐 접두어없이하는게 관습임
	
	레이블을 선택할때는 레이블셀렉터를 사용하는데,등호기반과(a=b)집합기반(a in b)이 있음
	그리고 and연산할때는 ,로 연결하면 됨
	등호기반은 그냥 그대로고
	집합기반에서 여러개값을 설정할떄는
		a in (b,c,d)
	이런식으로하면 되고
		gpu
		!gpu
	이렇게하면 값은확인하지않고 gpu라는 키가 있는 레이블을 전부 선택함
	
1.에너테이션 
	에너테이션은 레이블과 마찬가지로 키밸류쌍으로 구성하며 레이블처럼 사용자가 설정할수 있음
	레이블이 사용자가 설정한 특정 레이블의 오브젝트들을 선택한다면,에너테이션은 쿠버네티스 시스템이 필요한 정보들을 담고,걔들이 자원관리하는데 이용함
	그래서 에너테이션의 키는 쿠버네티스 시스템이 인식할수 있는 값을 사용함
	예를들어 앱을 배포할때 변경사유를 적는 CHANGE-CAUSE처럼,키워드를 써야함 사용자정의변수가 아니라
	

2.레이블을 이용한 카나리배포
	배포에는 롤링업데이트,블루/그린(기존파드와 같은개수의 신규파드를 실행하고,헬스체크가 끝나면 트래픽을 한꺼번에 신규파드로옮김),카나리등이 있음
	여기서 카나리는 기존파드에서 신규파드 한두개정도 섞어넣어서 정상적으로 작동하는지 체크하는것
	
	카나리를 하는방법은 그냥 레이블을 2개쓰는거임
	기존에 배포할때 쓰는레이블과,카나리구분용 레이블을 둘다 적어서 기존배포레이블에 카나리가 섞이게 만든다음,문제가생기면 카나리레이블로 걷어내는거
	
	배포자체는 디플로이먼트 2개를 쓰는거임 원래쓰던거1개 카나리1개,거기서 서비스는 레이블만 보고 트래픽배분하니까 기존배포레이블을 서비스에넣으면
	카나리랑 기존거랑 구분없이 트래픽을 줌
	
	여기서 문제가 생기면 카나리 디플로이먼트를 삭제하거나 카나리디플로이먼트의 레플리카를 0으로 설정하는거처럼 해서 서비스에서 제외시키면됨
	




10.컨피그맵
	컨피그맵은 설정값들을(db패스워드아이디같은거를) 저장해뒀다가 테스트랑 사용서비스를 다르게쓸때 그때 편하게 바꿔치기할려고 사용하는거임
	그래서 컨테이너는 그대로 쓰고 컨피그맵만 갈아치우는식으로 쓰는거
	
1.컨피그맵 템플릿
	컴피그맵 yaml은
		apiversion:api버전
		kind:ConfigMap
		metadata:
			name:컨피그맵이름1
			namespace:네임스페이스이름1
		data:
			db_url:localhost 키밸류쌍,뭘적어도됨
			db_user:myuser
			db_pass:mypass
			debug_info:debug
	
	이런식으로 데이터밑에 원하는 키밸류쌍을 적어서 그걸 가져다 쓰는거
	
	저걸 가져다쓰려면
	디플로이먼트에서
		앱버전
		카인드
		메타데이터
		스펙
		템플릿
			메타데이터
			스펙
				컨테이너
				env:
					name:설정이름
					valuefrom:
						configMapKeyRef:
							name:가져올컨피그맵이름
							key:컨피그맵의 키값 
			
		서비스생성파트는 생략
		

		
	이렇게 값을 한두개 가져올수도 있고,
	전체를 가져오려면
		env를
		envfrom:
			-configMapKeyRef:
				name:가져올컨피그맵이름
	으로 전체를 가져올수있음
	
	연결은 이렇게하고,실제로 쓸때는 상용과 테스트용 2개만든후에 저기 가져올컨피그맵이름만 바꾸고 apply하면됨
	
2.컨피그맵을 볼륨에 불러와서 사용하기
	컨피그맵을 컨테이너의 환경변수로 설정하는것이 아닌 다른 방식으로 사용할수도 있음
	컨테이너의 볼륨형식으로 컨피그맵을 설정해서 파일로 컨테이너에 제공할수 있음 
	
	사용법은
		앱버전
		카인드
		메타데이터
		스펙
		템플릿
			메타데이터
			스펙
				컨테이너
					이름:
					이미지:
					포트:
					volumeMounts:
						-name:볼륨이름
						 mountPath:/etc/config
				volumes:
					-name:볼륨이름
					 configMap:
						이름:컨피그맵이름
			
		서비스생성파트는 생략
		
		
	볼륨마운트로 볼륨을 선언하고 이름을붙이고 마운트위치를 정하고
	볼륨으로 볼륨이름이 가질 컨피그맵을 선언하고 이름을 연결해주면
	컨피그맵의 필드값 하나하나마다 파일이 하나씩 생성됨 마운트위치에
	
	


11.시크릿
1.시크릿 만들기
	시크릿은 비밀번호나 개인키같은 민감한정보를 저장하는 용도로 사용함,
	이런건 컨테이너안에 저장하지 않고 별도로 보관했다가 실제 파드를 실행할때의 템플릿으로 컨테이너에 제공함
	
	시크릿은 내장시크릿과 사용자정의 시크릿이 있음
	내장시크릿은 클러스터안에서 쿠버네티스 api에 접근할때 사용함
	클러스터 안에서 사용하는 serviceAccount라는 계정을 생성하면 자동으로 관련 시크릿을 만듬
	이 시크릿으로 서비스 어카운트가 사용권한을 갖는 api에 접근할수 있음
	
	사용자 정의 시크릿은 사용자가 만든 시크릿임
	
	서비스어카운트 생성방법은
			apiversion:api버전
			kind:ServiceAccount
			metadata:
				name:이름
				namespace:네임스페이스이름
	
	시크릿을 만드는 방법은 2개가있는데
	
		kubectl create secret generic 시크릿이름 --from-file=./파일경로 --from-file=./파일경로 --from-file=./파일경로
	저렇게 만들수도있고,
	템플릿으로 만들수도 있음
		apiversion:api버전
		kind:Secret
		metadata:
			name:시크릿이름
		type:Opaque
		data:
			키1:값1
			키2:값2
	
	여기서 type는 4개가있는데
		Opaque는 기본값으로 키값형식으로 임의의 데이터를 설정할수 있고
		kibernetes.io/service-account-token은 쿠버네티스 인증 토큰을 저장하고
		kubernetes.io/dockerconfigjson은 도커 저장소 인증정보를 저장하고
		kubernetes.io/tls는 tls인증서를 저장함
		
	그리고 시크릿을 확인하는건 똑같이
		kubectl get secret 이름 -o yaml
	로확인하면됨
	
	그리고 필드값은 base64인코딩방식으로 인코딩되어서 나옴


2.시크릿 사용하기
	시크릿은 파드의 환경변수나 볼륨을 이용한 파일형식으로 사용할수 있음
	1.환경변수로 시크릿사용
		디플로이먼트에선
			.spec.template.spec.container[].env 단에
			
			env:
				-name:붙일환경변수이름
				 valuefrom:
					secretKeyRef:
						name:연결할시크릿이름
						key:원하는키값
	
		이렇게 env하단에 저런식으로 연결하면됨
		
		그리고 사용할 시크릿은 미리 만들어져 있어야 함,
		만약 오타등으로 없는시크릿을 참조하거나 시크릿이 없으면 에러가 발생해 파드가 실행되지못함
		
		kube-apiserver에선 문제없다고 생각해서 스케줄링하는데,
		kubelet에서 불러올때 에러가 발생함,
		그리고 kubelet는 계속 불러오려고 재시도하니까 조심
		
		
	2.볼륨형식으로 파드에 시크릿 제공
		볼륨형식으로 파드에 시크릿넣을땐 그 컨피그맵때 했던거랑 똑같이 volumeMounts와 volumes로 하면됨
		
		디플로이먼트에서
			.spec.template.spec.container[].volumeMounts[]와 .spec.template.spec.container[].volumes[]
			
			volumeMounts:
				-name:볼륨이름1
				 mountPath:'경로'
				 readonly:true
			volumes:
				-name:볼륨이름1
				 secret:
					secretName:연결할시크릿이름
					
		이러면 볼륨마운트로 볼륨을 생성하고 경로를 지정하고 볼륨에서 볼륨이름에 시크릿을 연결해서 파일로 저장함
		
		제대로됐나 확인하려면 
			http://localhost:30900/volume-config?path=/etc/volume-secret/username
		이런식으로 경로에 접근해서 파일을 켜서 확인할수 있음
		
		그리고 kubectl get pods로 시크릿 설정이 된 파드이름을 확인하고,
		kubectl exec -it 파드이름 sh 로 컨테이너에 직접 접근해서 파일위치에 가서 확인할수도있음
		
	
	3.프라이빗 컨테이너 이미지를 가져올때 시크릿 사용
		보통 컨테이너 이미지를 가져올땐 공개된 이미지를 사용하지만,프라이빗이미지를 사용할땐 인증정보가 필요함
		로컬서버를 사용하면 인증정보를 저장해서 사용할수 있지만,그러면 보안상의 위험이 있어서 그렇게 설정하지 않음
		인증정보를 시크릿에 설정해 저장한 후 사용함
		
		쿠버네티스에는 kubectl create secret의 하위명령으로 도커 컨테이너 이미지저장소용 시크릿을 만드는 docker-registry가 있음
			kubectl create secret docker-registry dockersecret 
			--docker-username:이름 --docker-password:패스워드 --docker-email=이메일 --docker-server=url주소
			
		이런식으로 시크릿을 만들수있음 
		저기서 이름 패스워드 url을 넣어서 거기로 연결하면됨
		
		저걸 kubectl get secrets dockersecret -o yaml로 확인하면
		data아래에 dockerconfigjson이 있는데 그게 도커인증정보값임
		그리고 type는 kubernetes.io/dockerconfigjson으로 자동으로 되어있고
		
		이걸 디플로이먼트에서 사용할땐
			컨테이너 밑에 이미지를 불러오고
			containers:
				...
				imagePullSecrets:
					-name:dockersecret
		이렇게 시크릿을 연결해주면됨
		
		만약 시크릿이 틀리거나 설정을 안하면 errimagepull에러가 나게 됨
		
		
	4.시크릿으로 tls 인증서를 저장해 사용하기
		https 인증서를 저장하는 용도로 시크릿을 사용할수 있음
		인증서파일(tls.key tls.crt)를 받고
			kubectl create secret tls 시크릿이름 --key tls.key --cert tls.crt
			
		로 시크릿을 생성하고
			kubectl get secrets 시크릿이름 -o yaml
		으로 확인하면
		
		data 밑에 tls.crt tls.key가 있고,type는 kubernetes.io/tls로 설정되어있음
		이걸 인그레스와 연결해서 사용할수 있음
						

3.시크릿 데이터 용량 제한
	시크릿데이터는 etcd에 암호화 하지 않은 텍스트로 저장됨
	이때 시크릿 데이터의 용량이 너무 크면 쿠버네티스의 apiserver나 kubelet의 메모리 용량을 많이 차지함
	그래서 개별시크릿 데이터의 최대용량은 1메가고,작은거도 많이있으면 같은문제생기니까 총용량제한도 있음
	
	그리고 만약 누가 etcd에 직접 접근하면 시크릿데이터의 내용을 확인할수 있음
	etcd에는 이외에도 중요한 데이터가 많이 있으므로 중요한 서비스에 쿠버네티스를 사용중이라면 etcd의 접근을 제한해야함
	
	기본적으로 etcd를 실행할때 etcd 관련 명령을 사용하는 api통신에 tls인증이 적용되어있으므로 인증서가 있는 사용자만 etcd에 접근해 관련 명령을
	사용할수 있음
	
	그외에 etcd가 실행중인 마스터 자체에 접속해서 데이터 접근하는걸 막기위해 마스터에 접근을 계정단위로 막거나,
	etcd에 저장되는 시크릿데이터를 암호화할수 있음,이때는 별도로 암호화옵션을 지정해야함





12.파드 스케줄링
	파드 스케줄링은 파드를 어떤 노드에 실행할것인지에 관해 옵션을 줘서 원하는대로 노드에 배치하는것
	특정 레이블의 파드를 노드 하나에 모아두거나 특정ip대역의 노드에서만 실행할수도있고,
	반대로도 할수있음
	같은 기능이 있는 파드들이 한군데 모여있지 않게 골고루 분산해서 실행할수도 있음
	관리가 필요한 노드들을 다른 노드로 옮길수도 있음
1.노드셀렉터
	노드셀럭터는 파드의 .spec에 설정할수 있음
	이건 단어그대로 노드를 선택하는 기능임
	
	파드가 클러스터 안 어떤 노드에서 실행될지를 키 값 노드로 설정할수있음
	
	노드의 레이블을 보는건
		kubectl get nodes --show-labels
	로 볼수있고
		kubectl label nodes 이름 키=밸류
	로 레이블을 추가할수도 있음
	
	그런다음 파드의 .spec.nodeSelector 밑에 키:밸류를 명시하는것으로 거기에 속한데서만 실행할수 있게 설정할수있음
	

2.어피니티와 안티어피니티
	어피니티는 파드들을 한데 묶어서 같은 노드에서 실행하게 하는거고
	안티어피니티는 파드들을 다른 노드에 나누어서 실행하도록 설정하는것
	
	1.노드 어피니티
		노드 어피니티는 노드셀렉터와 비슷하게 노드의 레이블 기반으로 파드를 스케줄링함
		
		노드 어피니티와 노드 셀렉터를 동시에 설정할수도 있으며,이떄는 둘다 만족하는 노드에 파드를 스케줄링함
		
		노드 어피니티에는 두가지 필드가 있는데
			requiredDuringSchedulingIgnoredDuringExecution 은 스케줄링하는데 꼭 필요한 조건
			preferredDuringSchedulingIgnoredDuringExecution 은 스케줄링하는동안 만족하면 좋은 조건
			
		두 필드는 실행중에 조건이 바뀌어도 무시함,즉 스케줄링 할때만 영향을 끼치고,중간에 레이블이 바뀌어도 이미 실행중인 파드는 그대로 실행됨
		실행중에 노드조건변경을 감지하는 필드도 생긴다고함(이미생겼는진모르겠음)
		
		이거도 파드에 .spec밑에
			spec:
				affinity:
					nodeAffinity:
						requiredDuringSchedulingIgnoredDuringExecution:
							nodeSelectorTerms:
								-matchExpressions:
									-key:키1
									 operator:In (어케할건지 조건 설정)
									 values:
										-linux
										-window
									-key:키2
									 operator:Exists(어케할건지 조건설정)
						preferredDuringSchedulingIgnoredDuringExecution:
							-weight:10
							 preference:
								matchExpressions:
									-key:키
									 operator:In
									 values:
										-worker-node01


		이런식으로 씀
		키는 노드의 레이블키중 하나를 성정하고,오퍼레이터는 키가 만족할 조건임
		오퍼레이터는
			In:밸류필드에 설정한값중 레이블에 있는 값과 일치하는게 하나라도 있는지 확인
			notIn:in과 반대로 모두 맞지않는지 확인
			Exists:밸류없이 키필드에 있는값이 노드레이블에 있는지 확인
			doesnotExist:exists와 반대로 노드의 레이블에 key값이 없는지만 확인
			gt:밸류필드에 설정된값보다 큰 숫자형 데이터인지만 확인,이때는 값이 하나만있어야함
			lt:밸류필드에 설정된값보다 작은 숫자형 데이터인지만 확인,이때는 값이 하나만있어야함

		가 있음
		
		그리고 preferredDuringSchedulingIgnoredDuringExecution은
		weight가 있고, nodeSelectorTerms대신 preference를 쓴다는거 대신에는 똑같음
		preference는 가능하면 그 조건에 맞는걸 선호한다는 뜻
		조건을 만족하는거랑 아닌거있으면 만족하는노드를 선택하고,전부 아니면 그냥 아무데나 들어감
		weight는 1부터 100까지 값을 선택할수 있는데
		각 설정마다 웨이스트를 줄수있고,만족하는걸 전부 더해 가장 큰 노드를 선택함
		
	2.파드의 어피니티와 안티 어피니티
		파드 사이의 어피니티와 안티 어피니티는,
		디플로이먼트나 스테이트풀셋으로 파드를 배포했을때 개별 파드 사이의 관계를 정의하는 용도임
		
		컨테이너로 서비스를 운영하다 보면 서비스 a의 파드와 서비스 b의 파드들끼리 자주 통신할때가 있으면,서로 같은노드에 속하게 만들수있음
		대표적으로 db나 캐시와 통신하는 앱컨테이너를 같은노드에 두는것
		
		쿠버네티스 클러스터를 구성할때 리눅스의 bpf와 xd를 이용하는 실리엄 네트워크 플러그인을 쓰면 같은노드의 컨테이너끼리의 통신성능이 더 올라간대
		
		안티어피니티는 cpu나 네트워크를 많이 사용하는 컨테이너들을 여러 노드로 파드를 분산하는것,
		이걸 설정하지않으면 디플로이먼트로 배포한 파드가 노드 하나에만 실행되어서 자원을 많이 소모할수 있음
		그리고 서버를 늘렸을때 이미 시스템사용률이 높은 노드에 다시 가서 오히려 심해지거나 할수있기때문에,
		안티 어피니티로 자원을 많이쓰는 파드들은 분산시키는것
		
		안티어피니티의 설정은
			.spec.template.spec.affinity에 하면됨
			
				spec:
				...
				template:
					...
					spec:
						...
						affinity:
							podAntiAffinity:
								requiredDuringSchedulingIgnoredDuringExecution:
									-labelSelector:
										matchExpressions:
											-key:키1
											 operator:In
											 values:
												-밸류1
									topologyKey:'kubernetes.io/hostname'
									
							podAffinity:
								requiredDuringSchedulingIgnoredDuringExecution:
									-labelSelector:
										matchExpressions:
											-key:키2
											 operator:In
											 values:
												-밸류2
									topologyKey:'kubernetes.io/hostname'
									
		파드의 어피니티와 안티어피니티는 노드어피니티와 다르게,
		.spec.template.spec.affinity의 하위필드인 .podAffinity와 .podAntiAffinity로 설정함
		그이후에는 노드어피니티와 거의 같음
		
		
		topologyKey는 노드의 레이블을 이용해 파드의 어피니티와 안티 어피니티를 설정할수 있는 또 하나의 기준임
		쿠버네티스는 파드를 스케줄링할때 먼저 파드의 레이블기준으로 대상노드를 찾고,
		그리고 topologyKey필드를 확인해서 해당 노드가 원하는노드인지 확인함
		hostname을 기준으로 어피니티 설정을 만족하면 같은노드에 파드를 실행하고,안티어피니티 설정을 만족하면 다른노드에 파드를 실행함
		즉,노드뿐 아니라 idc의 같은렉인지 다른idc인지까지 구분할때를 위해 있는거임
		전체적인 조건을 하나 더 사용할수 있는거
		
		topologyKey는 성능이나 보안상의 이유로 제약사항이 있는데
			podAffinity의 하위필드와 podAffinity.requiredDuringSchedulingIgnoredDuringExecution의 하위필드엔
			topologyKey가 필수로 명시해야함
			
			podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution하위와 어드미션컨트롤러의 하위에 설정하는
			topologyKey는 kubernetes.io/hostname만 설정할수 있음
			다른걸쓰고싶으면 어드미션 컨트롤러의 필드설정을 바꿔야함
			
			podAffinity.preferredDuringSchedulingIgnoredDuringExecution의 하위필드는,
			topologyKey필드를 설정하지 않아도됨 이경우에는 전체 토폴로지를 대상으로 안티어피니티설정을 만족하는지 확인함
			전체 토폴로지는 kubernetes.io/hostname,failure-domain.kubernetes.io/zone,
			failure-domain.beta.kubernetes.io/region
			을 뜻함
			
		저 3가지를 제외하면 topologyKey는 필드에서 레이블에 사용하는 키라면 무엇이든 설정할수 있음
			

		이렇게 설정하면 어피니티끼리는 뭉치고 안티어피니티끼리는 흩어짐



3.테인트와 톨러테이션
	테인트는 특정노드에 테인트를 설정할수 있음
	그러면 테인트를 설정한 노드에는 파드들을 스케줄링 하지 않음
	
	그 테인트를 설정한 노드에 파드들을 스케줄링 하려면 톨러레이션을 설정해야 함
	그럼 테인트는 톨러레이션에서 설정한 특정 파드들만 실행하고 다른 파드는 실행하지 못하게 할수잇음
	
	이건 노드를 특정 역할만 하도록 만들때 사용함
	
	예를 들어 데이터베이스용 파드를 실행한 후,노드 전체의 cpu나 ram을 독점해서 사용할수 있게 설정하고
	gpu가 있는 노드에는 gpu를 사용하는 파드들만 실행되게 설정하는것
	
	테인트는 키,값,효과 3개로 구성됨
		kubectl taint nodes 노드이름 키=값:효과
	로 작성함
	
	효과는
		NoSchedule:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,기존에 실행되던 파드에는 적용되지않음
		PreferNoSchedule:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,
						하지만 클러스터 안 자원이 부족하면 테인트를 설정한 노드에서도 파드를 스케줄링할수 있음
		NoExecute:톨러레이션 설정이 없으면 파드를 스케줄링하지 않음,기존파드도 톨러레이션이없으면 종료시킴
		
	
	확인하는건
		kubectl describe nodes 노드이름
	으로 확인할수 있음
	
	톨러레이션은
		spec.template.spec.tolerations에 적으면됨
			tolerations:
				-keys:'키1'
				 operator:'equal' 오퍼레이터
				 value:'밸류1'
				 effect:'noSchedule'효과
				 
	
	그리고 저걸 넣은 디플로이먼트를 적용하면 테인트가 있는데에 스케줄링함
	
	테인트를 삭제하려면
		kubectl taint nodes 노드이름 키:효과-
	로 삭제할수있음
	
	tolerations의 하위 필드값으로 원하는 테인트의 설정값을 넣고,
	operator는 equal과 exists가 있음
	이퀄은 키 밸류 이펙트가 다 같은지 확인하고 exist는 앞 3개중 선별해서 사용할때 사용함
	즉 exist를 설정하면 value필드를 사용할수 없음
	
	그리고 operator값으로 exists만 설정하고 아무것도 안넣으면,
	테인트가 있든없든 아무데나 가서 스케줄링하고,
	키값만 설정하면 키값만 볼수있음(이펙트무시하고)
		tolerations:
			operator:'exists'
	와
		tolerations:
			-key:'키값1'
			 operator:'exists'
	이런식
	
			
4.클러스터를 관리하는 커든과 드레인
	특정 노드에 있는 파드들을 모두 다른 노드로 옮기거나 특정 노드에 파드들을 스케줄링 하지 않도록 제한할때 사용함
	앞에서 본 테인트도 같은용도의 명령어임
	
	1.커든설정
		커든을 설정하려면
			kubectl cordon 노드이름
		을 하면됨
		커든을 설정하면 그 노드에 더이상 파드를 스케줄링 하지 않음
		
		커든을 해제하려면
			kubectl uncordon 노드이름
		을 하면 됨
	2.드레인설정
		드레인은 노드 관리등의 이유로 지정된 노드에 있는 파드들을 다른 파드로 이동시키는 명령임
		먼저 새로운 파드를 노드에 스케줄링 하지 못하게 막고(커든)기존 해당노드에서 실행중이던 파드들을 삭제함
		
		노드에 데몬셋으로 실행한 파드들이 있으면 드레인을 쓸수없는데(삭제해도 즉시 재실행시켜서)
		이걸 무시하고 드레인하려면 --ignore-daemonsets=true옵션을 주면됨
		
		그리고 컨트롤러를 이용하지 않고 실행한 파드들도 드레인설정을 적용할수 없음
		컨트롤러를 이용하지 않은애들은 다시 복구할수 없기때문
		이걸 무시하고 강제삭제하려면 --force옵션을 붙이면됨
		
		그리고 kubectl이 직접 실행한 스태틱파드들은 apiserver로 실행되지 않아서 삭제되지 않음
		여기에 드레인을 적용하면 그레이스풀하게 파드들을 종료함
		파드들이 정상적으로 잘 종료되게 설정되었으면 종료명령을 받은 즉시 삭제하는건 아니고 기존작업을 정리한 후 종료됨
		
		사용법은 
			kubectl drain 노드이름 (옵션)
		으로 데몬셋있으면 뒤에 옵션붙이고 그냥실행한 파드있어도 옵션붙이면됨
		
		드레인을 해제하려면 커든해제하는거랑똑같이
			kubectl uncordon 노드이름
		하면됨,커든걸고 다 삭제하는게 내부구조이기때문
		
		
		
	

13.인증과 권한 관리
1.인증
	apiserver는 테스트목적으로 로컬호스트:8080에 http서버를 실행함
	그리고 일반적인 https인증은 접근하는 클라이언트에 인증서를 요구하지 않음
	
	하지만 사용자가 쿠버네티스의 api에 접근하려면 인증을 거쳐야함
	외부에서 쿠버네티스 api에 접근할수 있는 기본포트는 6443이고,tls인증이 적용되어 있음
	6443포트에 접근해 통신하려면 apiserver에 있는 인증서와 클라이언트의 인증서 사이의 검증을 통과해야함,
	인증되지않은 클라이언트가 api서버에 접근하는걸 막는것
	
	쿠버네티스는 일반적인 사용자계정과 서비스계정으로 인증을 요청함
	일반적인 사용자계정은 구글계정이나 그런 외부의 인증시스템을 연결해 사용하는거고,서비스계정은 쿠버네티스가 직접 관리하는 사용자계정임,
	시크릿을 할당해서 비밀번호역할을 하는것
	
	1.kubectl의 config파일에 있는 tsl인증정보구조 확인하기
		쿠버네티스는 apiserver와 통신할때의 기본 인증방법으로 tls(트랜스포트 레이어 시큐리티)를 사용함
		tls인증은 통신할떄 오가는 패킷을 암호화함
		보통 https웹서버를 설정할때는 서버에만 인증서를 설정하면되지만,tls는 클라이언트가 유효한지도 체크함
		그래서 kube-apiserver의 인증서와 연결되는 클라이언트 인증서를 이용해 접속함
		
		기본적으로 설치하면 디폴트값에 tsl인증정보가 포함되어있고,이걸 바꿔서 계정에 제한을 걸수있음
		
		kube/config파일을 열면됨
		이 안에
			cluster.insecure-skip-tls-verify는 true로주면 공인기관의 인증서인지 체크하지않고,false면 검증을 함
			보통은 프라이빗인증서를 쓰기떄문에(외부공개를안하니까) 기본값은 true임
			cluster.server는 외부에서 쿠버네티스api에 접속할 주소를 설정함
			외부에있는 쿠버네티스에 접근하려면 외부쿠버네티스의 kube-apiserver접근주소로 변경하면됨
			name은 클러스터의 이름을 설정함
			
			contexts필드는 사용자나 네임스페이스를 연결하는 설정임,상황에따라 여러컨텍스트설정이 있을수 있음
			
			context.cluster는 접근할 클러스터를 설정함
			context.user는 클러스터에 접근할 사용자 그룹이 누구인지를 설정함,users.name에 만들거 연결시키면됨
			context.namespace는 default네임스페이스가 아닌 특정 네임스페이스를 설정할수있음
			name는 컨텍스트의 이름
			
			current-context필드는 contexts필드가 여러개있을때 무엇을 선택해서 클러스터에 접근할지 결정함
			보통 접근할떄 kubectl config set-context 컨텍스트이름 으로 컨텍스트를 변경해서 다양한클러스터에 다양한 사용자로 접근할수있음

			users필드는 클러스터를 사용할 사용자 그룹을 명시함
			
			name은 사용자 그룹의 이름을 설정함
			name.client-certificate-data:클라이언트인증에 필요한 해시값을 설정함(tsl인증기반의 해시값쓰면됨)
			name.client-key-data:클라이언트의 키 해시값을 설정함(tsl인증기반의 해시값쓰면됨)
			
	2.서비스계정 토큰으로 인증하기
		config파일의 user필드를 tls인증이 아닌 서비스계정을 사용하도록 변경할수도 있음
		
		kubectl get serviceaccount로 서비스계정을 확인하고
		kubectl get serviceaccount 서비스계정이름 -o yaml 로 전체출력한후
		시크릿의 name로 시크릿이름을 따고
		그거로 시크릿검색해서 토큰을 딸수있음
		
		그리고 config에
			.contexts[].context.user을 서비스계정이름으로 바꾸고
			.users[].name필드값으로 서비스계정이름으로 바꾸고
			.users[].user.token에 시크릿에서 딴 토큰 넣어주면됨
		
2.권한 관리
	쿠버네티스 클러스터의 api에 접근하려면 먼저 접근할수 있는 사용자인지부터 인증을 받아야함
	인증후엔 사용자가 접근하려는 api를 사용할 권한이 있는지 확인한 후에 api를 사용할수 있음
	
	클러스터 하나를 여러명이 사용할때는 api나 네임스페이스별로 권한을구분해서 권한이 있는곳에만 접근하게 할수있고,
	특정자원의 읽기권한만 추가해서 읽기만 할수있게 할수있음
	
	보통은 역할기반권한관리를 사용함
	사용자와 역할을 별개로 선언한후,사용자에 역할을 매핑시켜서 권한을 부여하는식
	
	1.롤
		롤은 특정 api나 자원사용권한들을 명시해둔 규칙집합임
		롤은 일반롤과 클러스터롤 두가지가 있음
		
		일반롤은 해당 롤이 속한 네임스페이스에만 적용됨
		추가로 네임스페이스에 한정되지 않은 자원과 api들의 사용권한을 설정할수있음
		노드사용권한이나 헬스체크용 url인 '/healthz'같은 엔드포인트사용권한도 관리함
		
		롤 yaml은
			kind:Role
			apiversion:api버전 rbac.authorization.k8s.io/v1
			metadata:
				namespace:네임스페이스이름
				name:롤이름
			rules:
				-apiGroups:['사용api'] 비어있으면 전체사용
				 resources:['사용자원(pod같은)']
				 verbs:['get','list'같은 사용가능한 동작지정]
				 
		metadata.namespace는 이 롤이 속한 네임스페이스를 적고(여기만적용됨)
		metadata.name은 이 롤의 이름
		rules는 구체적으로 규칙을 적는곳
			apiGroups는 롤이 사용할 api그룹을 설정함
			resources는 어떤 자원에 접근할수있는지 명시함
			verbs는 어떤 동작을 할수 있는지 설정함
			
			여기서 verv에서 설정할수있는값은
				Create:자원생성
				Get:개별자원조회
				List:여러자원조회
				Update:기존자원내용 전체 업데이트
				Patch:기존자원중 일부내용 변경
				Delete:개별자원 삭제
				deletecollection:여러자원삭제
			가 있음
			
		이거도 똑같이 apply로 적용하면됨
		
		그리고 전체파드가 아니라 특정파드에만 규칙을 설정할수도있는데
			rules:
				resourceNames:['파드이름']
		을 하면 개별값(겟,델리트,업데이트,패치)를 건드리는건 저 파드만 할수있고,
		전체값(리스트,크리에이트,딜리트컬렉션)을 건드리는건 저 옵션에 영향을받지않고 할수있음
		
	2.클러스터롤
		이건 특정 네임스페이스권한이 아닌 클러스터 전체 사용권한을 관리함
		사용법은 똑같은데,메타데이터에 네임스페이스지정이 없다는거정도만 다름
		
		그외 특이점은,
		.aggregationRule로 다른 클러스터롤을 조합해 사용할수 있음
		
			kind:ClusterRole
			apiversion:api버전
			metadata:
				name:이름
			aggregationRule:
				clusterRoleSelectors:
					-matchLabels:
						레이블키:레이블밸류
			rules:[]
			
		이렇게 aggregationRule로 레이블로 서치하고 가져와서 거기있는 룰을 적용시킬수있음
		그래서 룰은 비워둬도 되고 추가설정해도됨
		
		그리고 클러스터롤은 자원이 아니라 url식으로도 규칙을 설정할수 있음
		rules[]의 하위필드로 .nonResourceURLs을 설정하는것
			nonResourceURLs:['/healthcheck','/metrics/*']
		이런식으로 할수있음
		그리고 이렇게설정하면 verbs필드값은 get과 post만 설정할수있음
		
		
	3.롤바인딩
		롤바인딩은 롤과 사용자를 바인딩하는 역할임
		사용자가 어떤 롤을 사용할지설정함
		이거도 롤바인딩과 클러스터롤바인딩으로 구분되고,롤바인딩은 네임스페이스에,클러스터롤바인딩은 클러스터 전체에 적용된다는거도 같음
		
		롤바인딩의 yaml은
			kind:RoleBinding
			apiversion:api버전
			metadata:
				name:이름1
				namespace:네임스페이스이름1
			subjects:
				-kind:ServiceAccount
				 name:서비스어카운트이름1
				 apiGroup:''
			roleRef:
				kind:Role
				name:롤이름
				apiGroup:rbac.authorization.k8s.io
				
		메타데이터 연결하는건똑같고
		subjects필드는 어떤 유형의 계정과 연결하는지 설정함
		여기선 만들어둔 서비스어카운트와 연결하고,
		apiGroup가 ''인건 코어api그룹으로 설정했다는것,v1과 같음
		
		roleref는 이미 만들어둔 롤에 연결하는것
		카인드와 네임으로 만들어둔거에 연결하고
		여기서 apiGroup은 rbac api를뜻하는 rbac.authorization.k8s.io를 설정
		
	4.클러스터롤바인딩
		이거도 거의 똑같이 쓰면됨
		클러스터롤바인딩의 yaml은
			kind:ClusterRoleBinding
			apiversion:api버전
			metadata:
				name:이름1
				
			subjects:
				-kind:ServiceAccount
				 name:서비스어카운트이름1
				 namespace:네임스페이스이름1
				 apiGroup:''
			roleRef:
				kind:Role
				name:롤이름
				apiGroup:rbac.authorization.k8s.io
		거의 똑같은데,알아둬야할건 subjects의 kind에서 선택할수있는건 User,Group,ServiceAccount3개가 있는데,
		여기서 user과 serviceAccount는 네임스페이스 정보가 필요하고
		(롤바인딩은 어짜피 네임스페이스하나적용이니까 위에있는거 가져다쓰면되는데 클러스터롤바인딩은 위에 없으니까 여기적어줘야함),
		group는 클러스터전체에 사용할수있어서 필요없음
		클러스터 관련설정이니까 메타데이터밑에 네임스페이스가없고 서브젝트하위에 있음
		서비스계정은 네임스페이스에 한정되어있으므로 서브젝트의 네임스페이스필드값으로는 어떤 네임스페이스에 속한계정인지 명시함
		
				
	5.정리
		롤바인딩은 사용자와 롤을 묶어서 특정네임스페이스에 권한을 할당하고
		클러스터롤바인딩은 사용자와 클러스터롤을 묶어서 클러스터에 권한을 할당함
		
		kubectl config 로 사용자관련정보를 설정할수도있음(파일에서 직접바꾸는거랑 같음) 
		
		그리고 룰에 '*'하면 모든권한 다 여는거임




14.데이터 저장(볼륨)
1.볼륨
	컨테이너는 기본적으로 상태가 없는 앱 컨테이너를 사용함
	상태가 없다는건 노드에 장애가 발생해서 컨테이너를 새로실행했을때 자유롭게 옮길수 있다는뜻
	
	하지만 앱의 특성에 따라 컨테이너에 문제가 생겨도 데이터를 보존해야할때가 있음
	대표적수단으로 데이터를 파일로 저장하는 젠킨스가있고,mysql같은 db도 컨테이너가 뻗어도 데이터가 사라지면 안됨
	이럴때 볼륨을 사용함
	
	볼륨을 사용하면 컨테이너를 재시작해도 데이터를 유지함
	퍼시스턴트 볼륨을 사용하면 데이터를 저장했던 노드가 아닌 다른노드에서 컨테이너를 재시작해도 데이터를 저장한 볼륨 그대로 사용할수 있게 함

	볼륨이나 퍼시스턴트 볼륨을 사용하면 단순히 서버하나에서 데이터를 저장해 사용하는거보다 안정적으로 서비스를 운영할수 있음
	
	aws,azure,gce로 시작하는 볼륨은 클라우드서비스에서 제공하는 볼륨서비스
	
	볼륨 관련 필드중 .spec.container.volumeMounts.mountPropagation은
	파드 하나 안에 있는 컨테이너들끼리,또는 같은 노드에 실행된 파드들끼리 볼륨을 공유할지 말지를 설정함
	필드값으로
		None:공유안함,기본값
		HostToContainer:호스트에서 해당 볼륨 하위에 마운트된 다른디렉토리들도 볼수있음(컨테이너공유)
		Bidrectional:하위도 볼수있고,호스트안 모든 컨테이너나 파드에서 공유
	가 있음
	
	1.emptyDir
		emptyDir은 파드가 실행되는 호스트의 디스크를 임시로 볼륨으로 할당해서 사용하는 방법
		이건 파드가 사라지면 볼륨의 데이터도 같이 사라지기때문에,연산할때 디스크와 메모리를 같이쓸때나,
		딥러닝처럼 연산이 오래걸릴때 중간저장용으로 써야할때 사용함
		컨테이너가 뻗어도 파드는 살아있으니까 저장해둔 데이터를 계속 이용할수있음
		
		emptyDir사용법은
			spec:
				container:
					...
					volumeMounts:
						-mountPath:/emptydir
						 name:볼륨이름
				volumes:
					-name:볼륨이름
					 emptyDir:{}
		이렇게 씀
		먼저 spec.volumes에 선언하고,
		그걸 컨테이너 설정에서 불러와서 사용함
		컨테이너 설정에서 컨테이너의 디렉토리를 지정해서 볼륨을 마운트하면됨
		
	2.hostPath
		hostPath는 파드가 실행된 호스트의 파일이나 디렉토리를 파드에 마운트함 
		emptyDir이 임시 디렉토리를 마운트하는거면,이건 호스트에 있는 실제 파일이나 디렉토리를 마운트함
		이건 파드가 재시작해도 호스트에 데이터가 남고,호스트의 중요 디렉토리를 컨테이너에 마운트해서 사용할수 있음
		도커 시스템용 디렉토리를 컨테이너에서 사용하거나,시스템용 디렉토리를 마운트해서 시스템을 모니터링할때도 사용할수있음
		
		사용법은
			spec:
				container:
					...
					volumeMounts:
						-mountPath:/경로
						 name:볼륨이름
				volumes:
					-name:볼륨이름
					 hostPath:
						path:/tmp
						type:Directory
		이렇게하면됨
		볼륨마운트의 경로는 파드내에서 어디를 사용할건지(어디에 저장하고 읽고쓰고할건지,파드의 디렉토리임)고
		
		볼륨의 hostPath는 호스트에서 어디를 쓸건지임
		즉 호스트와 파드의 디렉토리를 바인딩시키고 서로 똑같이 바뀌게해서 저장하고 읽고하는 느낌
		
		type는 설정한 경로가 어떤 타입인지임
			'':볼륨을 마운트하기전 아무것도 확인하지않음
			DirectoryOrCreate:설정한경로에 디렉토리가 없으면 빈 디렉토리를 만듬
			Directory:설정한 경로에 디렉토리가 없으면 파드생성이 안됨
			FileOrCreate:경로에 파일이없으면 빈파일을 만듬
			File:경로에 파일이없으면  파드생성이 안됨
			Socket:경로에 유닉스 소켓파일이 있어야함
			CharDevice:경로에 문자디바이스가 있는지 확인
			BlockDevice:경로에 블록 디바이스가 있는지 확인
		이런식으로 경로에 파일이나 디렉토리가 있는지 확인하고,생성하거나 에러를 칠수있음
		
	3.nfs
		nfs는 기존에 사용하는 nfs서버를 이용해서 파드에 마운트하는것
		nfs클라이언트 역할이라고 생각하면됨
		
		이건 여러파드에서 볼륨 하나를 공유해 읽고쓰기를 동시에 할때도 사용함
		이떄는 파드 하나에 안정성이 높은 외부스토리지를 볼륨으로 설정하고,해당파드에 nfs서버를 설정함
		다른 파드는 해당 파드의 nfs서버를 nfs볼륨으로 마운트함
		
		즉 서버는 hostPath로 볼륨을 만들고,클라이언트는 nfs로 볼륨을 만들어서 마운트하면됨
		이건 고성능이 필요한 읽기/쓰기면 쓰기힘든데,간단히 파일공유같은게 필요하면 편하게만들수있음
		
		서버를 만드는건
			spec:
				container:
					...
					ports:
						-name:nfs
						 containerPort:포트번호1
						-name:mountd
						 containerPort:포트번호2
						-name:rpcbind
						 containerPort:포트번호3
					securityContext:
						privileged:true
						
						
					volumeMounts:
						-mountPath:/경로
						 name:볼륨이름
				volumes:
					-name:볼륨이름
					 hostPath:
						path:/tmp
						type:Directory
		이런식으로 하면됨
		포트의 1번쨰 nfs는 이포트로 통신하는 포트임
		
		포트의 2번째 mountd는 nfs서버에서 사용하는 프로세스임
		요청이 왔을때 지정한 디렉토리로 볼륨을 마운트하는 mountd데몬이 사용하는 포트를 지정함
		
		포트의 3번째 rpcbind는 이것도 nfs서버에서 사용하는 프로세스임
		시스템에서 rpc서비스를 관리할 rpcbind데몬이 사용하는 포트를 지정함
		
		securityContext는 컨테이너의 보안설정을 함
		여기서는 컨테이너가 실행중인 호스트장치에 접근권한을 설정하는 privileged를 true로 줘서 모든호스트에 접근가능하게 함
		
		mountpath는 볼륨을 마운트할 디렉토리 경로
		
		클라이언트구성은
			spec:
				container:
					...
					volumeMounts:
						-mountPath:/경로 nfs볼륨을 마운트할 디렉토리
						 name:볼륨이름
				volumes:
					-name:볼륨이름
					 nfs:
						server:nfs서버의 ip
						path:/
		mountPath에는 마운트할 디렉토리를 넣고
		nfs.server에 연결할 파드의 ip를 주면됨
		
2.퍼시스턴트 볼륨과 퍼시스턴트 볼륨 클레임		
	쿠버네티스에서 볼륨을 사용하는 구조는 pv라고 하는 퍼시스턴트볼륨과 pvc라고 하는 퍼시스턴트 볼륨 클레임 2개로 분리되어있음
	
	pv는 볼륨 자체를 뜻함,클러스터 안에서는 자원으로 다루고,파드와는 별개로 관리되고 별도의 생명주기가 있음
	pvc는 사용자가 pv에 하는 요청임,사용하고싶은 용량,읽기쓰기모드등을 정해서 요청함
	쿠버네티스는 볼륨을 파드에 직접 할당하는 식이 아니라 pvc를 두어 파드와 파드가 사용할 스토리지를 분리했음
	이런구조는 파드 각각의 상황에 맞게 다양한 스토리지를 사용할수 있게 함
	
	클라우드 서비스를 사용할땐 봉인이 사용하는 클라우드 서비스에서 제공해주는 볼륨서비스를 사용할수도 있고,직접 구축한 스토리지를 사용할수도 있음
	그렇지만 파드에 직접 연결하는게 아니라 pvc를 거쳐서 사용하므로 파드가 뭘쓰는지 신경안써도됨

	pv와 pvc의 생명주기는
		프로비저닝-바인딩-사용-반환
	순임
	1.프로비저닝
		프로비저닝은 pv를 만드는거임
		프로비저닝 방법에는 pv를 미리 만들어두고 사용하는 정적방법과,요청이 있을때마다 pv를 만드는 동적방법이 있음
		
		정적으로 프로비저닝할땐 클러스터 관리자가 미리 적정용량의 pv를 만들어두고 요청이 있으면 미리 만들어둔 pv를 할당함
		사용할수있는 스토리지 용량에 제한이 있을때 유용함
		미리 만들어둔 pv의 용량이 100gb면 150gb를 사용하려고 하면 요청이 실패함
		스토리지에 용량이 남아있어도 pv가 없으면 실패함
		
		동적으로 프로비저닝할때는 사용자가 pvc를 거쳐서 pv를 요청했을때 생성해 제공함
		쿠버네티스 클러스터에 사용할 1tb스토리지가 있다면,사용자가 필요할때 원하는 용량만큼을 생성해서 사용할수 있음
		정적 프로비저닝과 달리 필요하면 200gb pv도 만들어서 사용할수 있음
		pvc는 동적 프로비저닝할때 여러 스토리지중 원하는 스토리지를 정의하는 스토리지 클래스로 pv를 생성함
	
	2.바인딩
		바인딩은 프로비저닝으로 만든 pv를 pvc와 연결하는 단계임
		pvc에서 원하는 스토리지의 용량과 접근방법을 명시해서 요청하면 거기에 맞는 pv가 할당됨
		이때 pvc에서 원하는 pv가 없다면 요청은실패하는데,실패했다고 요청을 끝내진않고 대기하다가 원하는 pv가 생기면 그때 바인딩됨
		pv와 pvc의 매핑은 1대1관계임
		pvc하나가 pv여러개에 바인딩할수없음
		
	3.사용
		pvc는 파드에 설정되고 파드는 pvc를 볼륨으로 인식해서 사용함
		할당된 pvc는 파드를 유지하는동안 계속 사용하며 시스템에서 임의로 삭제할수 없음
		이기능을 사용중인 스토리지오브젝트 보호 라고 함
		사용중인 스토리지를 임의로 삭제하면 치명적결과가 발생할수있기때문
		
		파드가 사용중인 pvc를 삭제하려고 하면 상태가 terminating지만 해당 pvc를 사용중인 파드가 남아있으면 삭제되지않고 남아있음
		
	4.반환
		사용이끝난 pvc는 삭제되고 pvc를 사용하던 pv를 초기화하는 과정을 거치는데,이걸 반환이라고 함
		이때 초기화정책은 3개가있음
			Retain:pv를 그대로 보존,pvc가삭제되면 사용중이던 pv는 해제상태가 되고 다른 pvc가 재사용할수없음
				   단순히 사용해제상태라서 pv안에 있는 데이터는 그대로있고,재사용하려면
					   1.pv삭제(pv가 외부스토리지와 연결되었다면 pv는 삭제되어도 외부스토리지의 볼륨은 그대로 남아있음)
					   2.스토리지의 남은데이터직접정리
					   3.남은 스토리지의 볼륨을 삭제하거나 재사용하려면 해당볼륨을 이용하는 pv 다시만듬
					   
			Delete:pv를 삭제하고 연결된 외부스토리지쪽 볼륨도 삭제함,프로비저닝할때 동적할당으로 생성된 pv들은 기본값이 delete임
			Recycle:pv의 데이터를 삭제하고 다시 새로운 pvc에서 pv를 사용할수 있게함,근데 이건 사라질예정이고 동적볼륨할당을 기본사용하는게 좋대
			
3.퍼시스턴트 볼륨 템플릿
	퍼시스턴트 볼륨 템플릿은
		apiversion:api버전
		kind:PersistentVolume
		metadata:
			name:pv-hostpath
		spec:
			capcity:
				storage:2gi 용량
			volumeMode:Filesystem
			accessModes:
				-ReadWriteOnce
			storageClassName:manual
			persistentVolumeReclaimPolicy:Delete
			hostPath:
				path:/경로
				
		이렇게하면됨
		
		capcity.storage는 용량설정이고
		
		volumeMode는 볼륨을 사용하는 형식인데,기본값인 Filesystem은 그냥 쓰던식이고,
		이걸 raw로 하면 로우 블록 디바이스형식으로 설정해서할수있다고함,먼소린지모르겠음 필요한일있을까
		
		spec.accessModes는 볼륨의 읽고쓰기옵션을 설정함
		값은
			ReadWriteOnce:노드하나에만 볼륨을 읽기/쓰기하도록 마운트
			ReadOnlyMany:여러노드에서 읽기전용으로 마운트할수있음
			ReadWriteMany:여러노드에서 읽기/쓰기가능하게 마운트할수있음
			
		spec.storageClassName은 스토리지 클래스를 설정함
		특정 스토리지 클래스가 있는 pv는 해당 스토리지 클래스에 맞는 pvc와 연결함(레이블같은거임)
		이값이 없으면 없는거끼리만 연결됨
		
		spec.persistentVolumeReclaimPolicy는 pv가 해제되었을때의 초기화옵션(앞에서한거)
		
		spec.hostPath는 해당pv의 볼륨플러그인을 명시함(즉 hostPath말고 다른거써도됨 자기가 쓸 플러그인)
		하위에는 마운트시킬 로컬서버의 경로설정
		
		이거외에도 spec.mountOptions필드가 있는데,여기로 볼륨을 마운트할때 추가옵션을 설정할수있음
		추가옵션이 잘못되면 마운트할수 없으니 주의
		
		그리고 다하고 확인했을때
		status가
			available:정상설정
			bound:특정pvc에 연결됨
			Released:pvc는 삭제되었고 pv는 초기화되지않음
			Failed:자동초기화실패
		가 있음
		
		
		
4.퍼시스턴트 볼륨 클레임 템플릿
	퍼시스턴트볼륨 클레임 템플릿은
		apiversion:api버전
		kind:PersistentVolumeClaim
		metadata:
			name:이름1
		spec:
			accessModes:
				-ReadWriteOnce
			volumeMode:Filesystem
			resources:
				requests:
					storage:1gi
			storageClassName:manual
			
	이렇게씀
	대부분은 퍼시스턴트볼륨과 같음
	저 옵션들이 같은볼륨에 가서 연결됨
	requests.storage는 용량인데,저거보다 작은볼륨밖에 없으면 무한대기함
	

5.레이블로 pvc와 pv 연결하기
	퍼시스턴트볼륨은 쿠버네티스 안에서 사용하는 자원이고,퍼시스턴트볼륨클레임은 해당 자원을 사용하겠다고 요청하는거니까
	파드와 서비스처럼 레이블을 사용할수 있음
		apiversion:api버전
		kind:PersistentVolume
		metadata:
			name:pv-hostpath
			labels:
				레이블키:레이블값
		spec:
			...
	과
		apiversion:api버전
		kind:PersistentVolumeClaim
		metadata:
			name:이름1
		spec:
			...
			selector:
				matchLabels:
					레이블키:레이블값
	으로 레이블을 넣고 셀렉터로 서치하는식으로 쓸수있음
	그리고 셀렉터.matchLabels대신 selector.matchExpressions[]으로 레이블조건을 설정할수도있음
	

6.파드에서 pvc 볼륨으로 사용하기
	파드에서 pvc를 사용할때는
		kind:디플로이먼트
		metadata:
			...
		spec:
			...
			spec:
				containers:
					...
					volumeMounts:
						-mountPath:'/경로'
						 name:볼륨이름
				volumes:
					-name:볼륨이름
					 persistentVolumeClaim:
						claimName:연결할pvc이름
		
	이렇게하면됨
	볼륨에서 연결할pvc로 연결하고
	볼륨마운트에서 볼륨이랑 연결하고 경로정해서 마운트하면됨


7.pvc크기 늘리기
	pvc의 크기를 늘리는건,사용중인 볼륨플러그인에 따라 가능할수있음
	이기능을 사용하려면 spec.storageClassName.allowVolumeExpansion이 true여야함
	
	크기를 늘릴떈 spec.resources.requests.storage필드에 더 높은 용량을 설정한 후 클러스터에 적용함
	파일시스템이 xfs,ext3,ext4라면 파일시스템이 있어도 늘릴수있음
	이때 작업은 해당pvc를 사용하는 파드가 재시작하거나 새로운파드를 실행할때만 진행됨
	그래서 사용중인거 늘리는거 추가되었다는데 딴책에나오겠지
8.노드별 볼륨갯수제한
	쿠버네티스에선 노드 하나에 설정할수 있는 볼륨 개수에 제한을 둠
	kube-scheduler의 KUBE_MAX_PD_VOLS환경변수를 이용해서 설정할수 있음
	그리고 클라우드 서비스마다 제한이 있음
	
	



15.클러스터 네트워킹 구성
1.파드네트워킹
	쿠버네티스에서는 파드마다 각각 ip를 가지고,여러노드를 사용해서 클러스터를 구성한 후 노드별로 실행한 파드들이 ip를 이용해 서로 통신함
	1.도커컨테이너의 네트워킹
		도커 브리지타입 네트워크는 호스트안에 도커0라는 브리지를 추가해서 컨테이너와 호스트 사이를 연결함
		
		여기서 도커0의 네임스페이스를 호스트 네트워크 네임스페이스 혹은 디폴트 네트워크 네임스페이스라고 함
		호스트의 기본네트워크는 여기서 만들어지고 관리됨
		그리고 컨테이너의 네임스페이스는 컨테이너 네트워크 네임스페이스라고 하고,
		컨테이너를 생성할때마다 만들어지고 컨테이너마다 별도의 네트워크임
		
		네트워크 네임스페이스는 서로 연결되기 전에는 독립적으로 동작함
		도커는 네트워크 네임스페이스를 각각 연결시켜주려고 베스라는 가상장치를 사용함
		베스는 한쪽끝은 컨테이너,한쪽끝은 호스트의 브리지(기본은 도커0브리지)에 연결해서 호스트 네임스페이스와 컨테이너네트워크 사이를 통신함
		도커브리지타입에서는 따로 설정안하면 172.17.0.1/24사이의 ip를 사용함
		
		이거외에도 오버레이,맥브이랜,호스트,링크등의 타입을 사용함
		각 타입별 설명은
			브리지:호스트에 브리지를 만들고 컨테이너와 호스트는 베스로 연결함
			오버레이:여러호스트가 있을때 각 호스트에 있는 컨테이너 네트워크를 오버레이네트워크로 연결시킴
			맥브이랜:이더넷 하나에서 여러 가상mac주소를 할당함,이거로 컨테이너에 mac주소와 ip주소를 할당함,스위치를안써서 브리지보다 20%빠름
			호스트:컨테이너네트워크 네임스페이스 대신 호스트의 네트워크 네임스페이스를 직접 사용함,
				 호스트ip를 직접쓰므로 컨테이너의 네트워크서비스포트를 호스트에서도 확인할수있음
			링크:컨테이너별로 네트워크네임스페이스를 만들지않고,이미 생성된(pause)네트워크 네임스페이스에 컨테이너를 연결함
			   쿠버네티스 파드네트워크의 기본
	
	2.파드네트워킹
		쿠버네티스는 도커와 단리 파드단위로 컨테이너를 관리함
		파드는 쿠버네티스에서 생성한 pause라는 컨테이너와 사용자가 생상한 컨테이너들의 그룹을 말함
		그래서 파드 하나에 속한 컨테이너는 다 같은ip를 가짐
		
		파드는 베스하나를 파드내에있는 전체컨테이너가 공유하고 그건 pause컨테이너가 만듬(그래서 ip가 같음)
		그래서 pause가 파드의 기반인프라컨테이너고,쿠버네티스가 생성하고 관리하는것
		
		파드의 veth0는 pause컨테이너 네트워크 네임스페이스에 속한 장치임
		같은 파드 안 다른 컨테이너들은 이 pause컨테이너 네트워크 네임스페이스를 공유해서 사용함
		그래서 컨테이너가 재시작해도 ip가 안변하는데 pause에 문제가생기면 ip가 변하는것
		그러니까 pause에 문제가 생기면 다른컨테이너가 정상이더라도 네트워크 통신을 할수 없음
		
		파드 하나에 여러 컨테이너가 생겨도 컨테이너 각각에 할당된 ip는 변하지않고,같은 파드안에선 로컬호스트로 서로 통신할수있음
		
		그래서 파드에 접근할땐 파드의 ip를 이용하고,파드안에서 컨테이너 구분은 포트를 이용해서 함
		그리고 파드안에 있는 컨테이너들은 네트워크 네임스페이스만 공유하고 나머지 네임스페이스들은 공유하지 않음
		
		여기까진 노드 하나에서의 파드네트워크고,노드가 여러개면 단순히생각하면
		라우터와 통신하는 제일 밖 eth0부분의 ip만 다르고 내부는 똑같이 생기게되는데
		
		쿠버네티스에는 파드 각각이 고유의 ip를 가지게 구성되기때문에
		각각 호스트마다 서브넷팅을 해서 대역폭을 잘라준다음 그안에서 파드들에게 분배하는식으로 해서 ip를 전부 다 다르게함
		
		이걸 하려면 호스트 네트워크 네임스페이스의 각종 네트워크 기능(iptables,커널라우팅,터널링,브리지)들을 사용해야하는데,이걸 cni라고함
		이건 표준이기도 하고 플러그인 이름이기도 한데,쿠버네티스는 파드와 호스트인터페이스를 연결하는 부분을 별도의 모듈로 분리해뒀기때문에
		필요하면 다른 네트워크 플러그인을 같이 사용할수 있음
		
		그리고 외부라우터에 각 호스트사이 라우팅을 정의해둘수도 있는데,이러면 기존네트워크랑 똑같이쓸수있지만,
		컨테이너 라우팅까지 고려해야해서 생각할게 많아짐
		그래서 기존환경을 고려해서 cni를 선택해야함


2.쿠버네티스 서비스 네트워킹
	쿠버네티스에서 실제 서비스용으로 파드와 파드끼리 통신만을 할때는 잘 없고,보통 여러파드를 실행하고 앞에 서비스를 두고 사용함
	그래서 실제 클러스터안에서 통신할때는 쿠버네티스 서비스의 ip를 거치도록 함
	
	쿠버네티스는 파드용 cidr(클래스없는도메인사이 라우팅)과 서비스용 cidr을 별도로 지정함
	파드용 cidr은 마스터용 컴포넌트를 실행할때 --cluster-cidr 옵션을 이용해서 지정하고,
	서비스용 cidr은 --service-cluster-ip-range옵션으로 지정함
	옵션이 다른만큼 파드와 서비스는 서로 다른 ip대역을 사용함
	
	쿠버네티스에서 nodeport서비스를 생성하면 쿠버네티스의 엔드포인트가 있는 호스트에 nat테이블이 생성됨
	그리거 사용자가 지정한 서비스용 ip중 하나의 파드의 ip와 연결시켜주는데 이걸 kubeproxy가 담당함
	
	외부에서 서비스ip에 접근하면 호스트까지는 라우팅으로 도달하고,그다음은 nat를 이용해 파드로 접근하면됨
	쿠버네티스의 nodeport 타입 서비스에서는 슬레이브 어디로 접근하든 지정된 파드로 연결됨
	지정된 파드가 없는 슬레이브로 접근하면 다시 지정된 파드가 있는 호스트로 패킷을 전달하려고 dnat(목적지주소를바꿈)함
	이것도 kube-proxy가 담당함
	
	nat영역의 설정은 kube-proxy가 담당함
	프록시가 apiserver를 지켜보다가 파드에 변경사항이 발생하면 설정된 서비스(프런트,백엔드)각각에 해당하는 nat규칙을 업데이트 함
	그래서 클러스터 안 모든 노드에는 proxy가 설치되어 있음

3.네트워크 플러그인
	상용클라우드서비스를 쓸때는 클라우드서비스에서 제공하는 기능을 쓰면되지만
	직접구성한다면 클러스터 네트워크도 직접 구성해야함
	1.플라넬
		플라넬은 레이어3네트워크를 구성할수있는 간단한방법을 제공함
		파드의 ip를 관리하는 별도의 데이터베이스가 필요하지않음
		오버레이타입의 구성으로 vxlan을 지원함
		네트워크를 구성하긴 쉬운데 쿠버네티스에서 설정하려는 네트워크 정책등을 지원하진않음
	2.칼리코
		얘가 보통 디폴트값으로사용됨
		얘는 bgp기반의 레이어3네트워크를 구성할수 있음
		보유한 네트워크장비들이 bgp를 지원하면 오버레이타입이 아닌 네이티브 클러스터네트워크를 구성할수있는데
		그러면 네트워크 성능이 좋아짐
	3.실리엄
		얘는 리눅스커널에 포함된 bpf기능을 이용함
		그래서 시스템을 재시작 할 필요없이 실행중에 필요한기능을 커널에 추가할수 있음
		얘는 bpf의 구현중 하나인 xdp를 이용해 시스템 실행중 네트워크 경로를 조정함
		커널수준에서 네트워크 경로를 조정할수있어서 같은노드의 컨테이너들이라면 통신성능이 많이 좋아짐
		원래는 같은노드컨테이너라도 커널을 거쳐서 네트워크카드까지 갔다가 커널을거쳐서 다른컨테이너에 가야하는데
		얘는 커널까지 갔을때 같은노드인지 확인하고 바로 전달하기때문
		그래서 얘를 쓸때는 cpu,메모리등의 사양이 좋은 노드들로 클러스터를 구성하고,연관성있는 파드들을 같은 노드에 실행하게 설정함


16.쿠버네티스 dns
	쿠버네티스에서는 클러스터 안에서만 사용하는 dns를 설정할수 있음
	그러면 파드사이에 통신할때 ip가 아닌 도메인을 사용할수있게됨
	그러면 파드에 문제가 생겨서 다른노드에서 재생성되어도 수정할필요가 없어지고,재생성할때 자동으로 변경된파드의 ip가 도메인에 등록됨
	dns를 클라이언트나 api게이트웨이가 호출할 서비스를 찾는 서비스 디스커버리(파드삭제후 재생성같은) 용도로 사용할수도 있음
	
1.클러스터안에서 도메인 사용
	쿠버네티스 안에서 사용하는 내부도메인은 서비스와 파드를 대상으로 사용하며,일정패턴이 있음
	특정 서비스에 접근하는 도메인은 서비스이름.네임스페이스이름.svc.cluster.local로 구성됨
	특정 파드에 접근하는 도메인은 파드ip주소.네임스페이스이름.pod.cluster.local로 구성되는데
	아이피주소를 쓸거면 의미가없으니까
	파드의 템플릿에
		spec:
			template:
				spec:
					hostname:호스트이름
					subdomain:서브도메인이름
	으로 해주면
	호스트이름.서브도메인이름.네임스페이스이름.svc.cluster.local로 접근할수있게됨
	여기서 주의할점은 .pod가 아니라 .svc임
	서비스에서 저쪽으로 연결해주는걸 처리해주기때문
	
2.dns질의구조
	dns도 클러스터내에서 파드로 실행됨
	그래서 파드마다 안에서 도메인이름을 어떤순서로 질의할지 설정할수있음
	이때 .spec.dnspolicy필드를 사용함
	필드값은
		default:파드가 실행중인 노드의 dns 설정을 불러와서 사용함
		clusterfirst:클러스터 안 도메인 형식과 일치하지 않는 www.abc.com같은걸 쓸때,외부dns인 업스트림dns에 도메인이름을 질의함
		clusterfirstwithhostnet:파드를 호스트모드로 사용할때 설정하는 hostnetwork옵션,호스트모드일때 반드시사용해야함
		None:파드가 쿠버네티스 클러스터 안 dns설정을 무시함,이때는 .spec.dnsconfig로 별도의 dns설정을 해야함
		
	1.kube-dns의 질의구조
		kube-dns파드는 kubedns,dnsmasq,sidecar컨테이너로 구성됨
		
		kubedns컨테이너는 마스터를 바라보다가 서비스나 엔드포인트의 변경사항이 있으면 메모리에 저장중인 dns데이터를 변경하고,
		
		sidecar컨테이너는 kubedns와 dnsmasq컨테이너에 헬스체크를 함,
		다른파드에서 도메인이름을 조회하면 kube-dns파드에 질의해서 ip를확인함,이때 kube-dns파드 안 dns캐시인 dnsmasq컨테이너로 질의함
		
		dnsmasq컨테이너는 kube-dns파드에 도메인이름을 질의했을때 결과가없으면 사용자정의dns에 질의하고,
		거기도 없으면 업스트림dns(8.8.8.8같은 머기업dns)에 질의함
		
	2.coredns의 질의구조
		coredns는 모듈형식이며 kube-dns와 다르게 coredns라는 컨테이너 하나만 있고,
		플러그인으로 새 기능을 추가할수 있는 유연한 구조임
		corefile라는 coredns 자체의 설정파일형식에 맞춰서 dns를 설정함
		
		쿠버네티스 안에서는 kube-system네임스페이스에 coredns라는 컨피그맵으로 corefile를 관리함
		
			kubectl describe configmap coredns -n kube-system
		으로 확인할수있음
		
		여기서 .:53의 하위항목은 이 서버의 dns정보임
		맨앞.은 도메인의 루트영역을 뜻하고,53은 포트정보임
		dns의 기본포트가 53임
		
		.:53의 중괄호로 묶인부분에서는 사용할 플러그인을 명시함
		coredns는 다양한 플러그인을 사용할수 있으며,특징은 dns쿼리를 받았을때 중괄호 안에 나열된 플러그인 순서대로 쿼리를 처리함
		
		일반적인 플러그인으로
			errors:표준출력으로 에러로그를 남김
			health:로컬호스트:8080/health로 dns헬스체크할수있음
			kubernetes:쿠버네티스의 서비스나 파드의 ip로 오는 쿼리에 응답함,여기에 하위설정이 있음
				pods insecure:kube-dns와의 호환성설정임,
								이걸 pods verified로 변경하면 같은네임스페이스에 속한 파드끼리만 a레코드에 대한 dns쿼리에 응답할수있음
				upstream:외부서비스나 cname등의 외부호스트를 가리키는 서비스용 dns쿼리에 응답하는설정
				fallthrouth:도메인찾기에 실패했을때 어떻게 동작할지 설정,없으면 nxdomain에러가 뜨는데,이게있으면 여기설정따라처리함
							in-addr.arpa와 ip6.arpa를 설정하면 ipv4와 ipv6에 맞는값을 설정하고,
							리버스쿼리(ip주소로 도메인을 찾는 쿼리)를 할수있어짐
			prometheus:프로메테우스,프로메테우스형식의 메트릭정보를 제공함
			proxy:쿠버네티스 클러스터 도메인으로 설정되지않은 쿼리를 /etc/resolv.conf에 설정된 외부dns로 보내서 처리
			cache:dns쿼리의 캐시유지기간을 30초로 설정
			loop:순환참조구조가 있는지찾아서 coredns를 중지함
			reload:corefile이 변경됐는지 감지해서 자동으로 설정내용을 반영
				   컨피그맵을 수정해도 파드를 재시작하지않으면 컨피그맵의 변경사항이 반영되지않음
				   reload를 설정하면 컨피그맵을 변경했을때 파드를 재시작안해도 2분뒤에 변경사항을 반영함
			loadnalance:도메인에 설정된 레코드가 여러개있을때 라운드로빙으로 요청을 보낼수있게 레코드순서를 무작위로 섞음
						즉 입력순으로하면 맨앞이 부하가쏠릴테니까,랜덤으로 섞어서 부하를 평평하게 맞춤
		
		등이 있음
		
3.파드안에 dns 직접설정
	파드안에 dns를 직접 설정하려면
		spec:
		dnsPolicy:ClusterFirst  dns설정
		dnsConfig:
			nameservers:
				-8.8.8.8
			searches:
				-default.svc.cluster.local
				-example.com
			options:
				-name:이름1
				 value:값1
				-name:이름2
	이런식으로 하면됨
	앞에서 말했던거처럼 dnsPolicy가 None면 dnsConfig가 무조건있어야함
	
	.spec.dnsConfig에는 3가지 필드를 설정할수있음
		nameservers:파드에서 사용할 dns의 ip, dnsPolicy가 None일땐 필수로 1개이상 설정해야함,최대3개
		searches:dns를 검색할때 사용하는 기본 도메인이름(파드의 도메인),
				 여기에 svc.cluster.local이라고 설정하면 a.b.svc.cluster.local이라는 도메인을 사용할떄,
				 a.b까지만 입력해도 svc.cluster.local를 포함해서 검색함,최대 6개
		options:하위에 name과 value로 원하는 dns옵션을 설정할수있음,name이 있어야 value를 적을수있음 당연히
		
	여기 설정한값은 파드의 /etc/resolv.conf에 추가됨
	
				 
		
		
17.로깅과 모니터링
1.로깅
	클러스터환경에서 앱컨테이너를 운영할때 주의해야할것중 하나가 로그처리임
	컨테이너오케스트레이터 상황에서 로그를 수집할때 로그를 로컬디스크에 저장하는방식은 하면안되기때문
	
	보통 일반적인상황에서야 컴퓨터한대가 전체를 통괄하지만,쿠버네티스같은상황에선 파드들이 노드들을 옮겨다니기때문에 그런식으로하면 로그에 구멍이 나게됨
	그렇다고 모든노드를 확인하는것도 비합리적임
	그래서 모든 노드의 로그를 통합해서 뷰하는 식으로 데몬셋같은걸 써서 모니터링함
	
	1.파드로그 확인
		쿠버네티스의 kubectl은 개별 노드에 접근하지 않고 직접 파드의 로그를 확인할수 있음
		이때 kubectl logs -f 파드이름 을 사용함
		
		여기서 -f옵션은 실행중인 로그를 지속적으로 수집하는 테일링을 실햄함
		
	2.일래스틱서치로 로그를 수집한 후 모아서 보기
		로컬디스크에 로그를 저장하면 용량문제로 삭제한 예전 로그를 확인할수 없음
		트래픽이 많은서버는 로그양도 많으므로 엄청 빨리 사라져서 문제를 인식하고 로그확인하려고 서버에 접속했는데 로그가 삭제된 후일지도 모름
		그래서 이런 문제를 개선하려고 장비 각각에 저장한 로그를 한곳에 모아서 살펴보도록 서버를 구축함
		이럴때 사용하는 도구가 일래스틱서치나 키바나같은거임
		
		물론 퍼블릭클라우드(아마존같은)라면 서비스 각각에서 로그수집도구를 제공하지만,사용하지못한다면 직접 로그수집시스템을 구축해야함
		로그 수집기에서 수집할 로그를 저장하는건 일래트릭서치고,로그를 조회하는건 키바나임
		
		일래스틱서치 디플로이먼트 yaml은
			kind:deployment
			...
			spec:
				replicas:1
				...
				template:
					...
					spec:
						containers:
							-name:이름
							 image:elastic/elasticsearch:6.4.0
							 env:
								-name:discovery.type
								 value:'single-node'
							ports:
								-containerPort:9200 포트1
								-containerPort:9300 포트2
			kind:service
			...
			spec:
				ports:
					-name:elasticsearch-rest
					 nodePort:30920
					 port:9200 포트1
				 	 protocol:TCP
					 targetPort:9200 포트1
					-name:elasticsearch-nodecom
					 nodePort:30930
					 port:9300 포트2
					 protocol:TCP
					 targetPort:9300 포트2
				...
				
		이렇게 작성하면되는데
		
		노드 하나로만 구성할거기떄문에 replicas는 1로줬고
		디플로이먼트의 컨테이너이미지에서 일레스틱서치를 가져왔고
		env하위필드로 싱글노드로 설정해서 노드가 하나인 일래스틱서치를 실행하게 했고
		디플로이먼트의 포트 2개는 일래스틱서치가 사용할 포트 2개임
		
		서비스는 디플로이먼트에서 설정했던 포트2개를 각각 서비스포트로 매핑해서 연결할수있게 해둠
		
		그러면 서비스에서 노드포트로 설정해둔 포트중 rest포트로 접근하면(30920)일래스틱서치에 접근할수 있음
		
		이거만 가지고도 확인은 할수있는데,보기힘드니까 일래스틱서치전용 대시보드인 키바나를 사용함
		키바나의 디플로이먼트는
			kind:deployment
			...
			spec:
				replicas:1
				...
				template:
					spec:
						containers:
							-name:kibana
							 image:elastic/kibana:6.4.0
							 env:
								-name:SERVER_NAME
								 value:"서버이름.com"
								-name:ELASTICSEARCH_URL
								 value:'http://elasticsearch-svc.default.svc.cluster.local:9200'일래스틱서치포트
							ports:
								-containerPort:5601
			kind:service
			...
			spec:
				ports:
					-nodePort:30561
					 port:5601  키바나디플로이먼트포트
					 protocol:TCP
					 targetPort:5601 키바나디플로이먼트포트
				selector:
					app:kibana 키바나디플로이먼트이름
				type:NodePort
		이런식으로 작성하는데
		키바나는 일래스틱서치의 데이터를 검색하므로 env의 ELASTICSEARCH_URL에 일래스틱서치의 도메인을 지정해주고
		우리는 서비스포트인 로컬호스트:30561로 접근하면됨
					 
	3.클러스터레벨 로깅
		컨테이너가 비정상종료되거나 노드에 장애가 있어도 앱 컨테이너의 로그를 확인할수 있어야함
		그래서 컨테이너,파드,노드의 생명주기와 분리된 스토리지를 구축해야함
		
		이렇게 생명주기와 분리된 스토리지를 구축하는 아키텍쳐를 클러스터레벨로깅이라고 함
		쿠버네티스 자체에서는 클러스터레벨 로깅 도구를 제공하지는 않으므로,외부도구로 구축해야함
		
		로그가 어떻게 생성/관리되냐면
		
			컨테이너로그:
				컨테이너의 로그수집은 도커가 담당함
				앱 컨테이너가 표준입출력으로 로그를 출력한다면 도커는 표준스트림2개를 특정 로그 드라이버로 리다이렉트함
				도커는 다양한 로그드라이버를 지원하며,기본으로 json-file을 사용함
				docker ps로 생성되어있는 컨테이너의 id를 확인하고,docker inspect 컨테이너id로 런타임이 로그를 어떻게 다루는지 확인할수있음
				
				거기를 보면 logPath는 컨테이너관련 메타데이터를 저장한곳의 위치고,메타데이터도 포함됨
				(var/lib/docker/containers/컨테이너ID/컨테이너ID-json.log)
				logconfog는 최대파일갯수와 최대사이즈를 지정할수있음
				
				kubelet은 logpath에 있는 파일에 관해 링크를 생성하고 로그를 관리함
				로그수집기는 앞 링크들을 테일링해 로그내용을 수집하고,이름으로부터 필요한 메타데이터(파드,컨테이너,네임스페이스이름)를 얻음
				
				로그를 보면
				로그내용,표준스트림종류,시간정보를 json형식으로 출력함
				여기까지가 도커의 역할임
					
				그리고 로그가 노드의 스토리지를 과도하게 사용하는 문제가 있으면
				도커런타임에서 로그로테이트 관련설정을 확인해야함
				ps -ef|grep dockerd를 실행하고
				--log-opt max-size,--log-opt max-file을 확인하고 조정하면됨
			
			시스템컴포넌트로그:
				쿠버네티스의 시스템 구성요소중 일부(kubelet,docker등)는 컨테이너기반으로 동작하지 않음
				
				예를들어 systemd는 기본설정인 systemd-journald로 로그를 관리함
				그렇지 않은 상황에서는 /var/log/디렉터리에 로그를 남김
				이때 반드시 노드에 로그로테이트 관련 설정이 있어야함
				
				시스템 구성요소들의 로그관련동작은 컴포넌트별로 차이가 있음
				예를들어 kubelet이라면
				systemctl status kubelet.service 명령으로 확인할수 있음
				--logtostderr은 systemd-journald가 로그를 표준스트림인 stderr로 출력할것인지 나타내고
				--log-dir은 로그디렉토리를 지정함
				--v는 로그레벨을 나타냄(얼마나 빡빡하게저장할것인가)
	4.플루언트디를 이용한 로그수집
		플루언트디를 이용해서 쿠버네티스에서 발생한 로그를 일래스틱서치에 저장하면 좋음
		일래스틱서치용 플루언트디 yaml은
			apiversion:api버전
			kind:DaemonSet
			metadata:
				name:fluentd 플루언트디
				namespace:kube-system
				labels:
					k8s-app:fluentd-logging
					version:v1
					kubernetes.io/cluster-service:'true'
			spec:
				template:
					metadata:
						labels:
							k8s-app:fluentd-logging
							version:v1
							kubernetes.io/cluster-service:'true'
					spec:
					tolerations:
						-key:node-role.kubernetes.io/master
						 effect:NoSchedule
					containers:
						-name:fluentd
						 image:fluent/fluentd-kubernetes-daemonset:elasticsearch
						 env:
							-name:FLUENT_ELASTICSEARCH_HOST
							 value:'elasticsearch-svc.default.svc.cluster.local'
							-name:FLUENT_ELASTICSEARCH_PORT
							 value:'9200'
							-name:FLUENT_ELASTICSEARCH_SCHEME
							 value:'http'
							-name:FLUENT_UID
							 value:'0'
						resources:
							limits:
								memory:200mi
							requests:
								cpu:100m
								memory:200mi
						volumeMounts:
							-name:varlog 
							 mountPath:/var/log
							-name:varlibdockercontainers
							 mountPath:/var/lib/docker/containers
							 readOnly:true
						terminationGracePeriodSeconds:30
						volumes:
							-name:varlog
							 hostpath:
								path:/var/log
							-name:varlibdockercontainers
							 hostpath:
								path:/var/lib/docker/containers
		이렇게 작성하면됨
		얘는 노드 하나마다 하나씩 있어야하니까 데몬셋으로 실행해서 로그를 수집하고
		namespace는 시스템으로줘서 직접실행한 앱컨테이너와 분리하고
		컨테이너이미지는 플루언트디를 가져오고
		env하위값은
			host는 일래스틱서치디플로이먼트에 로그를 보낼때 일래스틱서치의 도메인
			port는 일래스틱서치 포트
			scheme는 http와 https 설정
			fluent_uid는 플루언트디를 사용하는 사용자 아이디,저거랑 같으면 접근권한이 있는것
		첫번째와 두번쨰 볼륨마운트의 path값은 실제 로그를 저장하는 경로를 설정해 볼륨으로 마운트함
			/var/log는 쿠버네티스의 시스템프로세스의 로그가 저장되고
			/var/lib/docker/containers에는 쿠버네티스의 파드의 로그가 저장됨
			
		이걸 적용하면 일래스틱서치에 로그를 저장함
		
		이렇게 저장한 로그를 키바나에서 조회할땐
		키바나에서 일래스틱서치의 특정 인덱스들 데이터를 조회하는 인덱스 패턴을 만들어야함
		인덱스패턴에서 logstash-*을 하면 logstash-가 붙은 모든걸 조회하는(와일드카드)인덱스패턴임
		
		보통 일래스틱서치와 키바나를 함께 쓸때는 날짜별로 인덱스를 조회하는 인덱스 패턴을 만듬
		일래스틱서치에 날짜단위로 로그를 저장하면 로그가 많을때 오래된날짜의 인덱스만 삭제해서 용량을 확보함
		
	5.스턴을 이용한 실시간 로그 모니터링
		스턴은 실시간으로 여러개 파드의 로그를 보고싶을때 사용함
		
		스턴은 셸에서 바로 여러개의 파드로그를 실시간으로 볼수있어,서비스개발을 진행하는중에 파드상태를 확인할때 유용함
		스턴을 설치하고
			stern kube -n kube-system
		을 실행하면 로그를 확인할수 있음
		
		-n옵션으로 네임스페이스를 설정할수 있음
		이외의 옵션으로는
			--all-namespaces:모든 네임스페이스에 있는 로그를 한번에 출력
			-l:특정레이블의 파들들 로그만 선택출력
			-o:출력형식지정,-o json하면 json으로 출력
			--template:출력형식을 마음대로 변경해서 원하는 항목만 확인가능
		
			
2.쿠버네티스 대시보드
	쿠버네티스를 일반적으로 쓸때는 kubectl로 커맨드라인 입력은 잘 안함
	보통 대시보드를 쓰고 그걸 사용함
	대시보드를 쓰려면 이거도 디플로이먼트추가를 해야하는데,깃허브에서 대시보드yaml을 다운받고,
	포트와 env를(인증관련) 수정하고 그대로쓰면됨
	
	그리고 대시보드서비스의 포트로 접근하면 대시보드페이지가 뜨는데
	거기서 yaml작성하고 뭐하고 하면됨
	yaml작성할떈 오른쪽위에 생성누르고 생성하면되고
	워크로드-디플로이먼트에서 실행중인 파드를 편집할수있음
			
3.쿠버네티스 클러스터 모니터링	
	쿠버네티스 모니터링은 시스템메트릭과 서비스메트릭을 수집해서 확인할수 있음
	쿠버네티스 구성요소들을 직접 관리하는 코어메트릭파이프라인과 클러스터 사용자들에게 필요한 정보를 모니터링하는 모니터링 파이프라인이 있음
	
		시스템메트릭:
			시스템메트릭은 노드나 컨테이너의 cpu,메모리사용량같은 시스템관련 메트릭임
			시스템메트릭은 다시 코어메트릭과 비코어메트릭으로 나뉨
			코어메트릭은 쿠버네티스 내부 컴포넌트들이 사용하는 메트릭
			현재 클러스터 안이나 내장오토스케일링에서 사용할수 있는 자원이 얼마인지 파악함
			대시보드같은데서 보여주는 cpu/메모리,파드/컨테이너 디스크사용량등은 코어메트릭값임
			비코어 메트릭은 쿠버네티스가 직접 운용하지않는 다른 시스템메트릭을 뜻함
		서비스 메트릭:
			서비스메트릭은 애플리케이션을 모니터링할때 필요한 메트릭
			서비스메트릭은 쿠버네티스 인프라용 컨테이너에서 수집하는 메트릭과,사용자앱에서 수집하는 메트릭으로 나뉨
			쿠버네티스 인프라컨테이너에서 수집하는 메트릭은,클러스터를 관리할때 참고할수있고,
			사용자앱에서 수집하는 메트릭은 웹 서버의 응답시간 관련 값이나 시간당 http 500에러가 몇건이나 나타났는지등
			서비스 관련 정보를 파악함
		코어 메트릭 파이프라인:
			코어 메트릭 파이프라인은 쿠버네티스 관련 구성요소들을 직접 관리하는 파이프라인
			코어 시스템 메트릭을 수집해 핵심 요소의 모니터링을 담당함
			kubelet,메트릭서버,메트릭api등으로 구성되어있음
			여기서 제공하는 메트릭은 시스템 컴포넌트에서 사용함
			스케줄러나 hpa등에서 작업할때 기초자료로 활용
			별도의 외부모니터링과 연계하지않고 독립적으로 운영됨
			kubectl에 내장된 cadvisor는 노드/파드/컨테이너의 사용량 정보를 수집함
			메트릭 서버는 이 정보들을 kubelet에서 불러와 메모리에 저장함
			이렇게 저장한 메트릭정보는 마스터의 메트릭api를 이용해 다른 시스템컴포넌트가 조회할수 있음,
			단 메모리에 저장하므로 짧은기간의 정보만 보관함
		모니터링 파이프라인:
			모니터링 파이프라인은 기본 메트릭을 포함한 여러 메트릭을 수집함
			여기서 수집한 메트릭은 시스템보단,클러스터 사용자에게 필요한 모니터링에 사용됨
			쿠버네티스는 모니터링 파이프라인을 직접 관리하지않아서,다른거 가져다써야함
			모니터링 파이프라인은 시스템메트릭과 서비스메트릭 모두 수집할수 있음
			코어메트릭파이프라인과는 분리되어있으므로 알아서 조합해서 쓰면됨
			보통 프로메테우스쓰는듯
		
	1.힙스터
		안쓰니까생략
	2.메트릭서버
		메트릭서버는 힙스터를 간소화한거
		메트릭서버는 kubelet로 수집해 메모리에 저장한 파드나 노드의 메트릭 데이터를 kube-apiserver로 조회하는 메트릭 api를 제공함
		보통 쿠버네티스의 핵심데이터는 etcd에 다 모여있는데,메트릭데이터까지 저기저장하면 부하가 너무커서 메모리에 저장하는것
		근데 메모리는 메트릭서버용 파드를 재시작하면 다날아가니까 별도의 외부스토리지를 사용해야함
		
		메트릭서버를쓰려면 github클론해다가 쓰면됨
		이거 자세한건 책에있으니까 필요하면 보자,보통은 프로메테우스쓰는듯하긴함
	3.프로메테우스
		1.기본구조
			프로메테우스는 시계열 데이터를 저장할수 있는 다차원 데이터모델과,이걸 활용할수있는 promql이라는 쿼리언어가 있음
			기본적인 데이터수집은 수집대상에서 데이터를 가져오는 풀 구조임
			수집대상은 정적으로(주는거받기),혹은 서비스 디스커버리를 이용해 동적으로 설정할수 있고,외부에서 직접 푸시한 모니터링데이터를
			푸시게이트웨이로 받아서 저장할수도 있음
			데이터는 단순하게 디스크에 저장하며,외부스토리지에 저장할수도있음
			
			데이터의 시각화는 내장된 웹ui를 쓰거나 그라파나와 연결함(보통이렇게하는거같음)
		2.아키텍쳐
			프로메테우스에서 잡과 쿠버네티스본체에서 데이터를 수집하고,그라파나에서 promql로 쿼리를 날리면 거기에 대답을 하고,
			조건에 따라 알람을 줄수도있음
			
			주요컴포넌트는
				프로메테우스서버:시계열 데이터를 수집해서 저장
				클라이언트 라이브러리:앱을 개발할때 프로메테우스에서 데이터를 수집하게 만드는 라이브러리
				푸시 게이트웨이:클라이언트에서 직접 프로메테우스로 데이터를 보낼때 받는 역할
				익스포터:클라이언트 라이브러리를 내장하지 않은 앱에서 데이터를 수집함,어지간하면 처리됨
				알람관리자:알람을 보낼때 중복처리,그룹화등을 하고 알람을 어디로 보낼지 관리함
				
		3.프로메테우스 사용하기
			프로메테우스를 쓰려면 깃허브에서 쿠버네티스파일을 받고
			프로메테우스용 디플로이먼트와 서비스를 설정(어짜피 책봐야대니까 책보자)하고
			서비스에 설정한 포트로 접근하면됨
			
			그리고 그라파나랑 연동하려면 디플로이먼트를 만들고 env에 설정을 해주고 (이거도 책보자)
			그쪽 포트로 접근하면됨
			웹ui에서 add data source로 프로메테우스를 추가하면됨
			
			그리고 그라파나 공식사이트에 가면 사람들이 만들어둔 대시보드있으니까 가져다써도되고 그럼
			
		
18.오토스케일링
	오토스케일링은 클러스터 상태에 따라 디플로이먼트를 이용해 파드개수를 늘이거나 줄여주는것
	이때 사용하는게 hpa
1.hpa의 구조
	hpa는 컨트롤러 매니저 안에서 주기적으로 실행되면서 설정된 디플로이먼트의 상태를 확인함
	대충 30초에 한번씩 일어나서 숫자 늘리거나 줄이고 다시 30초자고 그런식
	hpa는 디플로이먼트에 속한 파드들의 상태를 모니터링하다가 지정된 조건에 이르면 디플로이먼트를 스케일해서 파드개수를 늘리거나 줄임
	실행할때마다 지정된 자원의 사용량을 쿠버네티스api로 확인한 후 설정된 hpa조건에 맞을때 오토스케일링을 실행함
	반복실행시간은 --horizontal-pod-autoscaler-sync-period로 설정할수있고,기본값은 30초
	
	어떤 기준으로 오토스케일링할지는
		대상파드개수=(현재파드의 cpu사용량의 총합/목표cpu사용률)의 올림
		만약 타깃사용률이 60이고
		파드2개가 50,80이면
		130/60=2.17
		2.17을 올림해서 3개가 됨
	오토스케일링할때 기준인 자원사용량은 현재시점의 데이터만 사용함
	그래서 오토스케일링후 파드가 늘어나는중에 다시 파드를 늘리라고 요청할수도 있음
	그래서 한번 요청하면 일정시간동안 추가로 오토스케일링하지 못하게 막을수도 있음
		--horizontal-pod-autoscaler-downscale-delay
	로 조정할수있고,기본값은 5분

2.hpa 설정(템플릿)
	hpa템플릿은
		apiVersion:autoscaling/v1
		kind:HorizontalPodAutoscaler
		metadata:
			name:이름
			namespace:네임스페이스
		spec:
			maxReplicas:최대치
			minReplicas:최소치
			scaleTargetRef:                   관리타겟
				apiVersion:extensions/v1beta1
				kind:Deployment
				name:관리할 디플로이먼트이름
			targetCpuUtilizationPercentage:30  목표cpu사용률
	이렇게 설정함
	spec.maxReplicas와 minReplicas로 최대최소치를 정하고
	목표대상은 scaleTargetRef에 종류와 관리대상이름을 명시해주는거로 정하고
	목표cpu사용율을 적어주면됨
	
	hpa의 상태는 kubectl get hpa로 확인할수 있음
	거기서 targets항목보면됨
	
	만약 kubectl top pods는 들어가는데 hpa가 cpu사용률을 확인하지 못하면,
	디플로이먼트의 spec.template.containers[].resources.requests.cpu필드가 설정되어있는지 확인
	이게없으면 계산을 할수없어서 unknown으로 나타남
	
	
19.사용자 정의 자원
	쿠버네티스는 잘 구조화된 api를 사용했으므로 확장성이 뛰어난데,이때 쿠버네티스가 제공하는 내장자원뿐 아니라,
	사용자가 필요한 자원을 쿠버네티스 안에서 정의해 사용할수 있음
	이를 사용자정의자원이라고 함
	사용자 정의자원은 kubectl같은 기본명령어들과 같이 다룰수있고,사용자정의자원을 이용하는 사용자정의 컨트롤러도 만들수있음
	
	사용자 정의자원 자체는 구조화한 데이터임
	사용자 정의 자원을 이용해서 원하는 동작을 하려면,사용자정의(앞으로 커스텀으로줄임)컨트롤러로 api를 만들어야함
	커스텀자원으로 원하는 상태를 선언하면,커스텀컨트롤러가 상태를 맞추는데 필요한 처리를 함
	
	커스텀 컨트롤러를 이용해 api를 만들때 만족해야할 조건은
		추가할api가 선언적형식
		추가한 커스텀자원을 kubectl로 다룰수있어야함
		추가한 타입을 대시보드같은ui에서 모니터링할수있어야함
		기존에 동작중인api를 쿠버네티스에 추가하는게아니라 새 api를 개발중임
		쿠버네티스의 기본api그룹과 네임스페이스형식을 맞춰서 새 api를 만들수있음
		추가하려는 커스텀자원이 클러스터안이나 특정네임스페이스 안에서만 동작해야함
		기본api를 활용할수있어야함
	이 있음
	커스텀자원을 사용할때 주의해야할점은,추가저장공간이 필요하고,커스텀컨트롤러에 버그가있어 예상못한문제가 생길수있음
	
	api를 만들때 중요한점은,쿠버네티스는 명령이 아니라 상태를 선언하는식으로 만들어져야함
	파드를 2개 추가하는게 아니라 파드가 4개라고 선언하는것
	
	커스텀컨트롤러는 처음부터 만드는거보다 미리 어느정도 정의된 템플릿으로 만드는게 편함
	커스텀 컨트롤러를 더 쉽게만들수있게해주는건
		kubebuilder:kubernetes-sig에서 관리하는 커스텀컨트롤러
		operator-sdk:코어os에서 오퍼레이터프레임워크의 일부로 내놓은 커스텀컨트롤러
		metacontroller:gcp에서 제공하는 사용자정의컨트롤러

1.사용자정의자원과 컨피그맵
	커스텀자원이 오브젝트의 조합이면,컨피그맵이랑 비슷한데
	둘을 고르는건
	
	컨피그맵의 조건은
		형식이 잘 정리된 설정파일사용
		전체설정을 컨피그맵에 키하나로 등록해서사용
		설정파일의 주용도가 파드안 프로그램실행일때
		쿠버네티스api가 아닌 파드안 파일이나 환경변수형태로 설정파일을 사용
		설정파일이 업데이트되면 디플로이먼트로 롤링업데이트해야함
	커스텀자원의 조건은
		새자원을 만들거나 업데이트할때 쿠버네티스 클라이언트라이브러리와 cli사용
		kubectl을 이용해 자원사용
		오브젝트의 업데이트를 지켜보다가 다른오브젝트를 crud하거나 다른오브젝트가 업데이트되었을때 커스텀자원을 crud하는 자동화를 만들고싶을때
		오브젝트를 업데이트하는 자동화를 만들고싶을때
		쿠버네티스 api에서 제공하는 .spec,.status,.metadata등의 필드사용
		관리하는 자원이나 다른 자원모음을 추상화한 오브젝트 사용
	이 있음

2.crd를 활용한 커스텀컨트롤러 사용하기
	crd로 커스텀자원 정의할때 쓰는 템플릿은
		apiversion:apiextensions.k8s.io/v1beta1
		kind:CustomResourceDefinition
		metadata:
			name:mypods.crd.example.com    plural+group
		spec:
			group:crd.example.com  restapi용 그룹이름,/apis/group/version형식
			version:v1    커스텀 api버전,직접붙이면됨
			versions:
				-name:v1  crd에서 지원하는 커스텀버전의 버전정보
				 served:true  해당버전을 사용가능한지 설정,true면 사용가능
				 storage:true  스토리지버전으로 지정되어야하는버전,스토리지버전은 하나만 존재가능
			scope:Namespaced  어디까지 접근가능한지 스코프설정
			names:
				plural:mypods  api를 요청하는 도메인에서 사용할 복수형 이름,/apis/group/version/plural형식으로 작성
				singular:mypod  cli에서 사용할 단수형이름
				kind:mypod   kind에 적을 종류의 이름
				shortNames:  단축어
					-mpod
				categories: 커스텀자원이 속할 자원그룹의 목록
					-all 
	정의는 이렇게하면되고 사용할때는
		apiversion:'crd.example.com/v1'
		kind:mypod
		metadata:
			name:이름1
		spec:
			image:이미지
			ports:
				-containerPort:8080
	이런식으로 똑같이쓰면됨

3.자원유효성 검사
	자원의 이미지필드와 컨테이너포트필드는 정의할때 설정하지 않았는데,
	crd로 api를 정의할때는 spec의 하위필드에 뭘사용할지 정의하지않음
	
	저기서 어떤내용을 설정할수 있는지 검증하는 유효성검사를 하려면 
	자원정의템플릿에
		...
		validation:
			openAPIV3Schema:
				properties:
					spec:
						properties:
						image:
							type:string 타입설정
							pattern:정규표현식
	

	이런식으로 정규표현식으로 사용할수있음

4.커스텀자원의 정보 추가하기
	kubectl get으로는 커스텀자원의 name과 age만 확인할수 있음
	그래서 커스텀자원을 정의할때.spec.additionalPrinterColumns[]로 필요한정보를 추가할수있음
	템플릿에 밑에 이거추가하면됨
		additionalPrinterColumns:
			-name:Age
			 type:date
			 priority:0  중요도,0이면 get으로 확인가능
			 JSONpath:.metadata.cretionTimestamp
			-name:Image
			 type:string
			 description:이미지항목정보(이름)
			 priority:1
			 JSONpath:.spec.image
			 
	중요도를 0으로하면 get했을때 나오고,0보다 크게설정하면 -o wide했을때 나옴
	
	type에는 integer,number(실수),string,boolean,date(생성된시간)를 성정할수있음
	JSONpath는 해당필드가 템플릿의 어떤 경로에 있는지를 지정함
	즉 템플릿에서 어디인지 찍어주면 그걸 리턴한다는거
	

5.프로메테우스 오퍼레이터 사용하기
	커스텀 컨트롤러를 사용하는 대표적인 예로 코어os의 오퍼레이터라는 프레임워크가 있음
	특정 앱의 생명주기를 자동으로 관리한다는 개념
	
	오픈소스앱을 도입할땐 전문가가 있는게 제일좋지만,여건이 안되면 오퍼레이터를 사용함
	보통 제일 많이 알려진게 프로메테우스임
	프로메테우스 오퍼레이터는 깃헙에서 받고 mainfests에있는거 전부실행한후 (-f)
	네임스페이스와 파드및설정을 한 후
	필드설정을 좀 수정하고(로컬이니까)
	대시보드를 들어가면 커스텀자원을 모니터링함

























