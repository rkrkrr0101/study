1.머신러닝
	스킵
2.kubeflow
1.설치
	스킵
2.쿠브플로우 컴포넌트
	쿠브플로우의 컴포넌트로는,대쉬보드,쥬피터노트북서버,페어링,카티브,파이프라인,ml모델트레이닝,서빙모델,메타데이터가 있음
	
	대쉬보드는 각 컴포넌트에 접근할수있는 게이트웨이임,쿠버네티스 게이트웨이를 통해서 쿠브플로우의 컴포넌트들의 엔드포인트에 접근할수있음
	여기서 접근할수있는 컴포넌트에는
		주피터노트북서버
		카티브
		파이프라인
		아티팩트 스토어
		매니지 컨트리뷰터
	가 있음,단 매니지컨트리뷰터는 현재 사용자가 현재 네임스페이스의 소유자일때만 노출됨
	
	대시보드에 접근은 그냥 서비스 ip포트보고 접근하면됨
	
	노트북서버는 쿠버네티스위에서 실행되는 쥬피터노트북임,쿠버네티스에서 리소스를 할당받아서,별로 뭐할필요없이 바로 사용할수있음
	
	노트북을 생성할땐,대쉬보드에서 뉴서버를 누르고,노트북이름과 네임스페이스를 적으면(기본값은 현재네임스페이스)됨
	그리고 노트북에서 사용할 도커이미지를 선택하면됨
	여기선 스탠다드이미지(텐서플로,kubectl,gcp관련,쿠브플로우라이브러리포함)와 커스텀이미지중에 선택하면됨
	그리고 cpu와 메모리를 입력하고(k8s 메모리쿼터)
	워크스페이스 볼륨,데이터볼륨을 입력하면됨(pv),이미 만들어진 pv사용도 가능하고,신규볼륨생성도 가능함
	그리고 옵션으로 환경변수나 시크릿같은 값을 별도로 정의할수있고,gpu할당도 할수있음
	
	쥬피터노트북은 k8s의 리소스를 사용할수있음,그래서 노트북터미널창에서 k get pod같은거 치면 들어감(물론 자기네임스페이스만)
	여기선 파드,디플로이먼트,서비스,잡,tf잡,파이토치잡을 사용할수있음
	
	기본적으로 노트북은 스테이트풀셋으로 관리됨
	
	페어링은 쿠브플로우가 설치된 환경에서 쉽게 ml모델을 학습/배포할수있는 패키지임
	얘가 하는일은
		작성한 모델코드를 도커라이징
		클라우드에서 트레이닝잡을 실행시킬수있는 api지원
		고차원 api지원으로 인해 학습된모델배포를 쉽게함
		
	페어링은 노트북,파이썬함수,파이썬파일을 도커이미지로 빌드하고,
	이미지가 빌드되면 설정한 도커레지스트리에 푸시하고,
	푸시가 완료되면 설정한 배포리소스타입에 따라 job,tfjob,kfserving등의 리소스로 변환해서 api서버에 요청함
	이과정을 나누면
		preprocessor:작성코드를 도커이미지에 넣을수있게 패키지화
		builder:패키지파일을 도커이미지화
		deployer:생성된 이미지를 클러스터에 배포
	로 볼수있음
	
	페어링은 파이썬 3.6이상에서 설치할수있음
	페어링은 도커레지스트리정보와 쿠브플로우클러스터에 접근할수있는 권한이 필요함(클러스터는 쿠브플로우로깔면 알아서가져오고,레지스트리는 넣어줘야함,검색)
	
	페어링패키지의 핵심은 fairing.config임
		fairing.config.set_builder('append',베이스이미지,레지스트리,푸시=true)//빌더설정
		fairing.config.set_deployer('job')//디플로이어설정
		fairing.config.fn(train)//동작함수설정(모델실행함수)
	이렇게 하면,빌더에있는 베이스이미지를 기반으로,모델실행함수를 실행하는 도커이미지를 생성하고,
	그걸 레지스트리에 푸시함
	그리고 그걸로 잡을 쿠버네티스클러스터에 실행요청함
	
	preprocessor(전처리기)는 도커이미지로 패키지화 할 대상을 설정함,기본값은 노트북이면 notebook,아니면 python
	전처리기는 4개의 타입이 있음
		python:파이썬파일을 패키징함,도커이미지에선 python /app/파일명으로 cmd가 생성됨,
				파이썬파일 그대로쓰기때문에 분기로 페어링실행과 train()을 구분함
		notebook:쥬피터노트북파일을 파이썬으로 변환한후 그걸 패키징함
		full_notebook:노트북파일을 수행한후 결과를 다시 노트북파일로 생성
		function:단일함수를 패키징함
	보통 노트북이나 파이썬을 쓰는듯
	그리고 전처리기는 페어링이 추가된 소스가 아닌 다른 리소스들의 리스트를 파라미터로 받음
		fairing.config.set_preprocessor('notebook',input_files=["data.csv","requirements.txt"])
		
	빌더는 전처리기가 생성한 패키지를 도커이미지화 시킴
	여기도 타입3개가 있는데
		append:도커 클라이언트를 사용하지않고,자체 라이브러리를 통해 이미지를 생성함
			   도커클라이언트를 사용할수없는환경에서 사용하기좋긴한데,도커데몬의 설정값을 사용할수없어서,
			   로그인이 필요한 레지스트리면 config.json을 만들어줘야함
		cluster:구글컨테이너툴인 kaniko를 사용해서 도커이미지를 만듬
				직렬화된 파이썬파일을 스토리지에 업로드하고 카니코잡을 쿠버네티스클러스터에 요청함,
				카니코잡은 스토리지에서 직렬화된 파이썬파일을 다운로드한후 이미지를 생성함
				스토리지는 클라우드서비스의 스토리지와 클러스터내 minio를 사용할수있음,
				레지스트리 푸시는 kaniko에서 처리하기때문에 클러스터가 해당레지스트리에 푸시할수있는 권한을 가지고있어야함
		docker:로컬 도커클라이언트로 도커이미지생성함
			   로컬에서 진행해서,로컬환경이 대상레지스트리에 푸시/풀권한이 있어야함
		
	디플로이어는 이미지생성이 완료되면 해당 이미지의 배포를 진행함,여기서 배포는 mljob이 될수도있고,서빙모델배포가 될수도있음
	사용방법은
		fairing.config.set_deployer('job',
									namespace='test',
									pod_spec_mutators=[k8s_utils.get_resource_mutator(cpu=2,memory=5)])
	
	이런식으로 배포형태(job,tfjob,serving,kfserving등을 선택)를 정하고,
	네임스페이스를 정한다음
	배포될 파드의 스펙을 정함(pvc나 리소스설정등)
		k8s_utils.get_resource_mutator로 리소스설정
		k8s_utils.mounting_pvc로 pvc설정
	을 할수있음
	
	config.run은 설정된 값기준으로 페어링을 실행해줌
	config.fn은 함수를 입력값으로 받아,전처리기를 function으로 설정하고 run을 실행하는 함수를 반환함
	
	페어링은 config클래스에서 좀 더 ml트레인에 특화된 클래스를 제공함
		backendinterface:백엔드인터페이스는 트레인잡이나 서빙잡을 실행할때 클러스터의 환경을 제공해주는 기본클래스임
		이 클래스를 상속해서 특정 클러스터의 특성에 맞게 전처리기,빌더,디플로이어 및 부가기능을 제공함,보통 클라우드는 다 있음
		
		trainjob:config클래스에서 나눠설정하던걸 모아둔거,백엔드클래스설정이 필요
		
		predictionendpoint:예측모델의 서비스를 생성함,모델생성,모델학습,예측까지 하나의클래스에서 설정할수있음
	
	
	
	
	카티브는 하이퍼파라미터 최적화와 뉴럴 아키텍쳐 탐색을 하는 컴포넌트임
	하이퍼파라미터는 그 학습률 dense층크기 dropout비율같은거고
	뉴럴아키텍쳐 탐색은 높은 예측정확도와 최적성능을 내는 인공신경망을 디자인하기위해 사용됨
	얘는 이걸위해 강화학습을 사용함
	카티브의 구조는
		experiment:하나의 최적화실행단위,job이라고 보면됨,쿠버네티스 커스텀리소스이고 trial을 실행함
		이건 5개의 영역으로 나누어짐
			trial count:실행횟수,병렬실행횟수
			trial template:trial 파드 명세
			objective:목표수치,최고값,최저값
			search parameter:탐색파라미터와 범위
			search algorithm:탐색알고리즘
	
		trial:최적화과정의 반복단위,experiment의 trial count수만큼 생성돼서,하나가 종료되면 그 다음trial이 생성됨
		하나의 trial에서 하나의 worker job이 실행됨,이거도 커스텀리소스임
		
		suggestion:이거도 커스텀리소스고,검색알고리즘으로 생성된 하이퍼파라미터 값의 모음임,하나의 experiment당 하나의 suggestion이 생성됨
		experiment에서 설정된 parameter와 검색알고리즘이 만들어낸 값을 각 trial에 제공함
		
		workerjob:파라미터와 suggestion값을 가지고 트리얼을 평가하며,목표값을 계산하는 프로세스임
		실제로 학습을 수행하는 일을 하고,워커는 단일실행인 k8s잡과 분산실행이 가능한 tfjob,파이토치잡을 다 사용할수있음
		
		
	
	experiment는 그냥 디플로이먼트처럼 ymal만들어서 생성하면됨,spec에 병렬실행갯수,최대실행수,최대실패수,objective넣고
	알고리즘을 정하고 trialtemplate에서 트리얼의 템플릿을 정의함,
	여기서 ml트레이닝코드가 패키징된 도커이미지를 사용하고,그 도커이미지는 하이퍼파라미터를 환경변수로 설정할수있어야함
	
	카티브의 하이퍼파라미터 검색 알고리즘은,
	그리드(모든 변수가 연속적이지않고,검색가짓수가 적을때 모든조합가능성에 대해 수행)
	랜덤(그냥 전체중 랜덤으로 횟수만큼돌림)
	베이지안 최적화(어떤 입력값을 받는 함수를 만들어서,그함수값을 최대로 만드는 최적해를 찾는걸 반복해서 그 최적해를 좁혀나가는방법,
			추출값을 기반으로 다시검색해서 시간대비성능이 좋음)
	하이퍼밴드(하이퍼밴드 프레임워크를 사용,설정에따라 성능이결정되고,자원이 많으면 랜덤보다 효율이 떨어짐)
	hyperropt tpe(nas에서 사용함)
	nas based on reinfircement learning(nas에서 lstm으로 탐색함)
	
	카티브는 메트릭컬렉터를 통해 각 트리얼의 메트릭을 수집함
	트리얼에서 실행되는 트레이닝코드에 메트릭(accuracy,loss같은)값을 stdout나 파일형태로 기록하면,그걸 모아서(사이드카컨테이너로 수집함) 보여줌
	experiment에서 메트릭컬렉터의 타입을 정의해서 원하는 형태의 결과를 수집할수있음
		stdout:stdout에서 나오는값수집
		file:특정파일에서 메트릭을 수집,경로지정가능
		tensorflowflowevent:텐서플로 tf.event가 저장된 디렉토리에서 메트릭을 수집,경로지정가능
		custom:사용자정의메트릭컬렉터 사용
		none:사용하지않음
	그리고 출력할때도 metricscollectorSpec.source.filter.metrics-Format필드를 통해 정의할수있고,정규식을 사용할수있음
	기본값은 메트릭이름=메트릭값 임
	
	카티브는 여러컴포넌트로 이루어지고,각 컴포넌트는 디플로이먼트로 실행됨,각 컴포넌트는 grpc를 통해 서로 통신함
	api는 카티브레포지터리를 보면 확인할수있음
		katib-manager:grpc api 서버
		katib-db:카티브 백엔드 데이터저장소,mysql이 사용됨
		katib-ui:카티브 유저인터페이스
		katib-controller:카티브 crd컨트롤러
		
	카티브는 webui를 제공함,메인화면에 하이퍼파라미터 튜닝과,nas버튼이 있고,그걸누르면 yaml입력칸이 나오는데(ui도 있음)거기 입력하고 실행하면
	현재 진행상황과 결과등을 확인할수있음
	
	
	
	파이프라인은 컨테이너 기반의 엔드투엔드 ml워크플로우를 만들고 배포할수있는 쿠버네티스 플랫폼임
	컨테이너 기반으로 구성되어,확장성,재사용성이 좋고,쿠브플로우의 대표적인 플랫폿이라고 볼수있음
	얘는 쿠버네티스 자원을 관리하기위해 argo라는 워크플로툴을 사용함
	파이프라인의 구성은
		experiment(실험),잡,런을 추적하고 관리하는 유저인터페이스
		ml워크플로우 단계별 스케줄링엔진
		파이프라인과 그 컴포넌트들을 생성하는 sdk
		sdk와 연동하는 주피터노트북
	으로 구성되고,쉬운 파이프라인 구성,쉬운 파이프라인 생성,쉬운 재사용을 지향함
	
	파이프라인은 워크플로의 컴포넌트들과,그것을 그래프형태로 결합하는걸 포함한 ml워크플로우의 한 형식이라고 설명할수있음,
	또한 파이프라인을 실행하기위한 입출력에 대한 정의도 포함됨
	사용자가 파이프라인을 개발한 후 쿠브플로우파이프라인을 통해 업로드,공유를 할수있고,
	이 파이프라인은 도커라이징을 할수있으며 그래프의 결합형태에 따라 순서대로 실행됨
	
	파이프라인이 실행되면 시스템은 각 단계에 맞는 파드를 실행하고,파드는 설정된 컨테이너를 실행시키고,컨테이너 안에있는 앱을 실행시킴
	스케줄러에 따라 순서대로 컨테이너들이 실행됨
	
	파이프라인의 실행순서는
		파이썬sdk:파이프라인 dsl을 통해 컴포넌트를 작성
		dsl compiler:파이썬코드를 쿠버네티스 yaml로 변환
		pipiline service:쿠버네티스 리소스에서 파이프라인을 생성하기위해 pipeline service를 호출
		쿠버네티스 리소스:파이프라인서비스는 k8s api서버를 호출해서 파이프라인을 실행하기위한 쿠버네티스 리소스를 생성
		orchestration controller:생성된 쿠버네티스 리소스에 정의된 파이프라인을 실행하기위한 컨테이너를 실행,이건 파드에서실행됨
		artifact storage:실행된 파드는 파이프라인의 실험정보를 담는 메타데이터를,준비된 데이터베이스에 담고,그 외 큰사이즈의 정보들인 artifact는
						서버나 클라우드에 저장함
		pipeline web server:실행된 파이프라인을 웹ui를 통해 진행상황과 생성되는 데이터 및 정보를 보여줌
		
	컴포넌트는 ml워크플로우의 한 단계를 수행하는 코드집합임,인풋,아웃풋,이름,상세구현등 함수와 유사함
	파이프라인 dsl로 작성된 파이썬코드가 yaml로 컴파일되는데,쿠브플로우 파이프라인의 컨테이너 컴포넌트 데이터모델형식으로 변환됨
	
	그래프는 파이프라인ui에서 파이프라인 런타임 실행을 나타내는 그림임
	파이프라인을 실행했거나 실행중이면,그 단계가 색으로 구분되어 표시됨
	그래프내의 각 노드는 파이프라인 내의 단계에 해당하며,그에따라 에이블이 지정됨
	
	런은 파이프라인의 단일 실행 단위임,즉 런은 파이프라인 명세를 실행함,객체와 인스턴스의 관계라고 보면됨
	파이프라인 ui를 통해 상세정보를 확인할수있음
	리커링런은 파이프라인을 주기적으로 실행하는 런임,크론형태도 가능하며,특정기간을 정의할수도 있음
	배치성 작업이나 모니터링작업에 적합함,이 반복적인 작업은 런 트리거가 담당함
	
	런트리거는 런을 새롭게 생성해야하나 말아야하나를 시스템에 알려주는 플래그임
	이건 크론형태와 periodic로 타이머형태가 있음
	
	스탭은 파이프라인에서 하나의 컴포넌트 실행을 뜻함,복잡한 파이프라인내에서 컴포넌트는 중첩되어실행될수도 있고,분기따라 선택적으로 실행될수도있음
	
	experiment는 파이프라인을 실행하는 워크스페이스임,파이프라인실행의 논리적그룹으로 봐도됨,기본적으론 default에서 실행
	
	output artifact는 파이프라인컴포넌트의 출력임,아티팩트를 통해 파이프라인의 다양한 구성요소가 어떻게 작동하는지 이해할수있음
	얘는 데이터의 일반텍스트보기에서 대화식시각화까지 다양함
	
	파이프라인은 3가지 인터페이스를 제공하고,각 인터페이스를 통해 ml워크플로를 생성,실행가능함
	여긴 웹ui와,python sdk,restapi가 있음
	파이썬에서 sdk로 실행하는건 책보고하자
	
	파이프라인 컴포넌트를 만들땐 도커이미지를 만들어야하는데,이건 약간만 건드려도 이미지를 다시만들어야함,그래서 이걸개선한게 경량파이썬컴포넌트임
	얘는,함수선언 외부에 어떤코드도 없고,임포트도 메인함수안에있어야하고,다른함수를 선언하려면 메인함수안에 선언되어야함
	
	파라미터는 파이프라인 구성요소들간의 값 전달을 위한 클래스임,파이프라인 함수의 매개변수는 파이프라인 실행시 파이프라임파람으로 등록됨,그래서 직접등록은 안함
	
	메트릭스는 파이썬 컴포넌트에서 사용한거처럼 결과값을 시각화해서 보거나 파일로 저장하는 컴포넌트임
	
	그리고 쿠버네티스 리소스를 파이프라인 컴포넌트로도 관리할수있음
	
	쿠브플로우는 쿠버네티스 잡뿐만 아니라 다양한 ml학습모델을 지원함
	tfjob같은거,궁금하면 보자
	
	서빙모델은 kfserving와 seldon core 두가지를 제공함
	둘다 오픈소스고,기본적으로 kfserving을 사용하는듯
	
	kfserving은 쿠버네티스에서 서버리스 추론을 가능하게함,ml프레임워크를 위한 추상화 인터페이스를 제공하기때문에,
	다양한 프레임워크를 운영환경에서도 쉽게 사용가능
	
	사용법은 책보면나오는데,서빙할 데이터셋 위치를 정의하고,데이터셋에 해당하는 프레임워크스펙을 정의하고,엔드포인트 스펙을 생성함,생성한 엔드포인트 스펙을
	인터페이스서비스의 메타정보스펙에 넣어서 인터페이스서비스를 생성하고 kfserving-client를 만들어 아까 생성한 인터페이스서비스를 가지고 생성함
	
	서비스가 생성되면 데이터셋을 현재 인터페이스서비스로 복사해오고,타임아웃기간동안 서빙모델이 동작함
	
	즉 특정위치에서 데이터셋을 가져오고,그걸가지고 추론던지는식임
	얘는 구글,aws,내부파일등 어지간한건 다지원함
	
	인터페이스서비스는 커스텀리소스고,인퍼런스서버를 제공함
	얘는 엔드포인트를 2개두고,랜덤하게 각자의 트래픽으로 로드밸런싱할수도 있음
	각 엔드포인트엔 3가지 컴포넌트로 구성되어있음
		predictor:모델서버가 네트워크엔드포인트를 가질수있게 만들어줌,인터페이스서비스의 주역할
		explainer:실제로 왜 그렇게 예측했는지에 대한 정보제공
		transformer:데이터 전후처리를 위한 모듈
	
	셀던서빙은 쿠브플로우와 같이 설치됨,pv나 클라우드스토리지에 학습된 모델이 있으면,셀던에서 제공하는 해당모델의 서버를 서비스해줌
	
	메타데이터는 ml워크플로우가 생성하는 메타데이터를 추적하고 관리함으로써 ml워크플로우를 이해하고 관리할수있게 도와줌
	여기서 메타데이터는 런,모델,데이터셋,다른 아티팩트들의 정보를 의미함
	아티팩트란 ml워크플로에서 컴포넌트의 입출력을 구성하는 파일과 오브젝트임
	이거도 뭐 필요해지면 보자,그냥 메트릭수집해서 보는거라 별로안어려울듯
	
	
	
	
	
3.핸즈온 쿠브플로우
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	